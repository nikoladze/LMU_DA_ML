{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bf9e43-9609-40c5-ae83-fd848fb2fe8b",
   "metadata": {},
   "source": [
    "# A neural network \"from scratch\" using automatic differentiation\n",
    "\n",
    "Inspired by\n",
    "* https://sidsite.com/posts/autodiff/\n",
    "* https://github.com/karpathy/micrograd and https://www.youtube.com/watch?v=VMj-3S1tku0\n",
    "\n",
    "In this notebook we are going to implement a neural network, a Multilayer Perceptron (MLP). To train it we will build our own Automatic Differentiation System."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b1285-d948-4377-a543-a85712ae1420",
   "metadata": {},
   "source": [
    "## Backpropagation/Reverse Mode Automatic differentiation\n",
    "\n",
    "Later we want to calculate the gradient w.r.t. the parameters (weights and biases of all neurons). To do so, we are going to use the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) - in the single variable case for a function $f(x) = f(g(x))$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x}$$\n",
    "\n",
    "In the [multivariable case](https://en.wikipedia.org/wiki/Chain_rule#Multivariable_case) this becomes\n",
    "\n",
    "$$J_{f, x} = J_{f, g}J_{g, x}$$\n",
    "\n",
    "where $J$ is the jacobian matrix and we do a matrix multiplication in the equation above. Written in components\n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x_j} = \\sum_k \\frac{\\partial f_i}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_j}$$\n",
    "\n",
    "So we need to **sum** over all contributions of sub-terms that depend on the variable we want to calculate the partial derivative for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d10e88-6de7-421b-a584-12c621ee4297",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Manually calculate the gradient of\n",
    "  \n",
    "$f(a, b) = (a + b) \\cdot (a \\cdot b)$\n",
    "\n",
    "using the multivariable chain rule via $f(a, b) = c \\cdot d$ with $c = a + b$ and $d = a \\cdot b$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad29eb8-8d66-4410-a140-7c04e4e62ee7",
   "metadata": {},
   "source": [
    "The computation graph of this looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a59ff0-76f9-41a4-8bea-06400a663e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "g = graphviz.Digraph(node_attr={\"shape\": \"record\"})\n",
    "g.attr(rankdir=\"LR\")\n",
    "g.node(\"a\", \"a\")\n",
    "g.node(\"b\", \"b\")\n",
    "g.node(\"c\", \"+|c\")\n",
    "g.node(\"d\", \"*|d\")\n",
    "g.node(\"f\", \"*|f\")\n",
    "g.edge(\"a\", \"c\")\n",
    "g.edge(\"b\", \"c\")\n",
    "g.edge(\"a\", \"d\")\n",
    "g.edge(\"b\", \"d\")\n",
    "g.edge(\"c\", \"f\")\n",
    "g.edge(\"d\", \"f\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85303fdf-6fd3-4ab0-85a5-46585dc1ae7c",
   "metadata": {},
   "source": [
    "There are [two types of automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation#Two_types_of_automatic_differentiation)\n",
    "\n",
    "* What we are doing here is **reverse mode** automatic differentiation or **backpropagation** - we accumulate the gradient by running through the computation graph in reverse order (applying the chain rule outside to inside). This is most efficient when calculating the gradient of a scalar function w.r.t. many parameters - as in NN traning.\n",
    "\n",
    "* One can also run in **forward mode** where the gradients are accumulated while calculating the forward computation graph (applying the chain rule inside to outside). This is most useful for calculating the derivative of many outputs w.r.t. few inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb992b-3e5d-421a-ab34-816641de2ea9",
   "metadata": {},
   "source": [
    "### `Variable` class\n",
    "\n",
    "So, what we need is the **local gradient** of all nodes in a computation graph. To implement this we define a `Variable` class that stores both *value* and the *local gradient*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde0035-d7d5-4ae0-a107-2911272eab78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, value, local_grad=(), name=\"\", op=\"\"):\n",
    "        self.value = value\n",
    "        self.local_grad = local_grad\n",
    "        self.name = name\n",
    "        self.op = op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Variable({self.value:.3g})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10922447-edcd-4256-8141-fdda874da184",
   "metadata": {},
   "source": [
    "We also gave it an optional `name` and `op` (name of the operation that produced it) attribute for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba908ff-d4df-430b-b994-95bb4c851174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b033f9-34dd-4d77-9838-85eecabcc984",
   "metadata": {},
   "source": [
    "And operations on Variables that also calculate both the value (\"forward pass\") and the local gradient (\"backward pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0f6a6-da3c-4287-a566-a0c4db1bfcfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return Variable(a.value + b.value, [(a, 1), (b, 1)], op=\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24420b26-9a30-4867-b279-2d543058f6c7",
   "metadata": {},
   "source": [
    "The gradient here is a *list of tuples* of `(Variable, partial_derivative)`. For the addition the partial derivative w.r.t. each input is 1.\n",
    "\n",
    "For multiplication we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838e374-89bd-41e1-b724-38cc55aa591d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mul(a, b):\n",
    "    return Variable(\n",
    "        a.value * b.value,\n",
    "        [(a, b.value), (b, a.value)],\n",
    "        op=\"*\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a6698-c1ab-4008-926b-0681209b9457",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Question:</b> What would we need to write for division? And for the power operator?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa642d2-280b-492b-8ec2-760a75b3417a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = add(Variable(3), Variable(4))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e406b-5248-4227-83fc-fc1a068073ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v.local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89271e99-a987-481e-b9da-78169f323ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = mul(Variable(3), Variable(5))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f49ee1-ba89-46a0-8778-63b499315be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v.local_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2074e6-9f34-468f-88c3-1ef3b7f77dca",
   "metadata": {},
   "source": [
    "For convenience, we overload the `+` and `*` operators of `Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027dc8c-6270-4b6a-8f4b-f86c16ebc0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable.__add__ = add\n",
    "Variable.__mul__ = mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f3e5c-70ad-4700-9810-39004ff2c86b",
   "metadata": {},
   "source": [
    "such that we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29ff81-fb02-42c9-8942-2d77c2576ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable(3) + Variable(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e91731-2e25-4790-8b18-544ba4639d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable(6) * Variable(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc1a2c-6066-4be6-93f5-068794a8bc3c",
   "metadata": {},
   "source": [
    "To add and multiply with constant numbers we wrap our functions such that they convert numbers to `Variable` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8906ae0-71d1-4f8c-be87-96a6c1c6019c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap(f):\n",
    "    def op(*args):\n",
    "        new_args = []\n",
    "        for arg in args:\n",
    "            if not isinstance(arg, Variable):\n",
    "                arg = Variable(arg)\n",
    "            new_args.append(arg)\n",
    "        return f(*new_args)\n",
    "\n",
    "    return op\n",
    "\n",
    "\n",
    "Variable.__add__ = wrap(Variable.__add__)\n",
    "Variable.__mul__ = wrap(Variable.__mul__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a97a31-f9bd-4d95-8bcd-92c7a7ba5a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable(3) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c315625-548a-47a1-9ad1-625fa6f8ccca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable(5) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691144f-4f6b-45ad-a923-5b21759736dc",
   "metadata": {},
   "source": [
    "To also add a `Variable` from the right to a number we overload `__radd__` and `__rmul__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aae3e4-78be-4e93-bc32-75f1fe32e568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Variable.__radd__ = Variable.__add__\n",
    "Variable.__rmul__ = Variable.__mul__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ef1a0-cb6e-4e79-af7f-23286563f453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "7 * Variable(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5a1b5-d12d-431d-893f-dc2289d7fabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "4 + Variable(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ea84d-32c3-4169-87f9-4e8d926ce1e8",
   "metadata": {},
   "source": [
    "### Computation graph\n",
    "\n",
    "Now let's visualize the graph of the computation you did before $f(a, b) = c \\cdot d$ with $c = a + b$ and $d = a \\cdot b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4812e5c-d453-464d-a38c-14f096c9d65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601e765-87d2-467d-87dd-f8651cff5be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_var(var):\n",
    "    out = f\"{var.value:.3g}\"\n",
    "    if var.name:\n",
    "        out = f\"{var.name}={out}\"\n",
    "    if var.op:\n",
    "        out = f\"{var.op}|{out}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ffd5d-3eb5-42ab-9266-765fbbaf78b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_graph(variable):\n",
    "    g = graphviz.Digraph(node_attr={\"shape\": \"record\", \"height\": \".1\"})\n",
    "    g.attr(rankdir=\"LR\")\n",
    "    counter = count(0)\n",
    "    nodes = {}\n",
    "\n",
    "    def add_node(variable):\n",
    "        node_name = f\"node{next(counter)}\"\n",
    "        g.node(node_name, format_var(variable))\n",
    "        nodes[variable] = node_name\n",
    "\n",
    "    def add_edges(variable):\n",
    "        if variable not in nodes:\n",
    "            add_node(variable)\n",
    "        for child_variable, deriv in variable.local_grad:\n",
    "            if child_variable not in nodes:\n",
    "                add_node(child_variable)\n",
    "            g.edge(nodes[child_variable], nodes[variable])\n",
    "            add_edges(child_variable)\n",
    "\n",
    "    add_edges(variable)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea75ef5-1877-4490-bdc3-334fa0718849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = Variable(3, name=\"a\")\n",
    "b = Variable(4, name=\"b\")\n",
    "c = a + b; c.name = \"c\"\n",
    "d = a * b; d.name = \"d\"\n",
    "f = c * d; f.name = \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68654a-be91-41d4-b34d-425d2c6df79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_graph(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d3b6f-150e-4f58-beeb-0719d454e242",
   "metadata": {},
   "source": [
    "To get the gradient of Variable `f` we need to run backwards through this graph, always summing the gradients of all paths leading to a node.\n",
    "\n",
    "Therefore we need to process the graph in **reverse [topological order](https://en.wikipedia.org/wiki/Topological_sorting)** - where a topological ordering means dependencies of nodes have to come before nodes that depend on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83211b22-4e83-4a94-bf1c-e7911863ff71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def topo_ordered_nodes(variable):\n",
    "    nodes = []\n",
    "    visited = set()\n",
    "\n",
    "    def add_nodes(variable):\n",
    "        if variable in visited:\n",
    "            return\n",
    "        visited.add(variable)\n",
    "        for child_variable, deriv in variable.local_grad:\n",
    "            add_nodes(child_variable)\n",
    "        nodes.append(variable)\n",
    "\n",
    "    add_nodes(variable)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59218a07-27d9-425c-a11f-e4774b89544e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topo_ordered_nodes(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d09b29-a037-495e-b316-7cf0a78763ad",
   "metadata": {},
   "source": [
    "### The backpropagation algorithm\n",
    "\n",
    "Now the gradient function - the **backpropagation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e422dee-62c8-4626-98a1-4e1e687cc787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3d0f7-ae58-4ecf-93e9-d792484cc6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gradient(variable):\n",
    "    grad = defaultdict(int)\n",
    "    grad[variable] = 1\n",
    "    for parent_variable in reversed(topo_ordered_nodes(variable)):\n",
    "        for child_variable, deriv in parent_variable.local_grad:\n",
    "            grad[child_variable] += deriv * grad[parent_variable]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef0002-2c15-4689-a510-5c1eea7f8182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_gradient(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e7db3-635d-4e19-a884-d604d747b45c",
   "metadata": {},
   "source": [
    "We can also approximate the derivative numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbcffde-806e-4ff0-b7a9-ac9e70a616f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    c = a + b\n",
    "    d = a * b\n",
    "    return c * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c874fa0-3992-4cfa-9624-a00fdba177a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dx = 1e-5\n",
    "dy = f(3 + dx, 4) - f(3, 4)\n",
    "dy / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1f76e-7d62-4e62-b7af-790dab3a0a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dx = 1e-5\n",
    "dy = f(3, 4 + dx) - f(3, 4)\n",
    "dy / dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e165ab6-96bf-4428-aab6-8d78240ab99b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Question:</b> Why then even use backpropagation and not just numerical gradients?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    <ol>\n",
    "        <li>\n",
    "            It can be more efficient since we only need to (reverse) traverse the computation graph <b>once</b> to get the gradient w.r.t. <b>all</b> variables. For numerical derivatives we need at least one function evaluation per variable.\n",
    "        </li>\n",
    "        <li>We get <b>exact</b> derivatives\n",
    "    </ol>\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ddc6b-8aac-4a34-8272-63ec68622924",
   "metadata": {},
   "source": [
    "### Neural network in terms of `Variable` instances\n",
    "\n",
    "To make a neural network, we start with a `Neuron` that has its `weights` and the `bias` defined in terms of `Variable` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c686d9-5ec9-4345-8ebe-32c955876406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36628dd-dda4-4c74-9f7b-5f3d4c7c59af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.weights = [Variable(random.normalvariate(0, 1)) for _ in range(n_inputs)]\n",
    "        self.bias = Variable(0)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return sum(x * weight for x, weight in zip(self.weights, inputs)) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e82edc-ff83-4959-abd6-adfcd0331687",
   "metadata": {},
   "source": [
    "We initialized the weights to random numbers around 0 which is a common practice. There are some arguments for using specific rules for how exactly to scale these, but for the purpose of this demonstration it's enough to just use a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c0bd7-c3e1-4114-9496-22ccee9da7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = [random.random() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1d727-3760-4467-999b-8f5d4af5a68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neuron = Neuron(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ad199-407d-4163-85d4-4f7e13bf6414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neuron(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc1fba-50ac-4565-8f33-2fc8ee9d4e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_graph(neuron(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d216b-5b72-4388-b8d3-c43fd3433a35",
   "metadata": {},
   "source": [
    "The sum looks a bit ugly (we have all the additions as individual operations in there)\n",
    "\n",
    "so let's introduce sum as an operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee63214-6129-404c-9b1c-da4e3b679f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def var_sum(*variables):\n",
    "    return Variable(sum(v.value for v in variables), [(v, 1) for v in variables], op=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3528bf-7279-42bf-8d2a-a372848e8c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def new_call(self, inputs):\n",
    "    return (\n",
    "        var_sum(*[x * weight for x, weight in zip(self.weights, inputs)]) + self.bias\n",
    "    )\n",
    "\n",
    "Neuron.__call__ = new_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43514ec8-f8f0-43b3-9411-6f1bef9fd8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neuron = Neuron(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43038602-e48d-479e-b6ed-344f21da3727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_graph(neuron(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80dbffa-7f19-47f4-8c4d-fcb19359a073",
   "metadata": {},
   "source": [
    "Next, we need the [activation functions](NN_Activation.ipynb) - we will use **ReLU** for the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61bd94-e20e-4eec-9b40-4341c7532595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(variable):\n",
    "    return Variable(\n",
    "        max(variable.value, 0),\n",
    "        [(variable, int(variable.value > 0))],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2a48d-aecc-4476-89c7-977ac521eb49",
   "metadata": {},
   "source": [
    "And the **sigmoid** activation function for the final layer since we are going to solve a binary classification problem.\n",
    "\n",
    "A useful way to write the [derivative of the sigmoid function](https://en.wikipedia.org/wiki/Logistic_function#Derivative) is in terms of the function value itself:\n",
    "\n",
    "$y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = y \\cdot (1 - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9c1a9-8e4d-4521-a5e3-cd49535cec31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f50bc6-f351-4a11-8b61-bd8a10e79dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(var):\n",
    "    x = var.value\n",
    "    y = 1 / (1 + exp(-x))\n",
    "    return Variable(y, [(var, y * (1 - y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00d415-5f43-4fe7-8a63-de367cf1d495",
   "metadata": {},
   "source": [
    "With this we define our neural network. A layer is just a list of neurons that receive the same inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c4e25-e1d8-460f-b355-2694a850db07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.neurons = [Neuron(n_inputs) for _ in range(n_outputs)]\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        return [neuron(inputs) for neuron in self.neurons]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a6599-dcfc-4767-ac3f-2d4f24d51b81",
   "metadata": {},
   "source": [
    "And a Multilayer Perceptron - **MLP** - a stack of Layers with activation fuctions after each Neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaa689-eec7-4766-bff5-ad397eb9c645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, n_inputs, neurons_per_layer):\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.layers = []\n",
    "        for n_outputs in neurons_per_layer:\n",
    "            self.layers.append(Layer(n_inputs, n_outputs))\n",
    "            n_inputs = n_outputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if layer is not self.layers[-1]:\n",
    "                x = [relu(xi) for xi in x]\n",
    "        return [sigmoid(xi) for xi in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a46e4-f33b-487e-a5cf-49a9d3204198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLP(5, [2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8c49c-f60c-4846-9c27-92de0fb52b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_graph(mlp(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420caae-a6bf-4c66-8820-5383b566289a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will use the binary cross entropy loss to train the model for a binary classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73565274-0139-489a-8b2e-5b23f8368420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@wrap\n",
    "def binary_crossentropy(var_y_true, var_y_pred):\n",
    "    y_true = var_y_true.value\n",
    "    y_pred = var_y_pred.value\n",
    "    eps = 1e-50 # to avoid math domain errors from log(0)\n",
    "    return Variable(\n",
    "        -(y_true * log(y_pred + eps) + (1 - y_true) * log(1 - y_pred + eps)),\n",
    "        [(var_y_pred, -y_true / (y_pred + eps) + (1 - y_true) / (1 - y_pred + eps))],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65883c54-162e-46b6-bca4-90962c2bab2b",
   "metadata": {},
   "source": [
    "Note that we skipped the gradient w.r.t. `y_true` because we don't need that for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17858692-2969-4f70-9261-79460519eff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = mlp(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eba29d-b467-45ff-84cd-bb88daee4517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca02a61-8c12-4d88-a782-77686299f524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002c36c-1ac1-4653-ade8-dd8e8bec2991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad = get_gradient(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16979ac0-1b05-4642-a41d-3dc1cb8f2131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97a400-8a0a-43ba-8257-af4860b7c8ca",
   "metadata": {},
   "source": [
    "Again, let's test a few derivatives numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c12302-746f-429f-9d88-922de9aa45c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "grad[mlp.layers[i].neurons[j].weights[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b65251-8164-49d8-bb6d-7ff6c100a34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(weight):\n",
    "    old_weight = mlp.layers[i].neurons[j].weights[k]\n",
    "    mlp.layers[i].neurons[j].weights[k].value = weight\n",
    "    y_pred = mlp(x)[0]\n",
    "    mlp.layers[i].neurons[j].weights[k] = old_weight\n",
    "    return binary_crossentropy(y_true, y_pred).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7031a-763d-44f1-8f26-5918b369375e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = mlp.layers[i].neurons[j].weights[k].value\n",
    "dx = 1e-5\n",
    "dy = f(weight + dx) - f(weight)\n",
    "dy / dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5d66-593e-4c83-80e5-b0a23e75bd00",
   "metadata": {},
   "source": [
    "Now the gradient update function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143af04b-4ab3-4a82-98d7-00dbfa65e594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update(mlp, grad, learning_rate=1):\n",
    "    for layer in mlp.layers:\n",
    "        for neuron in layer.neurons:\n",
    "            for weight in neuron.weights + [neuron.bias]:\n",
    "                weight.value -= grad[weight] * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a889e9-94a1-44c4-9ec5-7f3971f3cd4a",
   "metadata": {},
   "source": [
    "And the training loop - we will use *stochastic gradient decsent* (SGD) with batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a2da-30a5-47fa-a64d-b208dfdaec9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(mlp, X, y, epochs=10, batch_size=32, learning_rate=1):\n",
    "    for epoch in range(epochs):\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            x_batch = X[start : start + batch_size]\n",
    "            y_batch = y[start : start + batch_size]\n",
    "            outputs = [mlp(x)[0] for x in x_batch]\n",
    "            losses = [\n",
    "                binary_crossentropy(y_true, y_pred)\n",
    "                for y_true, y_pred in zip(y_batch, outputs)\n",
    "            ]\n",
    "            loss = var_sum(*losses) * (1 / len(losses)) # mean\n",
    "            grad = get_gradient(loss)\n",
    "            update(mlp, grad, learning_rate=1)\n",
    "        print(f\"{epoch=}, {loss.value=}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa688a56-60d6-4825-ad04-bf5947185f32",
   "metadata": {},
   "source": [
    "Let's try to train on the *moons* dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fba811-1a32-47ff-a70d-147388d5c554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b454c35-10db-4848-bcb0-07a489c98622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(1000, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ecbcd-2943-48d9-bb22-1fe95b9e0326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(*X[y == 0].T, \".\", label=\"0\")\n",
    "plt.plot(*X[y == 1].T, \".\", label=\"1\")\n",
    "plt.legend(title=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24b7db-4132-40af-8237-32be35e84f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLP(2, [16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f375b-be0d-40ee-b53c-2adb5cb241b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = fit(mlp, X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95040af-0cfc-4e33-87a8-0f08fa172011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(min(X[:, 0]), max(X[:, 0]), 100),\n",
    "    np.linspace(min(X[:, 1]), max(X[:, 1]), 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2a7a1-0c8c-4458-8570-5dbc12ca5334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy = np.stack([xx, yy], axis=-1).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8f1bf-34f4-4ad8-bd95-057f165e46bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = np.array([mlp(xi)[0].value for xi in xy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb6f4a-3a74-44b4-aa0f-370205b1a2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.contourf(xx, yy, z.reshape(xx.shape), cmap=\"RdBu\", alpha=0.8)\n",
    "plt.scatter(*X.T, marker=\".\", c=y, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22f2f9-1338-4669-a736-03c7f1b101ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra: Using arrays\n",
    "\n",
    "It's not too difficult to extend this system to use `numpy` arrays as input and output for each node in the computation graph.\n",
    "\n",
    "Recall the multivariable chain rule:\n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x_j} = \\sum_k \\frac{\\partial f_i}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_j}$$\n",
    "\n",
    "That means if we have a vector at each node, we would in theory need to store the local Jacobians at each stage. However looking at the formula for the gradient of a single scalar output (fixed $i$) $\\frac{\\partial f}{\\partial \\vec x}$ we see that we actually only need the product of the gradient vector $\\frac{\\partial f}{\\partial \\vec g}$ with the jacobian $\\frac{\\partial \\vec g}{\\partial \\vec x}$ - the **vector jacobian product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f2c3b-76f6-4cb3-aaf7-853bb5c4061e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, array, backward=None, op=None, name=None):\n",
    "        self.array = array\n",
    "        self.backward = None\n",
    "        self.op = op\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        prefix = \"Tensor(\"\n",
    "        lines = repr(self.array).splitlines()\n",
    "        for i in range(1, len(lines)):\n",
    "            lines[i] = \" \" * len(prefix) + lines[i]\n",
    "        array_repr = \"\\n\".join(lines)\n",
    "        return f\"{prefix}{array_repr})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        out = Tensor(self.array + other.array)\n",
    "        \n",
    "        def backward(grads):\n",
    "            return [(self, grads[self]), (other, grads[other])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6eba2-a053-4a3b-9943-44dc0f378040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4c71d-00f2-4468-95e0-9ad6e4d6ce6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tensor(np.random.rand(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279e25c-e436-4865-a66f-8bcf1b79f0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
