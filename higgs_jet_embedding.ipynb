{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_settings import set_tf_nthreads\n",
    "set_tf_nthreads(1) # best setting for this tutorial at CIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'PRI_jet_leading_pt' : 'PRI_jet_subleading_phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_cols = sum([[f\"PRI_{obj}_{field}\" for field in [\"pt\", \"eta\", \"phi\"]] for obj in [\"jet_leading\", \"jet_subleading\"]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other_cols = [col for col in df.columns if col.startswith(\"PRI\") and col not in jet_cols]\n",
    "other_cols = [col for col in df.columns if (col.startswith(\"PRI\") or col.startswith(\"DER\")) and col not in jet_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "#X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "X = df.loc[:,'PRI_tau_pt':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other = df[other_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet = df[jet_cols].to_numpy().reshape(-1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "(\n",
    "    X_train, X_test,\n",
    "    X_jet_train, X_jet_test,\n",
    "    X_other_train, X_other_test,\n",
    "    y_train, y_test,\n",
    "    eventID_train, event_ID_test,\n",
    "    weight_train, weight_test,\n",
    ") = train_test_split(\n",
    "    X, X_jet, X_other, y, eventID, weight, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the [approximate median significance][1] from the Kaggle competition to determine how good a solution was. Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function to compute approximate median significance (AMS)\n",
    "%pycat ams.py\n",
    "%run ams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total weights (yields)\n",
    "sigall  = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "sigtrain  = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "sigtest  = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet[:, :, 0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetScaler:\n",
    "    def __init__(self):\n",
    "        self.scalers = []\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for i in range(3):\n",
    "            x = X[:, :, i].ravel()\n",
    "            x = x[x != -999].reshape(-1, 1)\n",
    "            scaler = RobustScaler()\n",
    "            scaler.fit(x)\n",
    "            self.scalers.append(scaler)\n",
    "            \n",
    "    def transform(self, X):\n",
    "        outputs = []\n",
    "        for i, scaler in enumerate(self.scalers):\n",
    "            x = np.array(X[:, :, i].ravel())\n",
    "            x[x != -999] = scaler.transform(x[x != -999].reshape(-1, 1)).ravel()\n",
    "            outputs.append(x.reshape(-1, X.shape[1]))\n",
    "        return np.stack(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_other = RobustScaler()\n",
    "scaler_other.fit(X_other_train)\n",
    "scaler_jet = JetScaler()\n",
    "scaler_jet.fit(X_jet_train)\n",
    "X_other[X_other == -999] = 0\n",
    "X_other_scaled = scaler_other.transform(X_other_train)\n",
    "X_other_test_scaled = scaler_other.transform(X_other_test)\n",
    "X_jet_scaled = scaler_jet.transform(X_jet_train)\n",
    "X_jet_test_scaled = scaler_jet.transform(X_jet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with permutation invariant jet embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseB(*args, **kwargs):\n",
    "    def f(inp):\n",
    "        out = inp\n",
    "        out = Dense(*args, **kwargs)(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        return out\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    inp_jets = tf.keras.layers.Input(shape=(2, 3), name=\"jets\")\n",
    "    jets = inp_jets\n",
    "    inp_other = tf.keras.layers.Input(shape=(X_other_train.shape[1],), name=\"other\")\n",
    "    other = inp_other\n",
    "    other = DenseB(100, activation=\"relu\")(other)\n",
    "    other = DenseB(100, activation=\"relu\")(other)\n",
    "    #other = Dense(100, activation=\"relu\")(other)\n",
    "    jets = DenseB(100, activation=\"relu\")(jets)\n",
    "    jets = DenseB(100, activation=\"relu\")(jets)\n",
    "    #jets = Dense(100, activation=\"relu\")(jets)\n",
    "    mask = tf.keras.layers.Lambda(lambda x: tf.expand_dims(tf.cast(tf.reduce_all(x != -999, axis=2), tf.float32), axis=2))(inp_jets)\n",
    "    jets = tf.keras.layers.multiply([mask, jets])\n",
    "    jets = tf.keras.layers.GlobalAveragePooling1D()(jets)\n",
    "    out = tf.keras.layers.concatenate([jets, other])\n",
    "    out = DenseB(100, activation=\"relu\")(out)\n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "    return tf.keras.Model(inputs=[inp_jets, inp_other], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # or weighted metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy']) # or weighted metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: y_train.shape[0]/backtrain, 1:y_train.shape[0]/sigtrain}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_tot = np.array(weight_train*np.array(list(class_weight.values()))[y_train.astype(int)])\n",
    "weight_test_tot = np.array(weight_test*np.array(list(class_weight.values()))[y_test.astype(int)])\n",
    "weight_train_tot /= weight_train_tot.mean()\n",
    "weight_test_tot /= weight_test_tot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "history = model.fit(\n",
    "    #X_train_scaled,\n",
    "    {\"jets\": X_jet_scaled, \"other\": X_other_scaled},\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    sample_weight=weight_train_tot,\n",
    "    #validation_data=(X_test_scaled, y_test, weight_test_tot),\n",
    "    validation_data=({\"jets\": X_jet_test_scaled, \"other\": X_other_test_scaled}, y_test, weight_test_tot),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(verbose=True, patience=20, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training history returned by model.fit\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "#y_test_prob_keras = model.predict(X_test_scaled)[:, 0]\n",
    "y_train_prob_keras = model.predict({\"jets\": X_jet_scaled, \"other\": X_other_scaled})[:, 0]\n",
    "y_test_prob_keras = model.predict({\"jets\": X_jet_test_scaled, \"other\": X_other_test_scaled})[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the AMS scan\n",
    "from sklearn.metrics import roc_curve\n",
    "def ams_scan(y, y_prob, weights, label):\n",
    "    fpr, tpr, thr = roc_curve(y, y_prob, sample_weight=weights)\n",
    "    ams_vals = ams(tpr * sigall, fpr * backall)\n",
    "    print(\"{}: Maximum AMS {:.3f} for pcut {:.3f}\".format(label, ams_vals.max(), thr[np.argmax(ams_vals)]))\n",
    "    return thr, ams_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob_keras, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob_keras, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0.8, 1.)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}