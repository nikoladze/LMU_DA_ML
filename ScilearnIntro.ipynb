{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Scikit-Learn-Introduction\" data-toc-modified-id=\"Scikit-Learn-Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Scikit-Learn Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-visualization\" data-toc-modified-id=\"Data-visualization-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data visualization</a></span></li><li><span><a href=\"#Data-preparation\" data-toc-modified-id=\"Data-preparation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data preparation</a></span></li><li><span><a href=\"#Apply-model-and-make-predictions\" data-toc-modified-id=\"Apply-model-and-make-predictions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Apply model and make predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#test/evaluate-model\" data-toc-modified-id=\"test/evaluate-model-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>test/evaluate model</a></span></li></ul></li><li><span><a href=\"#Measure---Quality-of-Classification\" data-toc-modified-id=\"Measure---Quality-of-Classification-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Measure   Quality of Classification</a></span></li><li><span><a href=\"#Further-simple-models\" data-toc-modified-id=\"Further-simple-models-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Further simple models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gaussian-Naive-Bayes\" data-toc-modified-id=\"Gaussian-Naive-Bayes-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Gaussian Naive Bayes</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Probabilistic-Classification\" data-toc-modified-id=\"Probabilistic-Classification-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Probabilistic Classification</a></span></li></ul></li><li><span><a href=\"#Two-short-examples--on-unsupervised-ML\" data-toc-modified-id=\"Two-short-examples--on-unsupervised-ML-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Two short examples  on unsupervised ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Dimensionality Reduction</a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Clustering</a></span></li></ul></li><li><span><a href=\"#Clustering-applied-to-digit-data\" data-toc-modified-id=\"Clustering-applied-to-digit-data-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Clustering applied to digit data</a></span></li><li><span><a href=\"#Image-data-with-sklearn:\" data-toc-modified-id=\"Image-data-with-sklearn:-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Image data with sklearn:</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>PCA</a></span></li><li><span><a href=\"#Test-Isomap\" data-toc-modified-id=\"Test-Isomap-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Test Isomap</a></span></li><li><span><a href=\"#Digit-classification\" data-toc-modified-id=\"Digit-classification-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Digit classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-kNN:\" data-toc-modified-id=\"First-kNN:-1.8.3.1\"><span class=\"toc-item-num\">1.8.3.1&nbsp;&nbsp;</span>First kNN:</a></span></li><li><span><a href=\"#Then--Gaussian-Naive-Bayes:\" data-toc-modified-id=\"Then--Gaussian-Naive-Bayes:-1.8.3.2\"><span class=\"toc-item-num\">1.8.3.2&nbsp;&nbsp;</span>Then  Gaussian Naive Bayes:</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Introduction\n",
    "A number of Python packages provide implementations of  machine learning algorithms. \n",
    "**[Scikit-Learn](http://scikit-learn.org)** is one of the most popular.\n",
    "* it provides many of the common ML algorithms\n",
    "* well-designed, uniform API (programming interface)\n",
    "  * standardized and largely streamlined setup of the different models   \n",
    "    &rarr; easy to switch\n",
    "* good documentation\n",
    "\n",
    "The first example is based on the **[Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)**. This had already been introduced by famous statistician\n",
    "Ronald Fisher in 1936 and is used since then as instructive use case for classification. \n",
    "The data consists of\n",
    "* measurements of length and width of both sepal (Bl&uuml;tenkelch) and petal (Bl&uuml;te) \n",
    "* classification of Iris sub-species\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaboorn provides easy way to import iris dataset as pandas dataframe\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "First step should always be some investigation of data properties, i.e.\n",
    "* basic statistical properties\n",
    "* visualization of distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics with pandas\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of single feature\n",
    "sns.histplot(data=iris,x='sepal_length',hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined plot of 2 features\n",
    "sns.jointplot(data=iris,x='sepal_length',y='sepal_width', hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined plot matrix of all features in dataframe\n",
    "#\n",
    "# will provide scatter plot of all combinations of numerical columns in dataframe\n",
    "# target (=species) can be given and will cause different colors\n",
    "sns.pairplot(iris, hue='species', diag_kind='hist', height=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "For use in sklearn with  **supervised learning** the first step is always to split  data into \n",
    "* table/matrix of **features**\n",
    "* list of **targets**\n",
    "\n",
    "And then split the data into **train** and **test** sample:\n",
    "* `train_test_split` function from sklearn\n",
    "* by default 75% for training and 25% for test and validation\n",
    "  * can be specified as parameter\n",
    "* randomized selection of entries  \n",
    "&rarr; inital order does not matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature matrix\n",
    "X=iris.loc[:,'sepal_length':'petal_width']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "Y=iris.species\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break-up in train & test sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "#knn.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy iris\n",
    "#X_new = np.array([[5, 4.9, 4, 1.2]])\n",
    "# recent version want same datatype for testing\n",
    "X_new = pd.DataFrame(np.array([[5, 4.9, 4, 1.2]]),columns=X.columns)\n",
    "# 2D format required, nrows vs ncolums (1x2)\n",
    "X_new.shape #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X_new) # apply model to new data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test/evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X)\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test==y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scilearn function for score\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(Y, y_pred)\n",
    "print(\"Test set score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Further useful checks are the **classification report** and the **confusion matrix**,  \n",
    "they give detailed Info on mis-classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The meaning of `recall` etc. will be explained in a bit.) \n",
    "\n",
    "Another intuitive measure is the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = np.unique(y_test)\n",
    "mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "print (labels, '\\n', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Repeat with different settings for number of neighbors**\n",
    "\n",
    "**Usually high accuracy for Iris data**  \n",
    "as scatter plot suggested there is rather clear separation between species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Measure   Quality of Classification\n",
    "\n",
    "The above `classification_report` presented several parameters which are useful to quantify how well the qualification works.\n",
    "\n",
    "For these we need to introduce the following terms (assuming a classification with two classes *P* and *N*)\n",
    "* $t_p = $ true-positive: number of cases with predicted *P* and correct *P*\n",
    "* $t_n = $ true-negative: number of cases with predicted *N* and correct *N*\n",
    "* $f_p = $ false-positive: number of cases with predicted *P* and correct *N*\n",
    "* $f_n = $ false-negative: number of cases with predicted *N* and correct *P*\n",
    "![Confusion matrix](./figures/wikipedia_confusion_matrix.png \"More details: see Wikipedia article on confusion matrix\")\n",
    "\n",
    "\n",
    "Based on these, the parameters in the `classification_report`  are defined as:\n",
    "* `precision` (or `purity`): $ t_p / ( t_p + f_p ) $ , i.e. fraction of cases classified as *P* which are true *P*\n",
    "* `recall` (or `efficiency`): $ t_p / ( t_p + f_n ) $ , i.e. fraction of true *P* which are classified as *P*\n",
    "* `f1-score` : Mean of `precision` and `recall`\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Precision_and_recall for a more detailed discussion\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Further simple models\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "Also a conceptually simple model\n",
    "* basic assumption is that for each different category (*Iris-species*) the variables follow a Gaussian distribution.\n",
    "* In training the model determines parameters of these Gaussians\n",
    "* For classification then simply calculate probability of a given new Iris-data to be of species `i` based on Gaussian probability:\n",
    "$$ P(x) = \\frac{1}{{\\sigma_i \\sqrt {2\\pi } }}e^{{{ - \\left( {x - \\mu_i } \\right)^2 } \\, } \\left/ \\right. {\\, {2\\sigma_i ^2 }}}$$\n",
    "* where $\\mu_i$ and $\\sigma_i$ are mean and standarddeviation for respecitve variable and species `i`\n",
    "\n",
    "(We'll look at why it's called \"Bayes\" in a bit more detail [here](http://localhost:8888/notebooks/Higgs-Gaussian.ipynb#GaussianNB).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(X_train, y_train)                # 3. fit model to data\n",
    "y_gnb = model.predict(X_test)              # 4. predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scilearn function for score\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_gnb, y_test)\n",
    "print(\"Test set score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_gnb, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_gnb, labels=labels)\n",
    "print (labels,'\\n', mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "This method is similar to standard liner regression. However, it can be used for discrete dependent variables, i.e. classification use-cases.\n",
    "\n",
    "More info e.g. https://en.wikipedia.org/wiki/Logistic_regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 1. choose model class\n",
    "model = LogisticRegression(max_iter=500)            # 2. instantiate model\n",
    "model.fit(X_train, y_train)                         # 3. fit model to data\n",
    "y_lr = model.predict(X_test)                        # 4. predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scilearn function for score\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_lr, y_test)\n",
    "print(\"Test set score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Probabilistic Classification\n",
    "\n",
    "In general models can not only be used to give single classification as in above examples but one can also get a list of probabilities for the different possible outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yout=model.predict_proba(X_test)\n",
    "print (yout[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout[y_lr != y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(y_lr[:5],y_test[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on type of problem this information can be further used to distinguish clear cases and those with overlapping classifications.\n",
    "\n",
    "Or one can use it to adjust trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Two short examples  on unsupervised ML\n",
    "\n",
    "### Dimensionality Reduction\n",
    "The Iris data is also a good show case for unsupervised learning.  \n",
    "A common problem is **dimensionality reduction**, i.e. check if there is a lower dimensional representation which retains the essential features.\n",
    "* In case of Iris data there are four feature dimensions\n",
    "* scatter plot showed clear correlations between features\n",
    "  * indication that less dimensions might be sufficient\n",
    "  \n",
    "One standard method is principal component analysis (PCA), which can be applied in case of (reasonably) linear correlations.\n",
    "\n",
    "As before we have to do the usual scikit steps:\n",
    "* Setup PCA model with 2 dimensions\n",
    "* fit/train\n",
    "* get reduced dimensions as output of transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n",
    "model.fit(X)                           # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(X)              # 4. Transform the data to two dimensions\n",
    "X_2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", hue='species', data=iris, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the coefficients of the PCA transformation using the `model.components_` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(model.components_)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X.columns)), X.columns)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we plot the correlation like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"PCA1\", y=\"petal_width\", hue='species', data=iris, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Redo the kNN classification using the 2 PCA Iris components and compare with the original kNN using all 4 Iris features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "A clustering algorithm attempts to find distinct groups of data without reference to any labels.\n",
    "One powerful method is Gaussian mixture model (GMM) *(Details see Data Science Handbook: 05.12-Gaussian-Mixtures.ipynb)*  \n",
    "A GMM attempts to model the data as a collection of Gaussian blobs.\n",
    "\n",
    "We can fit the Gaussian mixture model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture       # 1. Choose the model class\n",
    "#\n",
    "model =  GaussianMixture(n_components=3,\n",
    "                         covariance_type='full')  # 2. Instantiate the model with hyperparameters\n",
    "\n",
    "model.fit(X)                                      # 3. Fit to data. Notice y is not specified!\n",
    "y_gmm = model.predict(X)                          # 4. Determine cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['cluster'] = y_gmm\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", data=iris, hue='species',\n",
    "           col='cluster', fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot PCA data for each identified cluster  \n",
    "Indicates good clustering, basically identical to species.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Clustering applied to digit data\n",
    "\n",
    "Another classic example case for ML is handwritten digits data.\n",
    "\n",
    "A suitable dataset is included with sklearn, first we look into it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sklearn specific container, basically a list of 8x8 pixels images\n",
    "\n",
    "We plot a sub-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot shows pixel image together with label (in green).\n",
    "\n",
    "* Some images are obvious\n",
    "* Others seem difficult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at data from 1st image --> 2D table resembles 0\n",
    "print (digits.images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data with sklearn:\n",
    "To use the data with sklearn as before we need a 2D structure: `samples x features` , i.e. the 8x8 images should be transformed into flat 1x64 array.   \n",
    "\n",
    "Already provided in Dataset, element `data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use as before just re-label\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try PCA\n",
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n",
    "model.fit(X)                           # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(X)              # 4. Transform the data to two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now reduced 64 to 2 dimensions  \n",
    "&rarr; visualize it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout=pd.DataFrame()\n",
    "xout['tag']=y\n",
    "xout['PCA1'] = X_2D[:, 0]\n",
    "xout['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(x=\"PCA1\", y=\"PCA2\", hue='tag', data=xout, fit_reg=False, markers='.');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some digits are nicely isolated, others less so\n",
    "\n",
    "Think about it, which digits tend to look similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the *principle components* that the PCA has extracted from the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA components of digits\n",
    "fig, ax = plt.subplots(1, 2, subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for idx, comp in enumerate(model.components_):\n",
    "    img = comp.reshape(8,8)\n",
    "    ax.ravel()[idx].imshow(img, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left image shows the most important, the right image the second-most important component extracted by the PCA.\n",
    "Compare this to the previous plot to see that this actually makes sense: \n",
    "* If you focus on the blue (\"negative\") pixels in the left image, those resemble the digit \"3\". Indeed, from the previous plot we see that the figures 3 cluster at low values of PCA1 (and around 0 for PCA2, i.e. they typically have not much of the second component in the right image). \n",
    "* The red in the left image could be part of a \"4\" which indeed has high values for PCA1.\n",
    "* Similarly, the red in the right image is somewhat the outline of a \"0\" which has large positive values for PCA2 (and small absolute values for PCA1).\n",
    "\n",
    "Can you find the digit \"1\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Isomap \n",
    "Alternative method for dimension reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "XI_2D = iso.transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XI_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout_iso=pd.DataFrame()\n",
    "xout_iso['tag']=y\n",
    "xout_iso['ISO1'] = XI_2D[:, 0]\n",
    "xout_iso['ISO2'] = XI_2D[:, 1]\n",
    "sns.lmplot(x=\"ISO1\", y=\"ISO2\", hue='tag', data=xout_iso, markers='.',fit_reg=False,\n",
    "           scatter_kws={'alpha':0.7});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation clearly better with that method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Digit classification\n",
    "Of course we want not just clustering but classification, so let's try our two models we had used before also on digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scilearn function for score\n",
    "from sklearn.metrics import accuracy_score\n",
    "ypred = knn.predict(Xtest)\n",
    "score = accuracy_score(ytest, ypred)\n",
    "print(\"Test set score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Detailed classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check confusion matrix**  \n",
    "very infomative for such a case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### kNN performs really well!\n",
    "\n",
    "***\n",
    "#### Then  Gaussian Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_model, ytest)\n",
    "print(\"Test set score: {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ytest, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest, y_model)\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GNB significantly worse, many more mis-ids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
