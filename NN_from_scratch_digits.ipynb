{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the code from chapter 18 of the book, \"Data Science from Scratch\" by Joel Grus, available on [github][1].\n",
    "\n",
    "[1]: https://github.com/joelgrus/data-science-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network will be simple: It will consist of 25 input features (pixels), a hidden layer with 4 neurons (configurable: `num_hidden` below) and an output layer with 10 neurons. The output of the neurons 0-9 in the output layer can be interpreted as a probability that the input is classified as the digit 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def argmax(l):\n",
    "    return l.index(max(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for evaluating the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input (always 1)\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    # outputs = two arrays (one array of size 4 for the hidden layer plus one array of size 10 for the output layer)\n",
    "    return outputs \n",
    "\n",
    "def predict(network, input):\n",
    "    \"\"\"run input through the network and return output of last layer\"\"\"\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function for backpropagation that we'll need to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # compute the delta (error term) of the output layer\n",
    "    output_deltas = [output * (1 - output) * (output - target[i]) # (1)\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # back-propagate errors to hidden layer: compute the delta (error term) of the hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                     dot(output_deltas, [n[i] for n in network[-1]]) # (2)\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    \n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]): # loop over weights of neurons in output layer\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]): # loop over output of neurons in hidden layer + bias\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output # (3)\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]): # loop over weights of neurons in hidden layer\n",
    "        for j, input in enumerate(input_vector + [1]): # loop over output of neurons in first layer, i.e. the inputs + bias\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two lines marked with **(1)** and **(3)** we rediscover\n",
    "$$ \\Delta w_{ij} = - \\eta \\frac{\\partial E}{\\partial w_{ij}} = - (\\hat y - y) \\varphi'(\\text{net}_j) x_i,$$\n",
    "which was introduced in the [short introduction][1] on backpropagation. \n",
    "\n",
    "[1]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **(1)** we compute $(\\hat y - y) \\varphi'(\\text{net}_j)$ with a learning rate $\\eta$ of $\\frac12$. The term $\\hat y - y$ is `output - target[i]` for each target output value (activation) of the neurons in the last layer (`outputs`) and the vector of target values (`target`), and $\\varphi'$ is the derivative of the sigmoid function that we use as activation function of the output layer, i.e. $\\varphi'(x) = \\varphi(x) (1 - \\varphi(x))$ or `output * (1 - output)` in code.\n",
    "\n",
    "Finally, in **(3)** this is just multiplied by $x_i$ which is `hidden_output`, an entry in the vector of output values of the hidden layer (`hidden_outputs`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand equation **(2)** in the above code, which has not been spelled out in the [introduction][2], look again at the [Backpropagation Algorithm][1], which specifies the error terms $\\delta_j^k$ for neuron $j$ in hidden layer $k$ as\n",
    "\n",
    "$$\\delta_j^k = g'(a_j^k) \\sum_{l=1}^{r^{k+1}} w_{jl}^{k+1}\\delta_l^{k+1}$$\n",
    "\n",
    "In our case, $k$ is our (single) hidden layer and $k+1$ is the output layer. The values $a_j^k$ in this notation are the outputs of the hidden layer.\n",
    "$g'(x)$ is again the derivative of the sigmoid. $r^{k+1}$ is the number of nodes in the output layer and the sum corresponds to the `dot` product multiplying the `output_deltas` ($\\delta_l^{k+1}$) and the weights `network[-1]` of the output layer ($w_{jl}^{k+1}$).\n",
    "\n",
    "[1]: https://brilliant.org/wiki/backpropagation/\n",
    "[2]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that **(3)** comes after **(2)**, i.e. we update the weights of the output neurons after computing the corrections for the hidden neurons. `network[-1]` is `output_layer`, i.e. changing `output_neuron` in **(3)** would otherwise affect the computation in **(2)** because the weights `n` would already be the updated ones.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stylized figures that will serve as inputs to train on (we only have one training set here with one input data per label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "   0, \"\"\"11111\n",
    "         1...1\n",
    "         1...1\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   1, \"\"\"..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\n",
    "         ..1..\"\"\",\n",
    "\n",
    "   2, \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         1....\n",
    "         11111\"\"\",\n",
    "\n",
    "   3, \"\"\"11111\n",
    "         ....1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "   4, \"\"\"1...1\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "   5, \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\",\n",
    "\n",
    "   6, \"\"\"11111\n",
    "         1....\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   7, \"\"\"11111\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\n",
    "         ....1\"\"\",\n",
    "\n",
    "   8, \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         1...1\n",
    "         11111\"\"\",\n",
    "\n",
    "   9, \"\"\"11111\n",
    "         1...1\n",
    "         11111\n",
    "         ....1\n",
    "         11111\"\"\"]\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs (pixel images), labels and targets (= one-hot labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = list(map(make_digit, raw_digits[1::2]))\n",
    "labels  = raw_digits[0::2]\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "           for j in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ten output neurons, so we cannot directly use the labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to train but need to convert these into a \"one-hot encoding\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network structure and initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)    # to get repeatable results\n",
    "input_size  = 25  # each input is a vector of length 25 (25 pixels)\n",
    "num_hidden  =  4  # number of neurons in the hidden layer\n",
    "output_size = 10  # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the training using the backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,000 iterations seems enough to converge\n",
    "for x in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "    if x % 1000 == 0:\n",
    "        accuracy = sum([argmax(predict(network, input)) == label for input, label in zip(inputs, labels)])\n",
    "        print(\"Iterations done: %d, accuracy: %.2f\" % (x, accuracy / len(inputs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the probabilities of the labels the network predicts on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []\n",
    "for i, input in enumerate(inputs):\n",
    "    outputs = predict(network, input)\n",
    "    print(i, [round(p,2) for p in outputs])\n",
    "    m.append(outputs)\n",
    "\n",
    "# This is not a confusion matrix.\n",
    "plt.imshow(m, plt.cm.Blues);\n",
    "plt.xlabel(\"Probability for label\")\n",
    "plt.ylabel(\"True label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the performance on some input the network has not seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([round(x, 2) for x in\n",
    "      predict(network,\n",
    "                [0,1,1,1,0,    # .@@@.\n",
    "                 0,0,0,1,1,    # ...@@\n",
    "                 0,0,1,1,0,    # ..@@.\n",
    "                 0,0,0,1,1,    # ...@@\n",
    "                 0,1,1,1,0])]) # .@@@.\n",
    "\n",
    "print([round(x, 2) for x in\n",
    "      predict(network, \n",
    "                [0,1,1,1,0,    # .@@@.\n",
    "                 1,0,0,1,1,    # @..@@\n",
    "                 0,1,1,1,0,    # .@@@.\n",
    "                 1,0,0,1,1,    # @..@@\n",
    "                 0,1,1,1,0])]) # .@@@.\n",
    "\n",
    "print([round(x, 2) for x in\n",
    "      predict(network, \n",
    "                [1,1,1,1,1,    # @@@@@\n",
    "                 0,0,0,0,1,    # ....@\n",
    "                 0,0,0,1,0,    # ...@.\n",
    "                 0,0,1,0,0,    # ..@..\n",
    "                 0,0,1,0,0])]) # ..@.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the network ist completely overtrained: It's perfect on the training data but generalizes very badly, even though the first two figures are still properly classified. In reality we'd want a lot more input samples for the training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Show the weights the network has learned for each of the five hidden neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_weights(neuron_idx, ax):\n",
    "    weights = network[0][neuron_idx]\n",
    "\n",
    "    grid = [weights[row:(row+5)]      # turn the weights into a 5x5 grid\n",
    "            for row in range(0, input_size, 5)] # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "    pos = ax.imshow(grid,\n",
    "                    cmap=matplotlib.cm.coolwarm,\n",
    "                    interpolation='none', # plot blocks as blocks\n",
    "                    vmin = -8, vmax = 8) # define a unique range for all subplots\n",
    "    \n",
    "    # print bias\n",
    "    ax.set_xlabel(\"bias = %.2f\" % weights[input_size])\n",
    "    return pos\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3), ncols=num_hidden)\n",
    "for idx in range(num_hidden):\n",
    "    pos = show_weights(idx, ax[idx])\n",
    "    #fig.colorbar(pos, ax = ax[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(blue = large negative, red = large positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(output_layer, cmap=matplotlib.cm.coolwarm)\n",
    "plt.xlabel(\"Weight of hidden neuron (and bias)\")\n",
    "plt.ylabel(\"Output label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(blue = large negative, red = large positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it discriminates e.g. 0 and 8 or 5 and 9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summed response of neurons in hidden layer to individual pixels (w/o bias) weighted by the weights of the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawNNView(idx, ax):\n",
    "    sum_weights = [sum([\n",
    "                     sigmoid(network[0][neuron_idx][i])*output_layer[idx][neuron_idx] for neuron_idx in range(num_hidden)\n",
    "                   ])\n",
    "                   for i in range(input_size)\n",
    "                  ]\n",
    "    grid = [sum_weights[row:(row+5)] for row in range(0, input_size, 5)]\n",
    "\n",
    "    pos = ax.imshow(grid,\n",
    "                    cmap=matplotlib.cm.coolwarm,\n",
    "                    interpolation='none',  # plot blocks as blocks\n",
    "                    vmin = -40, vmax = 40) # define a unique range for all subplots\n",
    "    ax.set_xlabel(\"output node %d\" % idx)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6), ncols = 5, nrows = 2)\n",
    "for idx in range(10):\n",
    "    pos = DrawNNView(idx, ax.flatten()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}