{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771cd86-26e0-48af-acdc-284fe9f76bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run clone_git_on_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00390d6d-b881-4848-b5e6-582cc5434d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_settings import set_tf_nthreads\n",
    "set_tf_nthreads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22b861-fc2f-4a0d-a8cd-78c0813c8d53",
   "metadata": {},
   "source": [
    "# Deep sets and graph networks\n",
    "\n",
    "The ML models we have looked at so far make the assumption that we have a fixed-dimensional vector of input features. In reality that might not always be the case. Some examples:\n",
    "\n",
    "* Sequences (text, audio, video)\n",
    "* Point clouds (e.g. points in 3D space)\n",
    "* Lists of objects (e.g. particles in a collision)\n",
    "* Graphs with different numbers of nodes and different numbers of connections for each node\n",
    "\n",
    "For sequences one approach are recurrent neural networks (RNNs) that utilize a state that gets updated as it iteratively processes input. However, these still need a defined ordering of the inputs and they have certain disadvantages (most prominently difficulty to model \"long-range\" correlations between inputs and difficulty to parallelize since they are sequential in nature).\n",
    "\n",
    "Another approach are models that apply **permutation invariant** transformations on the inputs. Both deep sets and graph networks make use of this. The nowadays (2023) also very popular [**transformers**](https://arxiv.org/abs/1706.03762) can be viewed as graph networks where all nodes are connected to each other.\n",
    "\n",
    "## Deep sets\n",
    "\n",
    "The simplest approach for a permutation invariant transformation is a **per-point transformation** ($\\phi$) followed by a **permutation invariant aggregation**, typically taking the sum/mean or min/max whose output can then be transformed ($\\rho$) by any means, e.g. another MLP.\n",
    "\n",
    "![](figures/deep_set_transformation.png)\n",
    "\n",
    "See [arXiv:1703.06114](https://arxiv.org/abs/1703.06114) for a detailed discussion.\n",
    "\n",
    "\n",
    "### Application to jets in Higgs dataset\n",
    "\n",
    "Remember the missing values in the dataset for the [HiggsChallenge](HiggsChallenge.ipynb)? Those occurred since we had a non-fixed length list of jets in each event (0, 1 or 2). Maybe we can embed the jets into a fixed length vector using a permutation invariant transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc7889-701f-4752-8909-91e59b083552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, Masking\n",
    "from tensorflow.keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98d8b8-bbc2-4c0f-bbfc-98dced5c512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')\n",
    "n_sig_tot = df[\"Weight\"][df.Label == \"s\"].sum()\n",
    "n_bkg_tot = df[\"Weight\"][df.Label == \"b\"].sum()\n",
    "# comment this out if you want to run on the full dataset\n",
    "df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f847742-6b7d-4c5c-8884-55bcd33f3810",
   "metadata": {},
   "source": [
    "First, we separate the jet features and other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2220d1-bf1a-4c08-8497-65b4904c8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_cols = sum([[f\"PRI_{obj}_{field}\" for field in [\"pt\", \"eta\", \"phi\"]] for obj in [\"jet_leading\", \"jet_subleading\"]], [])\n",
    "jet_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51009147-4bf2-4848-8662-a688f23f150c",
   "metadata": {},
   "source": [
    "We also exclude variables that are derived from the jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d4758-7d2f-4e99-93a5-4c821f18ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = ['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48e87a-94fc-4533-bf86-b46b456bfaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_cols = [col for col in df.columns if (col.startswith(\"PRI\") or col.startswith(\"DER\")) and col not in jet_cols and not col in excluded_cols]\n",
    "other_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf5635-eee9-4310-9d76-73d7c9b312e4",
   "metadata": {},
   "source": [
    "We will make the jet features a 3-D array of shape `(nevents, max_njets, n_jet_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a7e61-d1de-4bdc-8615-18888bafe228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_jet = df[jet_cols].to_numpy().reshape(-1, 2, 3)\n",
    "X_jet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f36fca-4a81-4201-bb32-c3a33d213a48",
   "metadata": {},
   "source": [
    "The rest of the features just stays a 2-D array as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fa2a0-ac99-4a65-898d-5b8186ecce89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_other = df[other_cols].to_numpy()\n",
    "X_other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d79c2-9788-4ecc-b596-182edb3eea80",
   "metadata": {},
   "source": [
    "Still we need to replace missing values by 0 which can occur for the quantity `DER_mass_MMC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf364f5-bc2e-47e8-8ddf-c34c250c9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other[X_other == -999] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b17cb8-10cf-46de-88ff-da00c24f4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df.Label == \"s\").to_numpy()\n",
    "weight = df['Weight'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfd999-fd16-4294-b540-83ff4a5dc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_jet_train, X_jet_test,\n",
    "    X_other_train, X_other_test,\n",
    "    y_train, y_test,\n",
    "    weight_train, weight_test,\n",
    ") = train_test_split(X_jet, X_other, y, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947b1c2-aec6-4b36-acc3-eeee6c3f261c",
   "metadata": {},
   "source": [
    "Now, let's scale the features. For the jets we have to be a bit careful only to consider non-missing values in the scaling. Also the scikit-learn scalers can only deal with 2D arrays - so let's define a custom scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e4d37-3a2f-481b-a94d-17da73b578b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetScaler:\n",
    "    def __init__(self, mask_value=-999):\n",
    "        self.mask_value = mask_value\n",
    "        self.scaler = RobustScaler()\n",
    "    \n",
    "    def fill_nan(self, X):\n",
    "        \"replace missing values by nan\"\n",
    "        X[(X == self.mask_value).all(axis=-1)] = np.nan\n",
    "        \n",
    "    def fit(self, X):\n",
    "        X = np.array(X) # copy\n",
    "        self.fill_nan(X)\n",
    "        X = X.reshape(-1, X.shape[-1]) # make 2D\n",
    "        self.scaler.fit(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        orig_shape = X.shape\n",
    "        X = np.array(X).reshape(-1, X.shape[-1])\n",
    "        self.fill_nan(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        X = np.nan_to_num(X, 0) # replace missing values by 0\n",
    "        return X.reshape(*orig_shape) # turn back into 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47e0d-3bdb-4898-b04f-de4a1b47e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_scaler = JetScaler()\n",
    "jet_scaler.fit(X_jet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f100494-e26b-40b3-8606-932277a5fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet_train_scaled = jet_scaler.transform(X_jet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57a3a-6c19-42ed-a0a7-f61a4dfcbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_scaler = RobustScaler()\n",
    "other_scaler.fit(X_other_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bd55d-4841-4096-a013-f9b0043e1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other_train_scaled = other_scaler.transform(X_other_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3a649-033b-436b-8485-2e9bc79337dd",
   "metadata": {},
   "source": [
    "Also we again balance the weights to have the same sum of weights for signal and background and average weight 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f3265-fea2-43dd-99b7-874e433612d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_signal = 1 / weight_train[y_train==1].sum()\n",
    "class_weight_background = 1 / weight_train[y_train==0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd0af4-648f-4ffb-bcee-fc706eaed809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weight(weight, y):\n",
    "    weight = np.array(weight)\n",
    "    weight[y==0] *= class_weight_background\n",
    "    weight[y==1] *= class_weight_signal\n",
    "    return weight / weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ab59e-37a1-4620-8d56-832824f92a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_scaled = transform_weight(weight_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87821988-4d9c-4b23-ae00-2c8574bbe6fa",
   "metadata": {},
   "source": [
    "Now the model - we use the functional API of keras\n",
    "\n",
    "**Note:** When applying the keras `Dense` layer to 3D arrays it is applied independently on each element along the second dimension This is precisely what we want for our per-point transformation $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a2cc2-7263-494e-adf3-d463641e1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    input_jets = Input(shape=(2, 3), name=\"jets\")\n",
    "    jets = input_jets\n",
    "    input_other = Input(shape=(X_other_train.shape[1],), name=\"other\")\n",
    "\n",
    "    # embed the jets using 3 hidden layers (shared per-jet)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    \n",
    "    # take the mean/average as a permutation invariant operation\n",
    "    # note: since we still process a sequence of fixed length 2 this could in priniciple receive contributions\n",
    "    # from non-existing jets if the NN encodes the 0s into a non-zero vector.\n",
    "    # We could use a Masking Layer, but that has problems (produces NaN) when the sequence is completely empty\n",
    "    # so we would need something custom which we don't do here (seems to still work reasonably well)\n",
    "    jets = tf.keras.layers.GlobalAveragePooling1D()(jets)\n",
    "    \n",
    "    # 3 hidden layers for the other features\n",
    "    other = input_other\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    \n",
    "    # concatenate embedded jets and other features and add final hidden layer + output\n",
    "    out = tf.keras.layers.concatenate([jets, other])\n",
    "    out = Dense(100, activation=\"relu\")(out)\n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_jets, input_other], outputs=[out])\n",
    "\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29829073-913e-47a0-bdd4-752b81ef1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4e457-3990-4a64-9a0e-f51016afaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69f375-fd14-4817-b30d-5e1b9160844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343cb08-22ff-4b32-8203-0fca305aebfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    {\"jets\": X_jet_train_scaled, \"other\": X_other_train_scaled},\n",
    "    y_train,\n",
    "    sample_weight=weight_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[history],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05e9f6-9304-4afa-b03b-9d941ee2aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c7644-6075-423f-8af5-8652f457df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet_test_scaled = jet_scaler.transform(X_jet_test)\n",
    "X_other_test_scaled = other_scaler.transform(X_other_test)\n",
    "weight_test_scaled = transform_weight(weight_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dca49-d683-4153-932d-4f6e4523d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict({\"jets\": X_jet_train_scaled, \"other\": X_other_train_scaled}, verbose=True)[:, 0]\n",
    "y_pred_test = model.predict({\"jets\": X_jet_test_scaled, \"other\": X_other_test_scaled}, verbose=True)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660f40d-7931-4f29-b558-bf49a165d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5402d3-173d-4437-bc46-e20e359d3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools import ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1546f9b-ac95-4b38-8dc9-5ee9596872c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ams_scan(y, y_prob, weights, label):\n",
    "    fpr, tpr, thr = roc_curve(y, y_prob, sample_weight=weights)\n",
    "    ams_vals = ams(tpr * n_sig_tot, fpr * n_bkg_tot)\n",
    "    print(\"{}: Maximum AMS {:.3f} for pcut {:.3f}\".format(label, ams_vals.max(), thr[np.argmax(ams_vals)]))\n",
    "    return thr, ams_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a395510-66e0-4406-a687-2d0b97c17064",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_pred_train, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_pred_test, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0.8, 1.)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec74bb-9b7a-4e04-9f4b-e385a1dbe7d4",
   "metadata": {},
   "source": [
    "## Application to top-tagging dataset\n",
    "\n",
    "Sets are a nice representation for objects in particles physics. Let's apply this to the jet constituents of the dataset from the [CNNTopTagging](CNNTopTagging.ipynb) notebook.\n",
    "\n",
    "We have prepared a subset of this dataset in original form containing the 4-momenta (Energy, px, py, pz) of up to 200 jet constituents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff0931-ebfd-44f3-a2ce-eed37d28cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tagging_path = \"data/top_tagging_with_adjacency.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023b117-4b0c-4fce-ac21-d3c85be88078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(top_tagging_path):\n",
    "    import requests\n",
    "    url = \"https://cloud.physik.lmu.de/index.php/s/AtESAET6JK6DiWZ/download\"\n",
    "    res = requests.get(url)\n",
    "    with open(top_tagging_path, \"wb\") as f:\n",
    "        f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437033f8-998b-4bc8-bef4-79b41a5234bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = np.load(top_tagging_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60d426-667a-4fb4-a21b-839701014ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = npz_file[\"jet_4mom\"]\n",
    "y = npz_file[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ba794-8100-4b11-9f0c-4e18197939ee",
   "metadata": {},
   "source": [
    "Here, the missing values are filled with `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b16122-17f3-4d03-a33c-6edac2f6fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63d69e-d55e-459a-9b77-86a40e1fc02d",
   "metadata": {},
   "source": [
    "We can reuse the `JetScaler` we defined for the Higgs Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62045413-e53c-47e1-b084-7576584224fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = JetScaler(mask_value=0)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777f737-232d-43d1-9c87-e6b374d22ea1",
   "metadata": {},
   "source": [
    "Here we can use a simple Sequential stack of layers since we only use the jet constituents as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038076f-b7f5-4c5f-8f7f-e98519111306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Masking(input_shape=X_train.shape[1:]),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c840a8-8316-4a16-8992-5a0103f12bb2",
   "metadata": {},
   "source": [
    "Here we were able to use a [Masking](https://keras.io/guides/understanding_masking_and_padding/) layer since the sequence is never completely empty.\n",
    "\n",
    "Again, the first layers operate independently on each constituent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa24013-2b40-496e-8b5f-c6afd4ac0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49700597-ca35-4f52-b577-3ac19c35165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1cc63-ef20-46dc-9bb1-eb40235d3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e947af-0e58-4740-83bd-aadf8619aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    callbacks=[history],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93920d-2ce5-4e49-9b95-2cf44ecc1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25bd90-d11f-4b8e-95a0-fc263aa14dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60511f-1def-4b57-b350-6fa3cf84f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = roc_curve(y_test, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63c51b-1292-4510-bd3c-52d88f056036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_tagging_performance(fpr, tpr):\n",
    "    plt.plot(tpr, 1. / fpr)\n",
    "    plt.ylabel(\"QCD jet rejection\")\n",
    "    plt.xlabel(\"Top quark jet efficiency\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    print(\"Top quark jet selection efficiency at 10^3 QCD jet rejection: \", np.max(tpr[fpr < 0.001]))\n",
    "    print(\"QCD jet rejection at 30% Top quark jet efficiency: \", 1. / np.min(fpr[tpr > 0.3]))\n",
    "    \n",
    "plot_top_tagging_performance(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ce54e-afe2-4111-a836-6e8f979844a8",
   "metadata": {},
   "source": [
    "## Graph convolutions/Graph neural networks\n",
    "\n",
    "Similar to convolutional networks where we update the state of each pixel by aggregating over neigboring pixels we can perform a *graph convolution* by aggregating over neighboring nodes in a graph:\n",
    "\n",
    "![cnn vs gcn](figures/cnn_vs_gcn.jpg)\n",
    "\n",
    "(figure from https://zhuanlan.zhihu.com/p/51990489)\n",
    "\n",
    "In the \"Deep sets\" language such a graph convolution corresponds to a *permutation equivariant* tranformation of the set of nodes, since it also does not depend on the ordering if the aggregation is done in a permutation invariant way (e.g. sum/mean/min/max).\n",
    "\n",
    "A rather simple implementation is given by the update rule introduced in [arXiv:1609.02907](https://arxiv.org/abs/1609.02907)\n",
    "\n",
    "$ H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) $\n",
    "\n",
    "where $A$ is the *adjacency matrix*, $D$ the *degree matrix*,  $H^{(l)}$ the hidden state of layer $l$ and $W^{(l)}$ the weight matrix of the layer $l$. The tilde above $A$ and $D$ indicates that self-loops were added (all nodes are neighbors of themselves).\n",
    "\n",
    "An equivalent formulation is\n",
    "\n",
    "$ h_i^{(l+1)} = \\sigma\\left(\\sum\\limits_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h^{(l)}_j W^{(l)}\\right) $\n",
    "\n",
    "where $ \\mathcal{N(i)} $ is the set of neighbors of node $i$ and $c_{ij} = \\sqrt{N_i}\\sqrt{N_j}$ with $N_i$ being the number of neigbors of node $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470499fd-1c46-4c0f-ac61-3d27d9084a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(adj):\n",
    "    \"\"\"\n",
    "    calculate outer product of sqrt(degree vector) and multiply with adjaceny matrix\n",
    "    \n",
    "    this corresponds to the D^{1/2}AD^{1/2} normalization suggested in Kipf & Welling (arXiv:1609.02907)\n",
    "    \"\"\"\n",
    "    deg_diag = tf.reduce_sum(adj, axis=2)\n",
    "    deg12_diag = tf.where(deg_diag > 0, deg_diag ** -0.5, 0)\n",
    "    return (\n",
    "        tf.matmul(\n",
    "            tf.expand_dims(deg12_diag, axis=2),\n",
    "            tf.expand_dims(deg12_diag, axis=1),\n",
    "        )\n",
    "        * adj\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904fe3e-784b-442b-b70a-218d5b064d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Simple graph convolution. Should be equivalent to Kipf & Welling (arXiv:1609.02907)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(units)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat, adjacency = inputs\n",
    "        return self.activation(tf.matmul(normalize_adjacency(adjacency), self.dense(feat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71b908-a775-491c-81f0-a141d8957b5c",
   "metadata": {},
   "source": [
    "One question is now - what is the graph in our dataset? Since The CNN architecture worked well it would make sense to define the graph by taking a certain number of nearest neighbors in the $\\eta-\\phi$ plane that was previously also used to define the image pixels.\n",
    "We prepared adjacency matrices for 7 nearest neigbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca534a-465f-43a2-85b7-491632cbac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = np.load(top_tagging_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61f82c-6e00-4163-b0ff-4457695e1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = npz_file[\"jet_4mom\"]\n",
    "y = npz_file[\"y\"]\n",
    "A = npz_file[\"adj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77331119-5900-4ff0-9fb4-c244e4e8791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptetaphi(X):\n",
    "    px = X[..., 1]\n",
    "    py = X[..., 2]\n",
    "    pz = X[..., 3]\n",
    "    pt = np.hypot(px, py)\n",
    "    eta = np.arcsinh(pz / pt)\n",
    "    phi = np.arcsin(py / pt)\n",
    "    return np.stack([pt, eta, phi], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d0044-f86c-4cb8-a70e-aa6420f4604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x, a):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nconst = (~(a == 0).all(axis=-1)).sum()\n",
    "    x = x[:nconst]\n",
    "    x = ptetaphi(x)\n",
    "    plt.scatter(x[:, 1], x[:, 2], s=100)\n",
    "    for i in range(nconst):\n",
    "        for j in range(nconst):\n",
    "            if a[i, j] or a[j, i]:\n",
    "                plt.plot([x[i, 1], x[j, 1]], [x[i, 2], x[j, 2]], color=\"C0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba287b-ff68-453a-b342-7ab33b33140c",
   "metadata": {},
   "source": [
    "Let's plot a few random graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6680d2-87e6-49a6-a20a-641d02b48b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(X))\n",
    "plot_graph(X[i], A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67314797-44ec-4b18-ae8c-78a5a25b54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, y, A)\n",
    "scaler = JetScaler(mask_value=0)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede25380-9593-460f-86f3-ad92796d79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(units=100, num_nodes=200, num_features=4):\n",
    "    adjacency_input = Input(shape=(num_nodes, num_nodes), name='adjacency')\n",
    "    feature_input = Input(shape=(num_nodes, num_features), name='features')\n",
    "\n",
    "    # constituent-level transformations\n",
    "    p = feature_input\n",
    "    for i in range(3):\n",
    "        p = Dense(units, activation=\"relu\")(p)\n",
    "\n",
    "    for i in range(3):\n",
    "        p = GraphConv(units, activation=\"relu\")([p, adjacency_input])\n",
    "\n",
    "    x = GlobalAveragePooling1D()(p)\n",
    "\n",
    "    # event-level transformations\n",
    "    for i in range(3):\n",
    "        x = Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    return tf.keras.models.Model(\n",
    "        inputs=[adjacency_input, feature_input],\n",
    "        outputs=[output]\n",
    "    )\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23d5b3-b8cc-4924-b27d-10536750d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4b82f-2571-4419-be82-9697d56c0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7834ecc-5a7f-45e1-8654-2d09c26c836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b6674-3d8d-4595-9e7c-e878299b2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    {\"features\": X_train, \"adjacency\": A_train},\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    callbacks=[history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09607904-5cf7-4747-9499-e582ec257435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e7bd9-087a-4a5d-8fbc-644d75043ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict({\"features\": X_test, \"adjacency\": A_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438c750-f32b-45ab-87b0-ceddedd10cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = roc_curve(y_test, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421315a-028e-4e25-a325-162e3cbe3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_tagging_performance(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c06f37-64db-47a2-b3ed-6b11894bb657",
   "metadata": {},
   "source": [
    "Some Notes:\n",
    "\n",
    "- We made it quite hard here for the neural network by putting in really the raw 4-momentum information\n",
    "- Possible improvements:\n",
    "  - Go to the $\\eta-\\phi$ plane\n",
    "  - Transform coordinates to be relative to the jet center\n",
    "  - Use graph operations that depend on the distance between points instead of absolute position (e.g. [EdgeConv](https://arxiv.org/abs/1801.07829))\n",
    "  - just train longer and/or on more data (we only used 10k samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab21cd-ec23-430e-9852-1d4fe3ee1bce",
   "metadata": {},
   "source": [
    "# Further possibilities\n",
    "\n",
    "We only touched the surface of what is possible with graph neural networks. In general, you can have arbitrary update rules that update in each step features of Nodes (V), Edges (e) and global aggregated features (u). Everyone of these 3 categories can receive input from any of the others:\n",
    "\n",
    "![graph network general update rule](figures/graph-network.png)\n",
    "\n",
    "(figure from [arXiv:1806.01261](https://arxiv.org/abs/1806.01261))\n",
    "\n",
    "More info/tutorials:\n",
    "\n",
    "http://tkipf.github.io/graph-convolutional-networks/  \n",
    "https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html  \n",
    "https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GraphConv.html#\n",
    "\n",
    "For more advanced applications with graph neural networks have a look at specialized libraries:\n",
    "\n",
    "[Spektral (tensorflow)](https://graphneural.network/)  \n",
    "[DGL (mainly pytorch, but also tensorflow)](https://dgl.ai)  \n",
    "[PyTorch Geometric](https://pytorch-geometric.readthedocs.io)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "If you actually want to implement graph networks, better consult these instead of manually building them. The examples in this tutorial are meant for educational purposes!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
