{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771cd86-26e0-48af-acdc-284fe9f76bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run clone_git_on_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00390d6d-b881-4848-b5e6-582cc5434d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_settings import set_tf_nthreads\n",
    "set_tf_nthreads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22b861-fc2f-4a0d-a8cd-78c0813c8d53",
   "metadata": {},
   "source": [
    "# Deep sets and graph networks\n",
    "\n",
    "The ML models we have looked at so far make the assumption that we have a fixed-dimensional vector of input features. In reality that might not always be the case. Some examples:\n",
    "\n",
    "* Sequences (text, audio, video)\n",
    "* Point clouds (e.g. points in 3D space)\n",
    "* Lists of objects (e.g. particles in a collision)\n",
    "* Graphs with different numbers of connections for each node\n",
    "\n",
    "For sequences one approach are recurrent neural networks (RNNs) that utilize a state that gets updated as it iteratively processes input. However, these still need a defined ordering of the inputs and they have certain disadvantages (most prominently difficulty to model \"long-range\" correlations between inputs and difficulty to parallelize since they are sequential in nature).\n",
    "\n",
    "Another approach are models that apply **permutation invariant** transformations on the inputs. Both deep sets and graph networks make use of this.\n",
    "\n",
    "## Deep sets\n",
    "\n",
    "The simplest approach for a permutation invariant transformation is a **per-point transformation** ($\\phi$) followed by a **permutation invariant aggregation**, typically taking the sum/mean or min/max whose output can then be transformed ($\\rho$) by any means, e.g. another MLP.\n",
    "\n",
    "![](figures/deep_set_transformation.png)\n",
    "\n",
    "See [arXiv:1703.06114](https://arxiv.org/abs/1703.06114) for a detailed discussion.\n",
    "\n",
    "\n",
    "### Application to jets in Higgs dataset\n",
    "\n",
    "Remember the missing values in the dataset for the [HiggsChallenge](HiggsChallenge.ipynb)? Those occurred since we had a non-fixed length list of jets in each event (0, 1 or 2). Maybe we can embed the jets into a fixed length vector using a permutation invariant transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc7889-701f-4752-8909-91e59b083552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98d8b8-bbc2-4c0f-bbfc-98dced5c512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')\n",
    "n_sig_tot = df[\"Weight\"][df.Label == \"s\"].sum()\n",
    "n_bkg_tot = df[\"Weight\"][df.Label == \"b\"].sum()\n",
    "# comment this out if you want to run on the full dataset\n",
    "df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f847742-6b7d-4c5c-8884-55bcd33f3810",
   "metadata": {},
   "source": [
    "First, we separate the jet features and other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2220d1-bf1a-4c08-8497-65b4904c8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_cols = sum([[f\"PRI_{obj}_{field}\" for field in [\"pt\", \"eta\", \"phi\"]] for obj in [\"jet_leading\", \"jet_subleading\"]], [])\n",
    "jet_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51009147-4bf2-4848-8662-a688f23f150c",
   "metadata": {},
   "source": [
    "We also exclude variables that are derived from the jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d4758-7d2f-4e99-93a5-4c821f18ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = ['DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_lep_eta_centrality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48e87a-94fc-4533-bf86-b46b456bfaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_cols = [col for col in df.columns if (col.startswith(\"PRI\") or col.startswith(\"DER\")) and col not in jet_cols and not col in excluded_cols]\n",
    "other_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf5635-eee9-4310-9d76-73d7c9b312e4",
   "metadata": {},
   "source": [
    "We will make the jet features a 3-D array of shape `(nevents, max_njets, n_jet_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a7e61-d1de-4bdc-8615-18888bafe228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_jet = df[jet_cols].to_numpy().reshape(-1, 2, 3)\n",
    "X_jet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f36fca-4a81-4201-bb32-c3a33d213a48",
   "metadata": {},
   "source": [
    "The rest of the features just stays a 2-D array as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fa2a0-ac99-4a65-898d-5b8186ecce89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_other = df[other_cols].to_numpy()\n",
    "X_other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d79c2-9788-4ecc-b596-182edb3eea80",
   "metadata": {},
   "source": [
    "Still we need to replace missing values by 0 which can occur for the quantity `DER_mass_MMC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf364f5-bc2e-47e8-8ddf-c34c250c9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other[X_other == -999] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b17cb8-10cf-46de-88ff-da00c24f4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df.Label == \"s\").to_numpy()\n",
    "weight = df['Weight'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfd999-fd16-4294-b540-83ff4a5dc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_jet_train, X_jet_test,\n",
    "    X_other_train, X_other_test,\n",
    "    y_train, y_test,\n",
    "    weight_train, weight_test,\n",
    ") = train_test_split(X_jet, X_other, y, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947b1c2-aec6-4b36-acc3-eeee6c3f261c",
   "metadata": {},
   "source": [
    "Now, let's scale the features. For the jets we have to be a bit careful only to consider non-missing values in the scaling. Also the scikit-learn scalers can only deal with 2D arrays - so let's define a custom scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e4d37-3a2f-481b-a94d-17da73b578b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetScaler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        X = np.array(X) # copy\n",
    "        X[X == -999] = np.nan # replace missing values by nan\n",
    "        X = X.reshape(-1, X.shape[-1]) # make 2D\n",
    "        self.scaler.fit(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        orig_shape = X.shape\n",
    "        X = np.array(X).reshape(-1, X.shape[-1])\n",
    "        X[X == -999] = np.nan\n",
    "        X = self.scaler.transform(X)\n",
    "        X = np.nan_to_num(X, 0) # replace missing values by 0\n",
    "        return X.reshape(*orig_shape) # turn back into 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47e0d-3bdb-4898-b04f-de4a1b47e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_scaler = JetScaler()\n",
    "jet_scaler.fit(X_jet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f100494-e26b-40b3-8606-932277a5fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet_train_scaled = jet_scaler.transform(X_jet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57a3a-6c19-42ed-a0a7-f61a4dfcbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_scaler = RobustScaler()\n",
    "other_scaler.fit(X_other_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bd55d-4841-4096-a013-f9b0043e1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other_train_scaled = other_scaler.transform(X_other_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3a649-033b-436b-8485-2e9bc79337dd",
   "metadata": {},
   "source": [
    "Also we again balance the weights to have the same sum of weights for signal and background and average weight 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f3265-fea2-43dd-99b7-874e433612d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_signal = 1 / weight_train[y_train==1].sum()\n",
    "class_weight_background = 1 / weight_train[y_train==0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd0af4-648f-4ffb-bcee-fc706eaed809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weight(weight, y):\n",
    "    weight = np.array(weight)\n",
    "    weight[y==0] *= class_weight_background\n",
    "    weight[y==1] *= class_weight_signal\n",
    "    return weight / weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ab59e-37a1-4620-8d56-832824f92a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_scaled = transform_weight(weight_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87821988-4d9c-4b23-ae00-2c8574bbe6fa",
   "metadata": {},
   "source": [
    "Now the model - we use the functional API of keras\n",
    "\n",
    "**Note:** When applying the keras `Dense` layer to 3D arrays it is applied independently on each element along the second dimension This is precisely what we want for our per-point transformation $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a2cc2-7263-494e-adf3-d463641e1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    input_jets = Input(shape=(2, 3), name=\"jets\")\n",
    "    jets = input_jets\n",
    "    input_other = Input(shape=(X_other_train.shape[1],), name=\"other\")\n",
    "    \n",
    "    # embed the jets using 3 hidden layers (shared per-jet)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    jets = Dense(100, activation=\"relu\")(jets)\n",
    "    # take the mean/average as a permutation invariant operation\n",
    "    jets = tf.keras.layers.GlobalAveragePooling1D()(jets)\n",
    "    \n",
    "    # 3 hidden layers for the other features\n",
    "    other = input_other\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    other = Dense(100, activation=\"relu\")(other)\n",
    "    \n",
    "    # concatenate embedded jets and other features and add final hidden layer + output\n",
    "    out = tf.keras.layers.concatenate([jets, other])\n",
    "    out = Dense(100, activation=\"relu\")(out)\n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_jets, input_other], outputs=[out])\n",
    "\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29829073-913e-47a0-bdd4-752b81ef1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4e457-3990-4a64-9a0e-f51016afaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343cb08-22ff-4b32-8203-0fca305aebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    {\"jets\": X_jet_train_scaled, \"other\": X_other_train_scaled},\n",
    "    y_train,\n",
    "    sample_weight=weight_train_scaled,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c7644-6075-423f-8af5-8652f457df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jet_test_scaled = jet_scaler.transform(X_jet_test)\n",
    "X_other_test_scaled = other_scaler.transform(X_other_test)\n",
    "weight_test_scaled = transform_weight(weight_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dca49-d683-4153-932d-4f6e4523d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict({\"jets\": X_jet_train_scaled, \"other\": X_other_train_scaled}, verbose=True)[:, 0]\n",
    "y_pred_test = model.predict({\"jets\": X_jet_test_scaled, \"other\": X_other_test_scaled}, verbose=True)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660f40d-7931-4f29-b558-bf49a165d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5402d3-173d-4437-bc46-e20e359d3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ams import ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4ab2d-f0b7-4dfb-a162-189b6400cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1546f9b-ac95-4b38-8dc9-5ee9596872c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ams_scan(y, y_prob, weights, label):\n",
    "    fpr, tpr, thr = roc_curve(y, y_prob, sample_weight=weights)\n",
    "    ams_vals = ams(tpr * n_sig_tot, fpr * n_bkg_tot)\n",
    "    print(\"{}: Maximum AMS {:.3f} for pcut {:.3f}\".format(label, ams_vals.max(), thr[np.argmax(ams_vals)]))\n",
    "    return thr, ams_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a395510-66e0-4406-a687-2d0b97c17064",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_pred_train, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_pred_test, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0.8, 1.)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}