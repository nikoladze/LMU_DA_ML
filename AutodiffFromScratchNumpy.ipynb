{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca88b8e-a5b2-414f-be12-9414cdd96aa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra: Using arrays (unfinished tutorial)\n",
    "\n",
    "It's not too difficult to extend this system to use `numpy` arrays as input and output for each node in the computation graph.\n",
    "\n",
    "Recall the multivariable chain rule:\n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x_j} = \\sum_k \\frac{\\partial f_i}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_j}$$\n",
    "\n",
    "That means if we have a vector at each node, we would in theory need to store the local Jacobians at each stage. However looking at the formula for the gradient of a single scalar output (fixed $i$) $\\frac{\\partial f}{\\partial \\vec x}$ we see that we actually only need the product of the gradient vector $\\frac{\\partial f}{\\partial \\vec g}$ with the jacobian $\\frac{\\partial \\vec g}{\\partial \\vec x}$ - the **vector jacobian product** (vjp)\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Explain all the operations\n",
    "* Explain vjp of matrix multiplication\n",
    "* Explain why we may need to sum over batch dimension during backprop\n",
    "* Explain how we can use `Tensor` objects in the gradient calculation itself, allowing for higher order derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21909d-5649-4eef-8320-8fe779ab0b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f91be-7047-4a60-b6d1-a865dd602354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, array, backward=None, op=None, name=None):\n",
    "        self.array = array\n",
    "        self._backward = None\n",
    "        self.op = op\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        prefix = \"Tensor(\"\n",
    "        lines = repr(self.array).splitlines()\n",
    "        for i in range(1, len(lines)):\n",
    "            lines[i] = \" \" * len(prefix) + lines[i]\n",
    "        array_repr = \"\\n\".join(lines)\n",
    "        return f\"{prefix}{array_repr})\"\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.array.shape\n",
    "    \n",
    "    def register(self, backward):\n",
    "        self._backward = backward\n",
    "        \n",
    "    @staticmethod\n",
    "    def wrap(f):\n",
    "        def op(*args):\n",
    "            new_args = []\n",
    "            for arg in args:\n",
    "                if not isinstance(arg, Tensor):\n",
    "                    arg = Tensor(arg)\n",
    "                new_args.append(arg)\n",
    "            return f(*new_args)\n",
    "\n",
    "        return op\n",
    "\n",
    "    @Tensor.wrap\n",
    "    def __add__(self, other):\n",
    "        out = Tensor(self.array + other.array)\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            # function that receives gradient vector and computes vjp\n",
    "            return [(self, grad), (other, grad)]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    @Tensor.wrap\n",
    "    def __sub__(self, other):\n",
    "        out = Tensor(self.array - other.array)\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad), (other, -1 * grad)]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    @Tensor.wrap\n",
    "    def __mul__(self, other):\n",
    "        out = Tensor(self.array * other.array)\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, other * grad), (other, self * grad)]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    @Tensor.wrap\n",
    "    def __truediv__(self, other):\n",
    "        out = Tensor(self.array / other.array)\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, -1 * grad / other), (other, -1 * grad * self / (other * other))]\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def log(self):\n",
    "        out = Tensor(np.log(self.array))\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad / self)]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        out = Tensor(np.swapaxes(self.array, -1, -2))\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad.T)]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        out = Tensor(self.array @ other.array, op=\"@\")\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [\n",
    "                (self, grad @ other.T),\n",
    "                (other, self.T @ grad),\n",
    "            ]\n",
    " \n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None):\n",
    "        out = Tensor(self.array.sum(axis=axis))\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad * Tensor(np.ones_like(self.array)))]\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(self.array, 0))\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad * Tensor(self.array > 0))]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1 / (1 + np.exp(-self.array)))\n",
    "        \n",
    "        @out.register\n",
    "        def backward(grad):\n",
    "            return [(self, grad * out * (1 - out))]\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def draw_graph(self):\n",
    "        g = graphviz.Digraph(node_attr={\"shape\": \"record\", \"height\": \".1\"})\n",
    "        g.attr(rankdir=\"LR\")\n",
    "        counter = count(0)\n",
    "        nodes = {}\n",
    "\n",
    "        def add_node(tensor):\n",
    "            node_name = f\"node{next(counter)}\"\n",
    "            g.node(node_name, \"Tensor\" if not tensor.name else tensor.name)#format_var(tensor))\n",
    "            nodes[tensor] = node_name\n",
    "\n",
    "        def add_edges(tensor, grad):\n",
    "            if tensor not in nodes:\n",
    "                add_node(tensor)\n",
    "            if tensor._backward is None:\n",
    "                return\n",
    "            for child_tensor, deriv in tensor._backward(grad):\n",
    "                if child_tensor not in nodes:\n",
    "                    add_node(child_tensor)\n",
    "                g.edge(nodes[child_tensor], nodes[tensor])\n",
    "                add_edges(child_tensor, deriv)\n",
    "\n",
    "        add_edges(self, Tensor(np.ones_like(self.array)))\n",
    "        return g\n",
    "    \n",
    "    @property\n",
    "    def topo_ordered_nodes(self):\n",
    "        nodes = []\n",
    "        visited = set()\n",
    "\n",
    "        def add_nodes(tensor, grad):\n",
    "            if tensor in visited:\n",
    "                return\n",
    "            visited.add(tensor)\n",
    "            if tensor._backward is not None:\n",
    "                for child_tensor, deriv in tensor._backward(grad):\n",
    "                    add_nodes(child_tensor, deriv)\n",
    "            nodes.append(tensor)\n",
    "\n",
    "        add_nodes(self, Tensor(np.ones_like(self.array)))\n",
    "\n",
    "        return nodes\n",
    "    \n",
    "    def gradients(self):\n",
    "        grads = defaultdict(lambda: Tensor(np.array(0)))\n",
    "        grads[self] = Tensor(np.ones_like(self.array))\n",
    "        for parent_tensor in reversed(self.topo_ordered_nodes):\n",
    "            grad = grads[parent_tensor]\n",
    "            if parent_tensor._backward is not None:\n",
    "                for child_tensor, deriv in parent_tensor._backward(grad):\n",
    "                    if deriv.shape != child_tensor.shape:\n",
    "                        deriv = deriv.sum(axis=0)\n",
    "                    grads[child_tensor] += deriv\n",
    "        return grads\n",
    "    \n",
    "Tensor.__rmul__ = Tensor.__mul__\n",
    "Tensor.__radd__ = Tensor.__add__\n",
    "Tensor.__rsub__ = lambda other, self: Tensor.__sub__(self, other)\n",
    "Tensor.__rtruediv__ = lambda other, self: Tensor.__truediv__(self, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c27c24-30e4-499d-9dce-2e063e583404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.weights = Tensor(np.random.normal(size=(n_inputs, n_outputs)), name=\"weights\")\n",
    "        self.bias = Tensor(np.zeros(n_outputs), name=\"bias\")\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        return inputs @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ed354-00a9-4339-bc2b-96d24752aec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = Tensor(np.random.rand(10, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a5483-0e19-4e3d-a260-07934313e87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = Layer(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8463e-d305-4d7f-b9fe-b51d298e8749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer(x).draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8dcad-6452-4680-b558-2e8ff363bc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, n_inputs, neurons_per_layer):\n",
    "        self.layers = []\n",
    "        for n_outputs in neurons_per_layer:\n",
    "            self.layers.append(Layer(n_inputs, n_outputs))\n",
    "            n_inputs = n_outputs\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if layer is not self.layers[-1]:\n",
    "                x = x.relu()\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea928f-3b40-4b34-bf0a-f70027124e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLP(x.shape[1], [5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc263bf1-5d22-4a08-a590-4201f8067342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = mlp(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e9832-5a63-4ff4-9837-f2413ad8ad2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred):\n",
    "    eps = 1e-50 # to avoid math domain errors from log(0)\n",
    "    return (-1 * (y_true * (y_pred + eps).log() + (1 - y_true) * (1 - y_pred + eps).log())).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9eedf0-3a73-4188-8ba7-0baa6ab484f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da824646-cd57-4795-94e3-ceeb26ab948b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_crossentropy(Tensor(np.random.rand(*out.shape)), out).draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d64493-e29e-4c15-a91f-28957f6e9ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
