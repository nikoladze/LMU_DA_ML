{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-and-Random-Forest-Models\" data-toc-modified-id=\"Decision-Tree-and-Random-Forest-Models-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Decision Tree and Random Forest Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-example\" data-toc-modified-id=\"Simple-example-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Simple example</a></span></li><li><span><a href=\"#Decision-tree-example-with-real-data\" data-toc-modified-id=\"Decision-tree-example-with-real-data-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Decision tree example with real data</a></span></li><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Feature importance</a></span></li></ul></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Random Forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-for-Cancer-Data\" data-toc-modified-id=\"Random-Forest-for-Cancer-Data-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Random Forest for Cancer Data</a></span></li><li><span><a href=\"#Random-Forest-for-Digits-Data\" data-toc-modified-id=\"Random-Forest-for-Digits-Data-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Random Forest for Digits Data</a></span></li><li><span><a href=\"#Add-on:-Try-a-more-complicated-dataset:-2-spirals.\" data-toc-modified-id=\"Add-on:-Try-a-more-complicated-dataset:-2-spirals.-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Add-on: Try a more complicated dataset: 2 spirals.</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "A further important model category. The basic principle is easy to understand:  \n",
    " Hierarchical series of  **if/else questions** \n",
    "\n",
    "*Example:* Game where you need to distinguish four kinds of animals:  \n",
    "* *Bear, Dolphin, Penguin, Hawk*\n",
    "\n",
    "Goal is to use as few questions as possible.\n",
    "\n",
    "One possible solution:\n",
    "\n",
    "![](figures/DT_animals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple example \n",
    "Illustrate DT with half-moon data, a simple dataset with half-moon shaped data distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,cmap='rainbow');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "print(\"GaussianNB: accuracy = %.2f\" % model.fit(X, y).score(X, y))\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "print(\"KNeighborsClassifier: accuracy = %.2f\" % model.fit(X, y).score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DT\n",
    "max_depth=3\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "tree=model.fit(X,y)\n",
    "tree.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to visualize DT\n",
    "#\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow',dofit=True):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    if dofit: \n",
    "        model.fit(X, y)\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DT with varying depth and visualize limits\n",
    "max_depth=9\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "tree=model.fit(X,y)\n",
    "print ('Depth, score: ', max_depth, tree.score(X,y))\n",
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high depth, clearly goes into over-training\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees** work well in principle, however, they are rather sensitive to over-training  \n",
    "&rarr; Validation curve left for exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Decision tree example with real data\n",
    "\n",
    "A frequently used data set for ML is a data set for *breast cancer diagnosis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print (cancer.feature_names)\n",
    "print (cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply decision-tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without limiting the depth, the DT will be evolved until perfect accuracy.\n",
    "\n",
    "But not really useful &rarr; Over-training\n",
    "\n",
    "Better approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance on the test set has improved by introducing a maximum depth of the trees. (The fact that we do no longer get perfect classfication on the training sample is not relevant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "\n",
    "A very useful additional result of DT classification is the *feature importance*.\n",
    "This gives for each feature a rating between 0 and 1 how important it is for the classification:\n",
    "* 0 means no effect, not useful\n",
    "* 1 means perfect separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to visualize\n",
    "def plot_feature_importance( model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.figure(figsize = (6,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features);\n",
    "\n",
    "    \n",
    "plot_feature_importance( tree )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly shows that feature ` ·∫Åorst radius` has largest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Decisions trees are potentially very powerful models but they are very also sensitive to overtraining (overfitting); therefore they are normally not directly used in practice. \n",
    "\n",
    "However, one can mitigate or solve this problem by using an ensemble of decision trees and not just a single DT.  \n",
    "The main trick is randomization:\n",
    "* train many DTs but\n",
    "    * each DT sees different parts of the data\n",
    "    * or different set of features\n",
    "\n",
    "This approach is called **Random Forest**:  \n",
    "Many randomized trees contribute and the final decision is made by some sort of majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with half moon data using 5 DTs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "# random forest with 5 DT\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(forest, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively, boundaries are more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better accuracy on validation set\n",
    "\n",
    "**Feature importance** also instructive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* many more features contribute\n",
    "* `worst radius` feature no longer dominant\n",
    "\n",
    "&rarr; Random Forests much better in classification and better exploit information of features \n",
    "\n",
    "Main drawback: decision process rather intransparent compared to single DT.\n",
    "\n",
    "An alternative to Random Forests are **Boosted** Decision Trees  \n",
    "&rarr; literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Random Forest for Digits Data\n",
    "\n",
    "Come back to example of digit classification and apply RandomForest to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply RFC to digit data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(model.score(Xtrain, ytrain)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(model.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Excellent performance of Random Forest!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add-on: Try a more complicated dataset: 2 spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "p = sorted(np.random.random(N)*np.pi*5)\n",
    "v1 = p*np.cos(p)\n",
    "v2 = p*np.sin(p)\n",
    "p2 = sorted(np.random.random(N)*np.pi*5+np.pi)\n",
    "w1 = p*np.cos(p2)\n",
    "w2 = p*np.sin(p2)\n",
    "plt.scatter(v1, v2)\n",
    "plt.scatter(w1, w2)\n",
    "y1 = np.zeros((N, 1))\n",
    "y2 = np.ones((N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.concatenate([v1, w1]), np.concatenate([v2, w2])], axis = 1)\n",
    "y = np.concatenate([y1, y2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply RFC to digit data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=1000)\n",
    "forest.fit(Xtrain, ytrain)\n",
    "ypred = forest.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ypred==ytest).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_classifier(forest, Xtest, ytest,dofit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
