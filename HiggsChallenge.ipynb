{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run clone_git_on_colab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using Boosted Decision Trees (BDTs), a popular method in experimental particle physics. \n",
    "\n",
    "* BDTs are based on an ensemble of _weak classifiers_ (decision trees).\n",
    "* Boosting increases the weight of misclassified events. \n",
    "* The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**.\n",
    "  * more information about the data is available from the links, and in particular in the accompanying **[documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**.\n",
    "  * much of the description below is taken from this documentation\n",
    "* The general idea is that we want to extract $H\\to\\tau^+\\tau^-$ signal from background. \n",
    "  * first channel where coupling of Higgs boson to fermions can be proven (before only coupling to bosons, $\\gamma$, $W$, $Z$)\n",
    "  * by now seen many other decays of Higgs, too, most recently even evidence for $H\\to\\mu^+\\mu^-$\n",
    "* In particular, selection here requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. \n",
    "* The challenge is based on Monte Carlo collision events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHC and ATLAS\n",
    "* LHC collides bunches of protons every 25 nanoseconds inside ATLAS detector\n",
    "* In the hard-scattering process, two colliding protons interact and part of the kinetic energy of the protons is converted into new particles.\n",
    "* Most resulting particles are unstable and short-lived → decay quickly into a cascade of lighter particles.\n",
    "* ATLAS detector measures the properties of the decay products: type, energy and momentum (3-D direction)\n",
    "* The decay products are identified and reconstructed from the low-level analogue and digital signals they trigger in the detector hardware.\n",
    "* Part of the energy will be converted into and carried away by neutrinos (e.g. from the decay of tau leptons, $\\tau^- \\to e^- \\nu_\\tau \\bar\\nu_e$) that cannot be measured, leading to an incomplete event reconstruction and an imbalance in the total transverse momentum.\n",
    "\n",
    "Some event displays that visualize collision events found in real data that show a signature matching a $H\\to\\tau\\tau$ decay can be found on the [public ATLAS page][1]. [This event][2], for example, shows $H\\to\\tau\\tau$ with one tau lepton further decaying leptonically and the other hadronically.\n",
    "\n",
    "[1]: https://twiki.cern.ch/twiki/bin/view/AtlasPublic/EventDisplaysFromHiggsSearches#H_AN1\n",
    "[2]: https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2012-160/figaux_07.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The data contains > 800 k simulated collision events, that were used in the reference [ATLAS analysis][1]:\n",
    "* 250 k for training\n",
    "* 100 k for testing (public leaderboard)\n",
    "* 450 k for testing (private leaderboard)\n",
    "* a small withheld dataset\n",
    "\n",
    "Here, we use the full dataset:\n",
    "\n",
    "[1]: http://cds.cern.ch/record/1632191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"KaggleSet\").count()[\"EventId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset mixes background (b) and signal (s) events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").count()[\"EventId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the actual $s:b$ ratio were so large ($\\sim1/3$), we would have found the Higgs much earlier. \n",
    "To obtain the actual number of signal and background events we expect in the 2012 ATLAS dataset, we need to take into account the provided weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").sum()[\"Weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, without any additional selection we expect a signal-background ratio of only 1.7 permille.\n",
    "\n",
    "Each simulated event has a weight\n",
    "* proportional to the conditional density divided by the instrumental density used by the simulator (an importance-sampling flavor),\n",
    "* and normalized for integrated luminosity (the size of the dataset; factors in cross section, beam intensity and run time of the collider)\n",
    "\n",
    "The weights are an artifact of the way the simulation works and not part of the input to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different weights correspond roughly to different background processes (due to the different cross sections)\n",
    "ax = plt.hist(df[\"Weight\"], bins = 100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three different background processes were retained in this dataset ($Z\\to\\tau\\tau$, top-quark-pair production, $W\\to\\ell\\nu$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief overview of variables, there is more information in the documentation. \n",
    "* 30 features\n",
    "  * The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from background. \n",
    "  * On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. \n",
    "    * They themselves generally do not provide much discrimination.\n",
    "    * One of the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. \n",
    "* *EventId* identifies the event but is not a \"feature.\" \n",
    "* The *Weight* is the event weight.\n",
    "  * used to obtain the proper normalization of the different signal and background samples\n",
    "  * sum of weights of all signal events should produce the signal yield expected to be observed in 2012 LHC data taking\n",
    "  * sum of weights of all background events should produce the background yield\n",
    "* *Label* indicates if it is a signal or background event. \n",
    "* Ignore the *Kaggle* variables --- they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate/visualize some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sub-set of vars for plotting\n",
    "varplot = ['DER_mass_MMC', \n",
    "           'DER_mass_jet_jet',\n",
    "           'DER_deltar_tau_lep',\n",
    "           'DER_pt_tot',\n",
    "           'PRI_jet_subleading_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing: map labels to integers\n",
    "df['Label'] = df['Label'].map({'b': 0, 's': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms of the above variables\n",
    "for key in varplot:\n",
    "    # plotting settings\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    bins = np.linspace(min(df[key]), max(df[key]), 30)\n",
    "    # plot signal & backg\n",
    "    signal = df[df['Label']==0][key]\n",
    "    backgr = df[df['Label']==1][key]\n",
    "    p = plt.hist([signal, backgr], bins=bins, alpha=0.3,stacked=True, label=['Background', 'Signal'])\n",
    "    \n",
    "    # decorate\n",
    "    plt.xlabel(key)\n",
    "    plt.ylabel('Number of Events')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot (only first 1k entries)\n",
    "_ = sns.pairplot(df.iloc[:1000], hue='Label', vars=varplot, diag_kind=\"hist\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal plot use _kernel density estimation_ to smear out data points in phase space and add up the result to obtain a smooth function. To show histograms use option ```diag_kind=\"hist\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays for ML models\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt'] # features to train on\n",
    "y = df['Label'] # labels\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split data and weights into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = \\\n",
    "    train_test_split(\n",
    "        X, y, eventID, weight, test_size=0.33, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ML trials w/ simple models\n",
    "1st attempt with simple models: GaussianNB and Logistic Regression\n",
    "* train\n",
    "* test\n",
    "* plot scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB (Gaussian Naive Bayes, assumes each class is drawn from an axis-aligned Gaussian distribution)\n",
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(X_train, y_train)                # 3. fit model to data\n",
    "gnb = model.predict(X_test)                # 4. predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot probabilities for sig & bg\n",
    "from mltools import plot_proba\n",
    "plot_proba( df, model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We go into a bit more details and also talk about \"imputation\" [here](Higgs-Gaussian.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "As next attempt, let's look at [logistic regression](ScilearnIntro.ipynb#Logistic-Regression). This is a very simple, linear model. In the exercises you can look at optimizing it a bit more.\n",
    "* logistic function: $f(x) = \\frac{1}{1+\\exp(-x)}$, $f(x): [-\\infty,\\infty] \\to [0,1]$\n",
    "* model: $y_i = f(x_i \\cdot \\beta) + \\epsilon_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver = \"lbfgs\", max_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lbfgs = limited-memory BFGS, BFGS = Broyden–Fletcher–Goldfarb–Shanno algorithm, an iterative method for solving unconstrained nonlinear optimization problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit takes ~mins\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "plot_proba(df, lr, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Logistic Regression - v2\n",
    "Now repeat but with only derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.loc[:,:'DER_pt_tot'].columns),len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 9 vs 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,:'DER_pt_tot'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using fewer features\n",
    "lr2 = LogisticRegression(solver = \"lbfgs\", max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr2.fit(X_train.loc[:,:'DER_pt_tot'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2.score(X_test.loc[:,:'DER_pt_tot'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob. dist.\n",
    "plot_proba( df, lr2, X.loc[:,:'DER_pt_tot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More sophisticated model: GradientBoostingClassifier\n",
    "The [GradientBoostingClassifier][1] provides _gradient-boosted regression trees_. \n",
    "* ensemble method that combines multiple decision trees\n",
    "* \"forward stage-wise fashion: each tree tries to correct the mistakes of the previous one (steered by the `learning_rate`)\n",
    "* trees are simple (shallow), idea is to combine many \"weak learners\" \n",
    "  * each tree can only provide good predictions on part of the data, but combined they can yield a powerful model\n",
    "  \n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=50, max_depth=10,\n",
    "                                    min_samples_leaf=200,\n",
    "                                    max_features=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit takes several minutes... can look into AMS while it runs\n",
    "gbc.fit(X_train, y_train) # (and n_jobs is not supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, gbc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "from mltools import plot_proba\n",
    "plot_proba(df, gbc, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBC also useful to judge/quantify importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for importance, key in reversed(sorted(zip(gbc.feature_importances_, X.keys()))):\n",
    "    print (\"%30s %6.3f\" % (key, importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools import plot_feature_importance\n",
    "plot_feature_importance( gbc, X.keys() , sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events weights, ROC curve and specific figure-of-merit \n",
    "For our task we have to \n",
    "* consider the event weights\n",
    "* we need a more general way to compare \n",
    "* choose the cut value for the classifier\n",
    "* introduce a problem-specific figure-of-merit \n",
    "\n",
    "### Figure-of-Merit: AMS\n",
    "Let's start with the last item, the definition of a problem-specific figure-of-merit, the approximate median significance ([AMS][1]), to determine how good a solution was.\n",
    "The goal is to maximize signal and minimize background, and the AMS is an approximate formula to quantify the signal significance. The maximal AMS gives best signal significance. \n",
    " This AMS was also used for the  Kaggle competition. \n",
    " \n",
    "A rule-of-thumb to estimate significance of signal over background is simply $\\frac{s} { \\sqrt{b} }$ and the value returned by AMS is actually rather close to this.\n",
    "\n",
    "**Note:** AMS is not a relative quantity but depends on the absolute size of the data set, therefore if you do not use the full data set (i.e. you split into training and testing) you have to reweight the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb\n",
    "\n",
    "AMS is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median significance (AMS)\n",
    "def ams(s,b):\n",
    "    # The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "    return np.sqrt(2*((s+b+10)*np.log(1+s/(b+10))-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Choice of pcut\n",
    "Before applying ams we discuss\n",
    "* choosing own pcut value for desired efficieny\n",
    "* calculating and applying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities for a signal event to be label 0 (background) or 1 (signal)\n",
    "y_train_prob        = gbc.predict_proba(X_train)[:, 1]\n",
    "y_test_prob         = gbc.predict_proba(X_test)[:, 1]\n",
    "y_train_prob_signal = gbc.predict_proba(X_train[y_train==1])[:, 1]\n",
    "y_test_prob_signal  = gbc.predict_proba(X_test[y_test==1])[:, 1]\n",
    "y_train_prob_backg  = gbc.predict_proba(X_train[y_train==0])[:, 1]\n",
    "y_test_prob_backg   = gbc.predict_proba(X_test[y_test==0])[:, 1]\n",
    "\n",
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 40% (i.e. 40 % of unweighted signal  events above pcut will be classified as signal), \n",
    "# but can optimize\n",
    "# np.percentile( array, fcut ) returns value for which fcut percent of array elements \n",
    "# are smaller than this value \n",
    "pcut         = np.percentile(y_train_prob_signal, 60) # NOTE: using y_train_prob here\n",
    "sel_signal   = (y_train_prob_signal > pcut).mean()*100\n",
    "sel_backg    = (y_train_prob_backg > pcut).mean()*100\n",
    "\n",
    "print(f\"{pcut=:.5f} selects {sel_signal:5.2f}% unweighted signal events\")\n",
    "print(f\"{pcut=:.5f} selects {sel_backg:5.2f}% unweighted background events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now include the weights** to get proper normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgtsig  = df[df.Label==1].Weight\n",
    "wgtback = df[df.Label==0].Weight\n",
    "\n",
    "# the density argument makes this a normalized plot (otherwise wouldn't see the signal on linear scale)\n",
    "#kwargs = dict(histtype='stepfilled', alpha=0.3, bins=40, density = True)\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, bins=40, density = False)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background', weights=wgtback, **kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal', weights=wgtsig, **kwargs)\n",
    "plt.legend();\n",
    "\n",
    "plt.yscale('log') #-- to try without density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the total weights (yields) for all events and for training and test samples\n",
    "sigall  = weight[y==1].sum() \n",
    "backall = weight[y==0].sum()\n",
    "\n",
    "# training-sample weights\n",
    "sigtrain  = weight_train[y_train==1].sum()\n",
    "backtrain = weight_train[y_train==0].sum()\n",
    "\n",
    "# test-sample weights\n",
    "sigtest  = weight_test[y_test==1].sum()\n",
    "backtest = weight_test[y_test==0].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"All  : {sigall:10.2f}  {backall:10.2f}\")\n",
    "print (f\"Train: {sigtrain:10.2f}  {backtrain:10.2f}\")\n",
    "print (f\"Test : {sigtest:10.2f}  {backtest:10.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel  = weight_train[(y_train==1) & (y_train_prob > pcut)].sum()\n",
    "backtrain_sel = weight_train[(y_train==0) & (y_train_prob > pcut)].sum()\n",
    "\n",
    "sigtest_sel  = weight_test[(y_test==1) & (y_test_prob > pcut)].sum()\n",
    "backtest_sel = weight_test[(y_test==0) & (y_test_prob > pcut)].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal and background efficiency with weights\n",
    "print (\"Train: eps_s = %f, eps_b = %f (eps_total: %f)\" % (sigtrain_sel / sigtrain, \n",
    "                                                          backtrain_sel / backtrain,\n",
    "                                                         (sigtrain_sel+backtrain_sel) / (sigtrain+backtrain)\n",
    "                                                         ))\n",
    "print (\"Test : eps_s = %f, eps_b = %f\" % (sigtest_sel / sigtest, backtest_sel / backtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to scale-up the selected yields to the (luminosity of the) original full sample\n",
    "sigtrain_sel_corr  = sigtrain_sel *sigall /sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr   = sigtest_sel *sigall /sigtest\n",
    "backtest_sel_corr  = backtest_sel*backall/backtest\n",
    "\n",
    "print(f\"Scaled selected yields in training sample, signal = {sigtrain_sel_corr:6.2f} , background = {backtrain_sel_corr:7.2f}\")\n",
    "print(f\"Scaled selected yields in test sample, signal     = {sigtest_sel_corr:6.2f} , background = {backtest_sel_corr:7.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ROC = \"receiver operating characteristic\")\n",
    "\n",
    "An important tool to see variation of signal and background efficiency (TPR and FPR) as a function of threshold at a glance (for a selection based on predicted signal / background probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:, 1], sample_weight = weight_test)\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve test\")\n",
    "fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_train, gbc.predict_proba(X_train)[:, 1], sample_weight = weight_train)\n",
    "plt.plot(fpr_tr, tpr_tr, label=\"ROC Curve train\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "mark_threshold = pcut # mark selected threshold\n",
    "idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "plt.plot(fpr[idx], tpr[idx], 'o', markersize=10, label=f\"threshold {mark_threshold:7.4f}\", fillstyle=\"none\", mew=2)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TPR = true positive rate = TP / (TP+FN) = TP / P (= recall)\n",
    "* FPR = false positive rate = FP / (FP+TN) = FP / N\n",
    "\n",
    "*Note*: As we have a lot more background than signal events, we typically want to choose a point with a very low false-positive rate. Let us therefore plot the same graph slightly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore') # get rid of nasty divide by zero errors\n",
    "plt.plot(tpr, 1/fpr, label = \"ROC curve test\")\n",
    "plt.plot(tpr_tr, 1/fpr_tr, label=\"ROC Curve train\")\n",
    "plt.plot(tpr[idx], 1/fpr[idx], 'o', markersize=10, label=f\"threshold {mark_threshold:7.4f}\", fillstyle=\"none\", mew=2)\n",
    "plt.legend();\n",
    "plt.xlabel(\"TPR\")\n",
    "plt.ylabel(\"1 / FPR = background rejection\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mltools import plot_roc_test_train\n",
    "#plot_roc_test_train(gbc, y_test, X_test, weight_test, y_train, X_train, weight_train, pcut=pcut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can use the ROC curve to compare different classifiers, a better performance measure is the AMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr, backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr, backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plot of AMS vs Pcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools import ams_scan\n",
    "label='Train'\n",
    "pcutv,amsv = ams_scan(y_train, gbc.predict_proba(X_train)[:, 1], weight_train, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "label='Test'\n",
    "pcutv,amsv = ams_scan(y_test, gbc.predict_proba(X_test)[:, 1], weight_test, sigall, backall)\n",
    "# calculate size and pcut of ams maximum\n",
    "pcutmax,amsmax = pcutv[np.argmax(amsv)] , amsv.max()\n",
    "print(f\"{label} Maximum AMS {amsmax:.3f} for pcut {pcutmax:.3f}\")\n",
    "plt.plot(pcutv,amsv,label=label)\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.plot(pcutmax, amsmax, 'o', markersize=10, label=f\"threshold {pcutmax:.4f}\", fillstyle=\"none\", mew=2)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Not too bad. Here are the scores of real submissions.\n",
    "![Comparison with submissions](figures/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course a bit of a simplification from a real physics analysis, where systematics often seem to take the most time. They are ignored here.\n",
    "![Comparison with real analysis](figures/tr140415_davidRousseau_Rome_Higgs_MVA_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _systematics_: systematic uncertainties on the event yields and BDT distributions, of experimental and theoretical origin (cf. section 11 in reference analysis)\n",
    "* _categories_: the reference analysis discriminates two production mechanisms of the Higgs boson, VBF (events with two characteristic jets from vector-boson fusion) and boosted (all other events, dominated by gluon fusion)\n",
    "* _embedded_: dominant Z→τ⁺τ⁻ background is taken from \"τ-embedded Z→μ⁺μ⁻ data\"\n",
    "* _anti tau_: revert some tau-identification criterion to create an \"anti(-ID) tau\" sample (used in \"fake-factor method\" to estimate background with objects misidentified as tau leptons)\n",
    "* _control regions_: phase-space regions enriched in (one type of) background process that allow to normalize a predicted background contribution to that observed in data\n",
    "* _tt_: background process, events with pair production of top quarks ($t\\bar t$)\n",
    "* _NP_: nuisance parameters (parameters of fit model that are not of physical interest but give it more flexibility to describe the data)\n",
    "* _TMVA_: [Toolkit for Multivariate Data Analysis with ROOT][1], a ROOT-integrated project providing an ML environment for multivariate classification and regression techniques\n",
    "\n",
    "[1]: https://root.cern.ch/tmva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Your tasks\n",
    "Inspiration for things to look into for the rest of the session and revise the above material:\n",
    "1. If you want to understand a bit better the input features, take a look at the definitions in the [documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf) to get a rough feeling for what physics they encode.\n",
    "1. Attempt to calculate the AMS for the logistic regression cases.\n",
    "1. Do we overfit? Add plots to see.\n",
    "1. Look again at the importance of variables to see if you understand why that makes sense.\n",
    "1. Should we **[preprocess](http://scikit-learn.org/stable/modules/preprocessing.html)** the input data to be the same scale? Note that we have some -999 values that indicate the variable could not be calculated.\n",
    "(Remember the discussion how to treat such problems in [this notebook][1].\n",
    "1. We have not used the event weights in the training. Can they help? (Note that you don't want to just apply the weights as is since they will make background dominate over signal.)\n",
    "1. The best scores in the Challenge all used cross-validation; if you have time, try to implement it.\n",
    "\n",
    "*Later we will [continue][2] on this example with neural networks.*\n",
    "\n",
    "[1]: Higgs-Gaussian.ipynb\n",
    "[2]: HiggsChallenge-NN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
