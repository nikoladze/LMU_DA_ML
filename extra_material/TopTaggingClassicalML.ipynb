{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook needs pyjet - an interface to the FastJet clustering algorithm:\n",
    "# http://fastjet.fr/\n",
    "# https://github.com/scikit-hep/pyjet\n",
    "#!pip install --user pyjet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top tagging with n-subjettiness\n",
    "Here we will try to construct the \"n-subjettiness\" variables (see [arXiv:1011.2268](https://arxiv.org/pdf/1011.2268.pdf)) and train a simple ML classifier on these instead of the more \"deep learning\" approach discussed in [CNNTopTagging.ipynb](CNNTopTagging.ipynb).\n",
    "\n",
    "The n-subjettiness is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau_n = \\frac{1}{R_0\\sum_k p_{\\mathrm{T},k}}\\sum_k p_{\\mathrm{T}, k} \\min(\\Delta R_{1, k}, \\Delta R_{2, k}, \\dots, \\Delta R_{n, k})\n",
    "\\end{equation}\n",
    "\n",
    "Where $k$ runs over all constituents of our jet and $n$ runs over re-clustered subjets with the hypothesis that the jet contains $n$ subjets. $\\Delta R$ is a distance in the $\\eta-\\phi$ plane and $R_0$ the radius parameter of the original jet clustering (in our case $R_0=0.8$). We can now construct these variables with different hypotheses for $n$ and feed them into a ML algorithm. Intuitively, $\\tau_n$ measures how well a jet can be described as being composed out of $n$ subjets. In our case we expect the QCD jets to be very \"1-subjetty\" and the top-quark jets to be very \"3-subjetty\".\n",
    "\n",
    "![Figure 4 from arXiv:1011.2268 [hep-ph]](figures/top_tagging/10112268_Fig4.svg \"Figure 4 from arXiv:1011.2268 [hep-ph]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyjet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"/large_tmp/LMU_DA_ML/top_tagging/train.h5\", \"table\", stop=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsubjettiness_features(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate n-subjettiness variables.\n",
    "    Caveat: not sure if this is correct\n",
    "    - the resulting distributions look a bit different from arXiv:1011.2268\n",
    "    \"\"\"\n",
    "    \n",
    "    import tqdm # visual gimmick: used to show progress bar\n",
    "    \n",
    "    R_0 = 0.8\n",
    "\n",
    "    # we have up to 200 constituents for each jet (the rest are padded with 0)\n",
    "    jet_columns = sum([[\"{}_{}\".format(var, i) for var in [\"E\", \"PX\", \"PY\", \"PZ\"]] for i in range(200)], [])\n",
    "    \n",
    "    # make plain array of jets with constituents for each jet (one dimension more)\n",
    "    jet_array = df[jet_columns].values.reshape(-1, 200, 4)\n",
    "    \n",
    "    # we want to calculate n-subjettiness for 1, 2, 3, 4 subjets\n",
    "    n_subjets_vars = {1 : [], 2 : [], 3 : [], 4 : []}\n",
    "    \n",
    "    for jet in tqdm.tqdm_notebook(jet_array):\n",
    "        # create structured numpy array in the right format for pyjet\n",
    "        jet = jet.astype(np.float64).view(\n",
    "            dtype=[(\"E\", np.float64), (\"px\", np.float64), (\"py\", np.float64), (\"pz\", np.float64)]\n",
    "        ).reshape(-1)\n",
    "        \n",
    "        # throw out 0-padded values for non-existent constituents\n",
    "        jet = jet[jet[\"E\"]!=0]\n",
    "        \n",
    "        # change of coordinate system: calculate pt, eta, phi from px, py, pz\n",
    "        pt  = np.sqrt(jet[\"px\"] ** 2 + jet[\"py\"] ** 2)\n",
    "        eta = np.arcsinh(jet[\"pz\"] / pt)\n",
    "        phi = np.arctan2(jet[\"py\"], jet[\"px\"])\n",
    "        \n",
    "        subjets_list = []\n",
    "        for n_subjets in n_subjets_vars:\n",
    "\n",
    "            # skip if we have less than n_subjets constituents\n",
    "            if len(jet) < n_subjets:\n",
    "                n_subjets_vars[n_subjets].append(0.)\n",
    "                continue\n",
    "\n",
    "            # run the exclusive-kt clustering for each n-jet hypothesis\n",
    "            subjets = pyjet.cluster(jet, R=R_0, p=1, ep=True).exclusive_jets(n_subjets)\n",
    "            # find the closest distance of each subjet to all constituents\n",
    "            dR = []\n",
    "            for subjet in subjets:\n",
    "                dR.append(np.sqrt((subjet.eta - eta) ** 2 + (subjet.phi - phi) ** 2))\n",
    "            closest_dR = np.stack(dR, axis=1).min(axis=1)\n",
    "            # calculate the actual n-subjettiness\n",
    "            n_subjets_vars[n_subjets].append(\n",
    "                (closest_dR * pt).sum() / (pt.sum() * R_0)\n",
    "            )\n",
    "            \n",
    "    return pd.DataFrame(n_subjets_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsub = get_nsubjettiness_features(df)\n",
    "# make nicer for seaborn\n",
    "df_nsub.columns = [\"tau_1\", \"tau_2\", \"tau_3\", \"tau_4\"]\n",
    "df_nsub[\"y\"] = df.is_signal_new.values\n",
    "df_nsub.loc[df_nsub.y==1, \"y\"] = \"Top\"\n",
    "df_nsub.loc[df_nsub.y==0, \"y\"] = \"QCD\"\n",
    "df_nsub.y = pd.Categorical(df_nsub.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = sns.pairplot(df_nsub.iloc[::100], hue=\"y\")\n",
    "for iy in range(len(pg.axes)):\n",
    "    for ix in range(len(pg.axes)):\n",
    "        if not ix == iy:\n",
    "            pg.axes[ix][iy].set_xlim(0, 0.5)\n",
    "            pg.axes[ix][iy].set_ylim(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_nsub[[\"tau_1\", \"tau_2\", \"tau_3\", \"tau_4\"]].values\n",
    "y = df.is_signal_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bdt.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(bins=100, range=(0, 1), alpha=0.5)\n",
    "plt.hist(scores[:,1][y==0], **opts)\n",
    "plt.hist(scores[:,1][y==1], **opts)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_curve(y, scores[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(roc[1], 1. / roc[0])\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(tpr[fpr < 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. / np.min(fpr[tpr > 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_hdf(\"/large_tmp/LMU_DA_ML/top_tagging/test.h5\", \"table\", stop=100000)\n",
    "df_test_nsub = get_nsubjettiness_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test_nsub.values\n",
    "y_test = df_test.is_signal_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = bdt.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_test = roc_curve(y_test, scores_test[:,1])\n",
    "plt.plot(roc_test[1], 1. / roc_test[0])\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test, tpr_test, thr_test = roc_test\n",
    "print(np.max(tpr_test[fpr_test < 0.001]))\n",
    "print(1. / np.min(fpr_test[tpr_test > 0.3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}