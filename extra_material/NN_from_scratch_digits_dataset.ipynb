{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reuses our self-made neural net from [a previous notebook][1] to train it on a more realistic (larger) dataset of digits and splitting the dataset into a training and a test sample to obtain an independent measurement of the accuracy on a dataset that the neural network has not seen before.\n",
    "\n",
    "[1]: NN_from_scratch_digits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the functions that stay unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def argmax(l):\n",
    "    return l.index(max(l))\n",
    "\n",
    "def reshape(array, xs, ys):\n",
    "    if len(array) != xs*ys:\n",
    "        raise ValueError(\"Wrong size in reshape\")\n",
    "    return [array[row:(row+xs)]\n",
    "            for row in range(0, len(array), xs)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input (always 1)\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    # outputs = two arrays (one array of size 4 for the hidden layer plus one array of size 10 for the output layer)\n",
    "    return outputs \n",
    "\n",
    "def predict(network, input):\n",
    "    \"\"\"run input through the network and return output of last layer\"\"\"\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the backpropagation function is mostly unchanged but we introduce a learning-rate parameter (`rate`), which is just a multiplicative factor for the adjustments we make to the weights in each call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target, rate = 1.0):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # compute the delta (error term) of the output layer\n",
    "    output_deltas = [output * (1 - output) * (output - target[i]) # (1)\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # back-propagate errors to hidden layer: compute the delta (error term) of the hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                     dot(output_deltas, [n[i] for n in network[-1]]) # (2)\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]): # loop over weights of neurons in output layer\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]): # loop over output of neurons in hidden layer + bias\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output * rate # (3)\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]): # loop over weights of neurons in hidden layer\n",
    "        for j, input in enumerate(input_vector + [1]): # loop over output of neurons in first layer, i.e. the inputs + bias\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input* rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with the digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the digits dataset from sklearn that [we have used before][1].\n",
    "\n",
    "[1]: ScilearnIntro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps consist in preprocessing the data to have it in a compatible format for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images and flatten -- note we're normalizing to a mean value << 1\n",
    "images = [list(image.flatten()/16/8) for image in digits.images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target to one-hot encoding\n",
    "numbers = [[1 if i == j else 0 for i in range(10)]\n",
    "            for j in digits.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(20, 8), ncols=10, nrows=5)\n",
    "idx = 0\n",
    "for g in axes:\n",
    "    for ax in g:\n",
    "        if idx >= len(images): break\n",
    "        figure = images[idx]\n",
    "        ax.imshow(reshape(figure, 8, 8))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our network. The size of the digits is 8*8 pixels. We use 10 neurons in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)    # to get reproducible results\n",
    "input_size  = 64  # each input is a vector of 64 pixels\n",
    "num_hidden  = 10  # number of neurons in the hidden layer\n",
    "output_size = 10  # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helper function computes the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "def accuracy(network, X, y):\n",
    "    total = float(len(y))\n",
    "    correct = sum([argmax(predict(network, input)) == argmax(y[idx]) for idx, input in enumerate(X)])\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start the training of the neural network. We will manually adjust the learning rate, starting with 1 and decreasing it later on. The `frac` variable determines which fraction of the digits dataset is used in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac    = 0.7\n",
    "len_train     = int(len(images) * train_frac)\n",
    "images_train  = images[:len_train]\n",
    "images_test   = images[len_train:]\n",
    "numbers_train = numbers[:len_train] \n",
    "numbers_test  = numbers[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sizes of the training, test and total sample of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_train), len(images_test), len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and print the accuracy both on the training and test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate  = 1  # learning rate\n",
    "batch_size  = 50 # fraction of sample to use in each round\n",
    "num_epochs  = 10 # number of epochs to run\n",
    "\n",
    "one_epoch = math.ceil(len(images_train) / batch_size) # roughly\n",
    "for x in range(one_epoch * num_epochs):\n",
    "    # pick a subsample\n",
    "    train_on = random.sample(list(zip(images_train, numbers_train)), batch_size)\n",
    "    for input_vector, target_vector in train_on:\n",
    "        backpropagate(network, input_vector, target_vector, learn_rate) # 65 Âµs/loop (4 hidden), 134 Âµs/loop (10 hidden), 380 Âµs/loop (32 hidden)\n",
    "    if x % one_epoch == 0:\n",
    "        print(\"Batches processed: %d, accuracy (train): %.3f, accuracy (test): %.3f\" % \n",
    "              (x,\n",
    "               accuracy(network, images_train, numbers_train),\n",
    "               accuracy(network, images_test , numbers_test),\n",
    "              ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions show the weights of the neurons in the hidden layer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x, y, hatch, color):\n",
    "    \"\"\"return a matplotlib 'patch' object with the specified\n",
    "    location, crosshatch pattern, and color\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                        hatch=hatch, fill=False, color=color)\n",
    "\n",
    "\n",
    "def show_weights(neuron_idx, ax):\n",
    "    weights = network[0][neuron_idx]\n",
    "\n",
    "    grid = [weights[row:(row+8)]      # turn the weights into a 5x5 grid\n",
    "            for row in range(0,64,8)] # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "    pos = ax.imshow(grid,\n",
    "                    cmap=matplotlib.cm.coolwarm,\n",
    "                    interpolation='none', # plot blocks as blocks\n",
    "                    vmin = -8, vmax = 8) # define a unique range for all subplots\n",
    "    \n",
    "    # print bias\n",
    "    ax.set_xlabel(\"bias = %.2f\" % weights[25])\n",
    "    return pos\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3), ncols=num_hidden)\n",
    "for idx in range(num_hidden):\n",
    "    pos = show_weights(idx, ax[idx])\n",
    "    #fig.colorbar(pos, ax = ax[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_layer, cmap=matplotlib.cm.coolwarm)\n",
    "plt.xlabel(\"Weight of hidden neuron (and bias)\")\n",
    "plt.ylabel(\"Output label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can continue the training by rerunning the cells above a few times and play with the `rate` and `frac` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network for an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "plt.imshow(reshape(images_test[idx], 8, 8))\n",
    "probs = predict(network, images_test[idx])\n",
    "print(\"Probabilities:\", probs)\n",
    "predicted_label = argmax(probs)\n",
    "true_label      = argmax(numbers_test[idx])\n",
    "print(\"i.e. prediction: %d (%s)\" % (argmax(probs), predicted_label == true_label))\n",
    "if predicted_label != true_label:\n",
    "    print(\"     true label: %d\" % true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I'd have a hard time to recognize this as a \"2\" but apparently both the network and the target label agree on this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}