{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Course Challenge\n",
    "### Predicting reconstruction efficiencies for tau leptons\n",
    "This is a physics-inspired challenge where your task is to make the best possible predictions for the probability that a certain particle, a tau lepton, will be correctly reconstructed in the detector. We simulated a lot (hundred of thousands) of collisions of a proton with another proton, where in each of these events, a small number of tau leptons (1, 2, or 3) is produced. Unfortunately, it is quite difficult to detect (\"reconstruct\") the presence of a tau lepton in a particle detector, as they decay before they have a chance to interact with the detector. We thus only see the decay products of the tau leptons, and from this only slightly more than half of the tau leptons is actually reconstructed as a tau lepton.\n",
    "\n",
    "The probability that a given true tau lepton is reconstructed (\"reconstruction efficiency\") depends on its properties: how much energy is has, where it hits our detector, and possibly other properties. Often, we therefore measure the efficiency as function of e.g. the transverse momentum ($p_T$) and the position ($\\eta$).\n",
    "The goal in this challenge is to predict this probability. \n",
    "\n",
    "This seems like a nice opportunity to practice our newly learned skills in data analysis and machine learning! Let's get to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "path_csv = \"https://cernbox.cern.ch/index.php/s/sLWDkDKVkNmlDuw/download\" # you may want to make a local copy\n",
    "#path_csv = \"output1.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "df = pd.read_csv(path_csv, sep=\";\", compression=\"gzip\", comment = \"#\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of input features available in the dataset: (one row = one tau lepton)\n",
    "* `mcChannelNumber`, `eventNumber`: unique identifiers for the dataset and collision event\n",
    "* `N_true_elec`: true number of electrons in the collision event\n",
    "* `N_true_muon`: true number of muons in the collision event\n",
    "* `N_true_taus`: true number of tau leptons in the collision event\n",
    "* `GenWeight`: relative physical probability of this collision event\n",
    "* `MetTST_met`: measured missing transverse momentum in GeV\n",
    "* `truth_pt`: true transverse momentum of the tau lepton in GeV\n",
    "* `truth_eta`, `truth_phi`: true geometrical coordinates of the tau lepton in the detector ($\\eta$ is called the [https://en.wikipedia.org/wiki/Pseudorapidity](pseudorapidity))\n",
    "* `truth_prong`, `truth_neutral`: true number of charged and neutral particles the tau lepton decayed into\n",
    "* `truth_charge`: charge of the tau lepton (in units of the elementary charge of the electron)\n",
    "* `dR_min`: geometrical distance of the tau lepton and its reconstruction (if it has been reconstructed, otherwise 999)\n",
    "* `match_pt`: measured transverse momentum of the tau lepton in GeV (if it has been reconstructed, otherwise -999)\n",
    "* `dR_min_taujet`: geometrical distance of the tau lepton and the closest jet in the detector (if there is a jet, otherwise 999)\n",
    "* `TruthMET_met`: true missing transverse momentum in GeV\n",
    "* `Vtx_n`: number of concurrent proton-proton collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Not all of the above features are suited as input features. \n",
    "* `dR` is defined as $\\Delta R = \\sqrt{\\eta^2 + \\phi^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target feature:\n",
    "* `reco_matched`: 1 if the tau lepton has been reconstructed, otherwise 0\n",
    "\n",
    "Note: This is what we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Examples for Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### global definitions\n",
    "# bin edges\n",
    "EX, EY = (\n",
    "    np.linspace(0,   3, 10), # eta\n",
    "    np.linspace(0, 400,  5)  # pt [GeV]\n",
    ")\n",
    "# random seed for train/test split\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function\n",
    "def GetEff(X, y):\n",
    "    # prints efficiencies in bins of pt and eta\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y, name = \"matched\")\n",
    "    # combine pt, eta columns from X with flag whether tau is reconstructed from y\n",
    "    df = pd.concat([X[[\"truth_pt\", \"truth_eta\"]].reset_index(drop = True), y.reset_index(drop = True)], axis=1)\n",
    "    # compute bins \n",
    "    ptbins  = pd.cut(    df[\"truth_pt\"]  , EY)\n",
    "    etabins = pd.cut(abs(df[\"truth_eta\"]), EX)\n",
    "    # group in bins\n",
    "    return df.groupby([etabins, ptbins])[y.name].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the actual efficiencies as function of two of the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual efficiencies:\")\n",
    "GetEff(df, df[\"reco_matched\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We define a subset of features to learn from -- this you can change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [u'truth_pt', u'truth_eta', u'truth_phi', u'truth_prong', u'dR_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[input_features]\n",
    "y = df[\"reco_matched\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit a BDT\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "### BDT\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "                           algorithm=\"SAMME\",\n",
    "                           n_estimators=500)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score of 1.0? That's perfect! But is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison\n",
    "print(\"Actual efficiencies:\")\n",
    "print(GetEff(X_test, y_test))\n",
    "\n",
    "print()\n",
    "print(\"Predicted efficiencies:\")\n",
    "print(GetEff(X_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Twist\n",
    "What we actually see is that the reconstruction efficiencies depend on the MC dataset number: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mcChannelNumber in df[\"mcChannelNumber\"].unique():\n",
    "    df1 = df[df[\"mcChannelNumber\"] == mcChannelNumber]\n",
    "    print()\n",
    "    print(\"Actual efficiencies for dataset number:\", mcChannelNumber)\n",
    "    print(GetEff(df1, df1[\"reco_matched\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC dataset numbers correspond to different ways of how the tau leptons are produced (i.e. which particles decay to produce the tau leptons), but the tau leptons themselves are elementary particles (like an electron) and as such have no way to know the process which produced them. In particular there should be no dependence on the MC dataset numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the challenge is to predict the reconstruction efficiencies from the input features without using the MC dataset numbers such that they match the actual numbers as closely as possible. \n",
    "\n",
    "The measure we will use is the mean squared difference for the predicted and true efficiencies (in the binning above) on some these and some additional samples as well as possibly the AUC for the prediction of \"reco_matched\" (to be discussed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Closing with some more convenience functions to test prediction on a particular sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictForChannel(model, X_trained, df, mcChannelNumber):\n",
    "    # test model on channelnumber\n",
    "    columns = X_trained.columns\n",
    "    df1 = df[df[\"mcChannelNumber\"] == mcChannelNumber]\n",
    "    #y_pred = model.predict_proba(df1[columns])[:,1]\n",
    "    y_pred = model.predict(df1[columns])\n",
    "    return df1, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetEff(*PredictForChannel(model, X_train, df, 397049))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to predict challenge metrics for histogram method:\n",
    "To have something to compare to, here is how the method of just using a histogram in the variables `truth_pt` and `truth_eta` as our classifier performs.\n",
    "\n",
    "The prediction is done by looking up the efficiencies from the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_proba_from_hist(pt, abseta, eff_hist):\n",
    "    # add overflow and underflow bins\n",
    "    # (duplicate the first and last one)\n",
    "    h = eff_hist\n",
    "    h = np.append(h, h[-1:], axis=0)\n",
    "    h = np.append(h[0:1], h, axis=0)\n",
    "    h = np.append(h, h[:,-1:], axis=1)\n",
    "    h = np.append(h[:,0:1], h, axis=1)\n",
    "\n",
    "    # bin indices\n",
    "    eta_idx = np.digitize(abseta, EX)\n",
    "    pt_idx = np.digitize(pt, EY)\n",
    "    \n",
    "    # lookup efficiencies\n",
    "    effs = np.empty(len(eta_idx))\n",
    "    for i, (x, y) in enumerate(zip(eta_idx, pt_idx)):\n",
    "        effs[i] = h[x, y]\n",
    "    \n",
    "    return effs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_train = GetEff(X_train, y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_test = pred_proba_from_hist(X_test.truth_pt, X_test.truth_eta.abs(), hist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_and_auc(y_true, proba):\n",
    "    fpr, tpr, thr = roc_curve(y_test, proba)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"black\")\n",
    "    print(\"AUC:\", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_and_auc(y_test, proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, df_mcChannelNumber, proba):\n",
    "    for mcChannelNumber in df[\"mcChannelNumber\"].unique():\n",
    "        df1 = df[df[\"mcChannelNumber\"] == mcChannelNumber]\n",
    "        df2 = X[df_mcChannelNumber == mcChannelNumber]\n",
    "        print(\"Mean squared error for dataset number:\", mcChannelNumber)\n",
    "        actual_hist = GetEff(df1, df1[\"reco_matched\"])\n",
    "        pred_hist = GetEff(df2, proba)\n",
    "        squared_diff = ((actual_hist.values - pred_hist.values) ** 2)\n",
    "        mean_squared_error = np.where(~np.isnan(squared_diff), squared_diff, 0).sum()\n",
    "        print(mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the same random seed as above to get the same splitting\n",
    "df_mcChannelNumber_train, df_mcChannelNumber_test = train_test_split(df[\"mcChannelNumber\"], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(X_test, df_mcChannelNumber_test, proba_test)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}