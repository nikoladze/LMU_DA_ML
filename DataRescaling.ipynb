{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features in our data are just numbers without units attached to them. However, giving a length in cm or inch (or in [Newton seconds vs pound-force seconds][1]) will give different values and thus different outputs when feeding these numbers e.g. to a neural network. \n",
    "\n",
    "---\n",
    "### Mars Climate Orbiter: a sad story\n",
    "* mission launched Dec 98, planned to reach Mars orbit in Sep 99\n",
    "* Trajectory Correction Maneuver-4 was computed beginning of September\n",
    "    * some ground software by Lockheed Martin produced results in a United States customary unit, contrary to its specification, while a second system by NASA expected those results to be in SI units\n",
    "    * specifically: the total impulse produced by thruster firings was given in pound-force seconds instead of Ns (factor 4.45 mismatch)\n",
    "* unfortunately, the disagreement was noticed but the concern by the engineers ignored because they didn't fill the forms correctly\n",
    "* a 5th correction maneuver was considered but not done\n",
    "* the probe entered a too low Marc orbit (57 km instead of 226 km) and either skipped from the atmosphere and reentered space or was destroyed\n",
    "* total cost of the mission: around 0.3 billion \\$ \n",
    "---\n",
    "\n",
    "Even more importantly, the values of different features may naturally cover completely different ranges. This may have a large effect on the performance of machine learning models:\n",
    "* obviously e.g. when computing the distance of nearest neighbors in the kNN estimator\n",
    "* but also if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "\n",
    "Some algorithms will be more susceptible to the scaling of the data than others:\n",
    "* Neural networks expect all input features to vary in a similar way, and ideally to look like standard normally distributed data, i.e. Gaussian with zero mean and unit variance.\n",
    "* BDTs that just apply `if` statements on the features are typically very robust.\n",
    "\n",
    "The `sklearn` documentation provides further information on [preprocessing of data][2].\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure\n",
    "[2]: https://scikit-learn.org/stable/modules/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example I\n",
    "One example that illustrates the importance of preprocessing the data to rescale feature values to \"standard ranges\" is the application of SVMs to the `sklearn`'s cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here a *linear support vector machine*, a common linear classification model, implemented in `LinearSVC` (support vector classifier). SVC try to find a (hyper-) plane in phase space that optimally separates the two given populations. \n",
    "* Predictions are then made by checking on which side of this plane new samples lie. \n",
    "* The (hyper-) plane is defined by only a few of the training samples (the supporting vectors of features); those that lie close to the resulting plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC and load data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset have very different ranges as illustrated by the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features\n",
    "plt.figure(figsize = (20, 4))\n",
    "plt.boxplot(cancer.data, whis = \"range\")\n",
    "plt.xlabel(\"feature\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data without rescaling\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very low accuracy compared to our previous results.\n",
    "(Note that `SVC` by default uses `kernel=\"rbf\"`, i.e. a [radial-basis-function kernel][1]. The use of the Euclidean distance between the feature vectors may be causing this. If we instead use `SVC(kernel=\"linear\")`, the resulting score is much higher.)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rescale the features and retrain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# compute minimum and maximum on the training data\n",
    "# note that we must train the scaler only on the training data (but not the full dataset)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "# retrain\n",
    "svm.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same effect (although much smaller) e.g. on MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "print(\"Score using unscaled data:\", mlp.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", mlp.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no effect e.g. on BDT\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "bdt = AdaBoostClassifier(random_state=0)\n",
    "print(\"Score using unscaled data:\", bdt.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", bdt.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large values of features may also lead to convergence issues. \n",
    "(Note that many linear models include regularization terms that are introduced to avoid very large values of the parameters.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(43)\n",
    "nsamples = 50\n",
    "pops     = 2\n",
    "yshift   = 0.5\n",
    "yscale   = 100 # try changing to 1 later\n",
    "noise    = 0.3\n",
    "\n",
    "### helpers\n",
    "def MakeXYcorrSample(x0 = 0, y0 = 0, noise = noise, yscale = yscale, pops = pops, yshift = yshift):\n",
    "    # space\n",
    "    X = np.zeros((nsamples * pops, 2))\n",
    "    y = np.zeros(nsamples * pops)\n",
    "    # fill\n",
    "    X[:, 0] = x0 + np.random.rand(nsamples * pops)\n",
    "    for n in range(pops):\n",
    "        X[n*nsamples:(n+1)*nsamples, 1] = (\n",
    "            yscale * (y0 + X[n*nsamples:(n+1)*nsamples, 0] - noise/2. + noise * np.random.rand(nsamples) + n*yshift)\n",
    "        )\n",
    "        y[n*nsamples:(n+1)*nsamples] = n\n",
    "    return X, y\n",
    "\n",
    "### make and plot dataset\n",
    "X, y = MakeXYcorrSample()\n",
    "for pop in range(pops):\n",
    "    plt.scatter(*X[y == pop,0:2].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: We have two populations that are obviously easy to separate? Or are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "### fit\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "print(\"Score on training data:\", model.score(X_train, y_train))\n",
    "print(\"Score on validation data:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training data shows that we cannot even model the training data correctly using the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize\n",
    "for pop in range(pops):\n",
    "    plt.scatter(*X[y == pop,0:2].T)\n",
    "if \"coef_\" in dir(model):\n",
    "    eps = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps  \n",
    "    xx = np.linspace(x_min, x_max, 1000)\n",
    "    yy = np.linspace(y_min, y_max, 1000)\n",
    "    X1, X2 = np.meshgrid(xx, yy)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    decision_values = model.predict(X_grid)\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    plt.gca().imshow(\n",
    "        decision_values.reshape(X1.shape),\n",
    "        extent=(x_min, x_max, y_min, y_max),\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        alpha=0.5,\n",
    "        cmap = ListedColormap(['#0000aa', '#ff2020', '#50ff50'])\n",
    "    )\n",
    "    #\n",
    "    line = np.linspace(0, 1)\n",
    "    for coef, intercept in zip(model.coef_, model.intercept_):\n",
    "        ax = plt.plot(line, -(line * coef[0] + intercept) / coef[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this problem how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the scaling is the problem. Try to \n",
    "* change to rerun with `yscale` set to 1 (instead of `yscale = 100`) when generating the populations to separate\n",
    "* introduce a Scaler (like the one we used above)\n",
    "In both cases you should get 100 % separation both on the training and on the test sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
