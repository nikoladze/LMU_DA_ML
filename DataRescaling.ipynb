{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features in our data are just numbers without units attached to them. However, giving a length in cm or inch (or in [Newton seconds vs pound-force seconds][1]) will give different values and thus different outputs when feeding these numbers e.g. to a neural network. \n",
    "\n",
    "---\n",
    "### Mars Climate Orbiter: a sad story\n",
    "* mission launched Dec 98, planned to reach Mars orbit in Sep 99\n",
    "* Trajectory Correction Maneuver-4 was computed beginning of September\n",
    "    * some ground software by Lockheed Martin produced results in a United States customary unit, contrary to its specification, while a second system by NASA expected those results to be in SI units\n",
    "    * specifically: the total impulse produced by thruster firings was given in pound-force seconds instead of Ns (factor 4.45 mismatch)\n",
    "* unfortunately, the disagreement was noticed but the concern by the engineers ignored because they didn't fill the forms correctly\n",
    "    * a 5th correction maneuver was considered but not done\n",
    "* the probe entered a too low Mars orbit (57 km instead of 226 km)\n",
    "    * communication was lost on 23.09.1999 when it went into orbital insertion\n",
    "    * likely skipped from the atmosphere and reentered space or was destroyed\n",
    "* total cost of the mission: around 0.33 billion USD\n",
    "---\n",
    "\n",
    "Even more importantly, the values of different features may naturally cover completely different ranges. This may have a large effect on the performance of machine learning models:\n",
    "* obviously e.g. when computing the distance of nearest neighbors in the kNN estimator\n",
    "* but also if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "\n",
    "In general, learning algorithms benefit from standardization of the data set.\n",
    "Some algorithms will be more susceptible to the scaling of the data than others:\n",
    "* Neural networks expect all input features to vary in a similar way, and ideally to look like standard normally distributed data, i.e. Gaussian with zero mean and unit variance.\n",
    "* BDTs just apply `if` statements on the features and are typically very robust.\n",
    "\n",
    "The `sklearn` documentation provides further information on [preprocessing of data][2].\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure\n",
    "[2]: https://scikit-learn.org/stable/modules/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Read about different scalers in `sklearn` ([visual overview](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#original-data)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example I\n",
    "One example that illustrates the importance of preprocessing the data to rescale feature values to \"standard ranges\" is the application of SVMs to the `sklearn`'s cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here a *linear support vector machine*, a common linear classification model, implemented in `LinearSVC` (support vector classifier). SVC try to find a (hyper-) plane in phase space that optimally separates the two given populations. \n",
    "* Predictions are then made by checking on which side of this plane new samples lie. \n",
    "* The (hyper-) plane is defined by only a few of the training samples (the supporting vectors of features); those that lie close to the resulting plane.\n",
    "* generative classification vs discriminative classification: \n",
    "    * generative classification: model each class and compute likelihood of belonging to each (e.g. Gaussian Naive Bayes)\n",
    "    * discriminative classification: find curve or manifold in feature space separating the classes from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVC and load data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset have very different ranges as illustrated by the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features\n",
    "plt.figure(figsize = (20, 8))\n",
    "plt.boxplot(cancer.data, showfliers = False) # boxes: lower to upper quartile, whiskers: Q1-1.5IQR/Q3+1.5IQR, outliert are not shown\n",
    "plt.xlabel(\"feature\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data without rescaling\n",
    "svm = SVC(gamma = \"auto\", random_state = 42)\n",
    "svm.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very low accuracy compared to our previous results.\n",
    "(Note that `SVC` by default uses `kernel=\"rbf\"`, i.e. a [radial-basis-function kernel][1]. The use of the Euclidean distance between the feature vectors may be causing this. If we instead use `SVC(kernel=\"linear\")`, the resulting score is much higher.)\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rescale the features and retrain\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# compute minimum and maximum on the training data\n",
    "# note that we must train the scaler only on the training data (but not the full dataset)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "# retrain\n",
    "svm.fit(X_train_scaled, y_train).score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### you can plot it if you want to see the effect\n",
    "plt.figure(figsize = (20, 4))\n",
    "plt.boxplot(scaler.transform(cancer.data), showfliers = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same effect (although much smaller) e.g. on MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "print(\"Score using unscaled data:\", mlp.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", mlp.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no effect e.g. on BDT\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "bdt = AdaBoostClassifier(random_state=0)\n",
    "print(\"Score using unscaled data:\", bdt.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"Score using rescaled data:\", bdt.fit(X_train_scaled, y_train).score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large values of features may also lead to convergence issues. \n",
    "(Note that many linear models include regularization terms that are introduced to avoid very large values of the parameters.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a quick function to define a datasets with two populations that should be easy to separate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(43)\n",
    "nsamples = 50\n",
    "pops     = 2\n",
    "yshift   = 0.5\n",
    "yscale   = 100 # try changing to 1 later\n",
    "noise    = 0.3\n",
    "\n",
    "### helpers\n",
    "def MakeXYcorrSample(x0 = 0, y0 = 0, noise = noise, yscale = yscale, pops = pops, yshift = yshift):\n",
    "    # space\n",
    "    X = np.zeros((nsamples * pops, 2))\n",
    "    y = np.zeros(nsamples * pops)\n",
    "    # fill\n",
    "    X[:, 0] = x0 + np.random.rand(nsamples * pops)\n",
    "    for n in range(pops):\n",
    "        X[n*nsamples:(n+1)*nsamples, 1] = (\n",
    "            yscale * (y0 + X[n*nsamples:(n+1)*nsamples, 0] - noise/2. + noise * np.random.rand(nsamples) + n*yshift)\n",
    "        )\n",
    "        y[n*nsamples:(n+1)*nsamples] = n\n",
    "    return X, y\n",
    "\n",
    "### make and plot dataset\n",
    "X, y = MakeXYcorrSample()\n",
    "for pop in range(pops):\n",
    "    plt.scatter(*X[y == pop,0:2].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: We have two populations that are obviously easy to separate? Or are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "### fit\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC().fit(X_train, y_train)\n",
    "\n",
    "print(\"Score on training data:\", model.score(X_train, y_train))\n",
    "print(\"Score on validation data:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training data shows that we cannot even model the training data correctly using the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools import  visualize_classifier\n",
    "visualize_classifier(model, X, y, cmap=\"rainbow\", plot_proba=False)\n",
    "\n",
    "# add line for separation\n",
    "line = np.linspace(0, 1)\n",
    "coef, intercept = model.coef_.flatten(), model.intercept_.flatten()\n",
    "plt.plot(line, -(line * coef[0] + intercept) / coef[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this problem how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Again, the scaling is the problem. Try to \n",
    "* change to rerun with `yscale` set to 1 (instead of `yscale = 100`) when generating the populations to separate\n",
    "* introduce a Scaler (like the one we used above)\n",
    "In both cases you should get 100 % separation both on the training and on the test sample.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
