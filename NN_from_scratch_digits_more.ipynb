{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reuses our self-made neural net from [a previous notebook][1] to train it on a more realistic (larger) dataset of digits (but still without splitting the data into a training and a test dataset as we would normally do to obtain an independent measurement of the accuracy on a dataset that the neural network has not seen before.)\n",
    "\n",
    "[1]: NN_from_scratch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the functions that stay unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def argmax(l):\n",
    "    return l.index(max(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    # outputs = two arrays (one array of size 4 for the hidden layer plus one array of size 10 for the output layer)\n",
    "    return outputs \n",
    "\n",
    "def predict(network, input):\n",
    "    \"\"\"run input through the network and return output of last layer\"\"\"\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the backpropagation function is mostly unchanged but we introduce a learning-rate parameter (`rate`), which is just a multiplicative factor for the adjustments we make to the weights in each call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target, rate = 1.0):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]): # loop over weights of neurons in output layer\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]): # loop over output of neurons in hidden layer\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output * rate\n",
    "\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]]) # (*)\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]): # loop over weights of neurons in hidden layer\n",
    "        for j, input in enumerate(input_vector + [1]): # loop over output of neurons in first layer, i.e. the inputs\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input * rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with the digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the digits dataset from sklearn that [we have used before][1].\n",
    "\n",
    "[1]: ScilearnIntro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps consist in preprocessing the data to have it in a compatible format for our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images and flatten -- note we're normalizing to a mean value << 1\n",
    "images = [list(image.flatten()/16/8) for image in digits.images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target to one-hot encoding\n",
    "numbers = [[1 if i == j else 0 for i in range(10)]\n",
    "            for j in digits.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(20, 8), ncols=10, nrows=5)\n",
    "idx = 0\n",
    "for g in axes:\n",
    "    for ax in g:\n",
    "        if idx >= len(images): break\n",
    "        figure = images[idx]\n",
    "        ax.imshow(np.array(figure).reshape(8,8))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our network. The size of the digits is 8*8 pixels. We use 10 neurons in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 64  # each input is a vector of 64 pixels\n",
    "num_hidden = 10  # number of neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helper function computes the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "def accuracy(network, X, y):\n",
    "    total = float(len(y))\n",
    "    correct = sum([argmax(predict(network, input)) == argmax(y[idx]) for idx, input in enumerate(X)])\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start the training of the neural network. We will manually adjust the learning rate, starting with 1 and decreasing it later on. The `frac` variable determines which fraction of the digits dataset is used in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 1 # learning rate\n",
    "frac = 1 # fraction of sample to use in each round\n",
    "num_run = int(50/frac) # iterations over the subsample\n",
    "for x in range(num_run):\n",
    "    for input_vector, target_vector in random.sample(list(zip(images, numbers)), int(len(images)*0.1)):\n",
    "    #for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector, rate) # 65 µs/loop (4 hidden), 134 µs/loop (10 hidden), 380 µs/loop (32 hidden)\n",
    "    if x % (num_run / 10) == 0:\n",
    "        print(\"Iterations done: %d, accuracy: %.3f\" % (x, accuracy(network, images, numbers)))\n",
    "        #print(\"Iterations done: %d, accuracy: %.3f\" % (x, accuracy(network, inputs, targets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions show the weights of the neurons in the hidden layer as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x, y, hatch, color):\n",
    "    \"\"\"return a matplotlib 'patch' object with the specified\n",
    "    location, crosshatch pattern, and color\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                        hatch=hatch, fill=False, color=color)\n",
    "\n",
    "\n",
    "def show_weights(neuron_idx, ax):\n",
    "    weights = network[0][neuron_idx]\n",
    "\n",
    "    grid = [weights[row:(row+8)]      # turn the weights into a 5x5 grid\n",
    "            for row in range(0,64,8)] # [weights[0:5], ..., weights[20:25]]\n",
    "\n",
    "    pos = ax.imshow(grid,\n",
    "                    cmap=matplotlib.cm.coolwarm,\n",
    "                    interpolation='none', # plot blocks as blocks\n",
    "                    vmin = -8, vmax = 8) # define a unique range for all subplots\n",
    "    \n",
    "    # print bias\n",
    "    ax.set_xlabel(\"bias = %.2f\" % weights[25])\n",
    "    return pos\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3), ncols=num_hidden)\n",
    "for idx in range(num_hidden):\n",
    "    pos = show_weights(idx, ax[idx])\n",
    "    #fig.colorbar(pos, ax = ax[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_layer, cmap=matplotlib.cm.coolwarm)\n",
    "plt.xlabel(\"Weight of hidden neuron (and bias)\")\n",
    "plt.ylabel(\"Output label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can continue the training by rerunning the cells above a few times and play with the `rate` and `frac` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
