{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A neural network \"from scratch\" using numpy\n",
    "\n",
    "This tutorial is adapted from the following article:\n",
    "\n",
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Each layer will be defined by a number of inputs and outputs and an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "First, the layers have to be initialized. This means we need to choose start values for the parameters. For the weights, this is most important since we need to \"break the symmetry\" - if all weights are the same the gradient will start at 0 and no optimization is possible.\n",
    "\n",
    "Therefore the weights are typically initialized in some random way. Here we just use a scaled Gaussian random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Next, we need to define the activation functions and the derivatives for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    dZ = (Z >= 0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "The output vector $\\mathbf{a}$ of each layer is calculated by\n",
    "\n",
    "$\\mathbf{a} = \\sigma(\\mathbf{z}) = \\sigma(W \\mathbf{a}_\\mathrm{prev} + \\mathbf{b})$\n",
    "\n",
    "where $\\mathbf{a}_\\mathrm{prev}$ is the output vector of the previous layer (or the input vector in case of the first layer), $W$ is the weight matrix and $\\mathbf{b}$ is the bias vector. The activation function $\\sigma$ is applied component-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.matmul(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When propagating forward through the layers, we will keep the intermediate results in a memory dictionary which we can reuse later for calculating the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We will use this neural network for a classification task. The figure of merit to minimize is the binary cross entropy\n",
    "\n",
    "\\begin{equation}\n",
    "L = -\\frac{1}{m}\\sum_{i=0}^{m} \\left(y_i \\ln \\hat{y}_i + (1-y_i)\\ln(1-\\hat{y}_i)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "With the vector of NN outputs $\\mathbf{\\hat{y}}$ and the vector of true labels $\\mathbf{y}$ for $m$ training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_value(Y_hat, Y):\n",
    "    return - np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative w.r.t. the NN output $\\hat{\\mathbf{y}}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\left(\\frac{y_i}{\\hat{y}_i}-\\frac{1-y_i}{1-\\hat{y}_i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We leave out the $\\frac{1}{m}$ here, since we will take the mean of the gradients for all training examples in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(Y_hat, Y):\n",
    "    return - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the gradient - \"Back propagation\"\n",
    "\n",
    "The gradient w.r.t. the parameters of all layers can be calculated by using the chain rule. We are \"propagating back the errors on the Loss\" through the network. Reminder: the output of each layer was given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a} = \\sigma(\\mathbf{z}) = \\sigma(W \\mathbf{a}_\\mathrm{prev} + \\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivatives of the Loss w.r.t the weights, biases of a layer can be calculated starting from the partial derivatives w.r.t the output of the following layer $\\frac{\\partial L}{\\partial \\mathbf{a}}$ that was calculated previously.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}} &= \\frac{\\partial L}{\\partial \\mathbf{a}}\\circ \\pmb{\\sigma}'\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial W}\n",
    "&= \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\mathbf{a}^T\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}} &= \\frac{\\partial L}{\\partial \\mathbf{z}}\n",
    "\\end{align}\n",
    "\n",
    "To continue backwards to the previous layer we also need the partial derivatives w.r.t the output of the previous layer\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{a}_\\mathrm{prev}} = W^T \\cdot \\frac{\\partial L}{\\partial \\mathbf{z}}\n",
    "\\end{equation}\n",
    "\n",
    "The symbol \"$\\circ$\" means element wise multiplication, the symbol \"$\\cdot$\" matrix multiplication. Note the vectors and matrices in the equations above - these gradients are read like\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W} & = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\cdots & \\frac{\\partial L}{\\partial w_{1m}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\cdots & \\frac{\\partial L}{\\partial w_{2m}} \\\\\n",
    "& \\vdots &  & \\\\\n",
    "\\frac{\\partial L}{\\partial w_{n1}} & \\frac{\\partial L}{\\partial w_{n2}} & \\cdots & \\frac{\\partial L}{\\partial w_{nm}}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial\\mathbf{a}} &=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial a_1} \\\\\n",
    "\\frac{\\partial L}{\\partial a_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial a_n} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Compare these result to what you would get when using the chain rule, just for single numbers (lets assume we have just one neuron in each layer):\n",
    "\n",
    "\\begin{align}\n",
    "a &= \\sigma(z) = \\sigma(wa_\\mathrm{prev} + b)\n",
    "\\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial z} &= \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z} = \\frac{\\partial L}{\\partial a} \\sigma'\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial w} &= \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial z} a\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial z}\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial a_\\mathrm{prev}} &= \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial z} w\n",
    "\\end{align}\n",
    "\n",
    "So going to multiple neurons per layer essentially means replacing weights by the weight matrices and biases and outputs by the bias and output vectors (need to figure out how to transpose them correctly). In the implemntation below each output array has an additional dimension that corresponds to the index of the training example - that way we can efficiently calculate the gradient for all training examples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        derivative_activation_func = relu_derivative\n",
    "    elif activation is \"sigmoid\":\n",
    "        derivative_activation_func = sigmoid_derivative\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "            \n",
    "    dZ_curr = dA_curr * derivative_activation_func(Z_curr)\n",
    "    dW_curr = np.matmul(\n",
    "        dZ_curr,\n",
    "        # need to transpose only the last 2 dimensions, \n",
    "        # since the first dimension is the training example index\n",
    "        np.transpose(A_prev, (0, 2, 1))\n",
    "    )\n",
    "    db_curr = dZ_curr\n",
    "    dA_prev = np.matmul(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    dA_prev = get_loss_derivative(Y_hat, Y)\n",
    "     \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = np.mean(dW_curr, axis=0)\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = np.mean(db_curr, axis=0)\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimize the loss using Gradient descent\n",
    "\n",
    "Since we now have the gradient w.r.t. all parameters we can follow the gradient to a potential minimum by successively subtracting it from the current parameter values. This introduces an Hyperparameter - the \"learning rate\" that corresponds to a step size we need to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the gradient for all training examples for each optimization step (parameter update). One iteration over the whole training dataset is called one \"epoch\". In practice this is typically done in a stochastic way - called \"Stochastic Grandient Descent\" (**SGD**) where the gradient is evaluated on random subsets (\"batches\"), which typically leads to faster convergence since less iterations over the whole dataset are nescessary. For simplicity we stick here with calculating each gradient step for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    loss_history = []\n",
    "    \n",
    "    # the operations assume\n",
    "    # the outputs of the last layer to be a 1x1 matrix\n",
    "    Y = Y.reshape(-1, 1, 1)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X.reshape(list(X.shape)+[1]), params_values, nn_architecture)\n",
    "        loss = get_loss_value(Y_hat, Y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run!\n",
    "\n",
    "Lets try it out with a toy dataset of 2 \"moons\" in a 2-dimensional input feature space. Our goal is to learn which points in this space correspond to label 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(1000, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y==0][:,0], X[y==0][:,1], \".\", label=\"0\")\n",
    "plt.plot(X[y==1][:,0], X[y==1][:,1], \".\", label=\"1\")\n",
    "plt.legend(title=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a 1-hidden-layer NN (2 layers in total) should be sufficient, but feel free to add more layers for experimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loss_history = train(X, y, nn_architecture=nn_architecture, epochs=1000, learning_rate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization seems to have converged to some minimum. Lets visualize the NN output in the 2-dimensional input space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a uniform grid of values around the training data\n",
    "step = 0.1\n",
    "grid = np.meshgrid(\n",
    "    np.arange(X[:,0].min(), X[:,0].max()+step, step),\n",
    "    np.arange(X[:,1].min(), X[:,1].max()+step, step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack and reshape into an array which we can pass through the NN\n",
    "xp = np.stack([grid[0].reshape(-1), grid[1].reshape(-1)], axis=1).reshape(-1, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output - will be values betwenn 0 and 1 \n",
    "scores, _ = full_forward_propagation(xp, params, nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_scores(X, y, grid, scores):\n",
    "    plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "    plt.colorbar(label=\"NN output\")\n",
    "    plt.scatter(X[y==1][:,0], X[y==1][:,1], marker=\".\", edgecolors=\"black\", color=\"red\")\n",
    "    plt.scatter(X[y==0][:,0], X[y==0][:,1], marker=\".\", edgecolors=\"black\", color=\"blue\")\n",
    "    plt.xlim(grid[0].min(), grid[0].max())\n",
    "    plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_and_scores(X, y, grid, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross check with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sklearn = MLPClassifier(learning_rate_init=0.1)\n",
    "model_sklearn.fit(X, y)\n",
    "plot_data_and_scores(X, y, grid, model_sklearn.predict_proba(xp.reshape(-1, 2))[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was now a much more complex setup, due to the many default settings in sklearn. It used 100 neurons, a more sophisticated optimization algorithm, batch gradient descent and a regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following settings should match more or less what we tried to do manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sklearn_simple = MLPClassifier(\n",
    "    hidden_layer_sizes=(16,),\n",
    "    learning_rate_init=1.,\n",
    "    momentum=0.,\n",
    "    alpha=0.,\n",
    "    batch_size=len(X),\n",
    "    solver=\"sgd\",\n",
    "    max_iter=1000,\n",
    "    random_state=99\n",
    ")\n",
    "model_sklearn_simple.fit(X, y)\n",
    "plot_data_and_scores(X, y, grid, model_sklearn_simple.predict_proba(xp.reshape(-1, 2))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
