{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_settings import set_tf_nthreads\n",
    "set_tf_nthreads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks -- some extra tricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the same as what we have done in the notebook \n",
    "on the [Higgs Challenge Example using Neural Networks](HiggsChallenge-NN_DL.ipynb)\n",
    "but here we're going to use a neural network with a more complex (deeper) structure (deeper = more layers) and we are optimizing the usage of event weights\n",
    "to squeeze out even a bit more performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "columns = list(X.columns)\n",
    "X = X.to_numpy()\n",
    "y = df['Label'].to_numpy()\n",
    "weight = df['Weight'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the [approximate median significance][1] from the Kaggle competition to determine how good a solution was. Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweight the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools import ams\n",
    "ams??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total weights (yields)\n",
    "sigall  = weight.dot(y)\n",
    "backall = weight.dot(y == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling\n",
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features. Also, let's set the -999 values to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "X_train[X_train==-999.] = 0.\n",
    "X_test[X_test==-999.] = 0.\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral networks with Keras\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probably want to use something like Keras instead. \n",
    "\n",
    "Example for a deeper NN using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=128, input_shape=X_train.shape[1:]),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(units=128),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(units=128),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(units=1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dense`: \"Just your regular densely-connected NN layer.\"\n",
    "  * implements the operation: output = activation(dot(input, kernel) + bias)\n",
    "    * kernel is a weights matrix created by the layer\n",
    "    * bias is a bias vector created by the layer (only applicable if `use_bias` is True)\n",
    "  * `units`: dimensionality of the output array\n",
    "  * `input_shape`: expected shape of the input arrays (only needed for first layer)\n",
    "  * `activation`: element-wise activation function\n",
    "  * `kernel_regularizer`: constraint function applied to the kernel weights matrix (see [constraints][1])\n",
    "* `BatchNormalization` : Technical trick to adjust weights and speedup computation - also acts as mild regularization (see [BatchNormalization][2])\n",
    "* `Activation`: Specify activation function (see [activation discussion](NN_Activation.ipynb))\n",
    "  \n",
    "  \n",
    "[1]: https://keras.io/constraints/\n",
    "[2]: https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/\n",
    "\n",
    "Note: There is a common practice to use powers of 2 (or multiples of powers of 2) for parameters that describe shapes of vectors in NN computations. Most of the time this does not matter, but sometimes it does, e.g. see this [tweet from Andrej Karpathy](https://twitter.com/karpathy/status/1621578354024677377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # or weighted metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `optimizer`: name of optimizer or optimizer instance. See [optimizers][1].\n",
    "  * _Adam_: an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments ([paper][2], a short [summary][4])\n",
    "* `loss`: name of objective function or objective function. See [losses][3].\n",
    "  * _binary crossentropy_: a measure of dissimilarity, used here to define the loss function that should be minimized \n",
    "    $$H_p(q) = -\\frac{1}{N}\\sum_{i=1}^N [{y_i} \\log(\\hat{y}_i)+(1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "    * also called *log loss*: we *maximize the predicted probability* of the true label by *minimizing the negative logarithm of it* (compare e.g. [Maximum Likelihood method](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation))\n",
    "       * here the true labels are $y_i=1$ for the positive class and $y_i=0$ for the negative class\n",
    "       * the estimated probabilities are $\\hat y_{i}$\n",
    "       * $N$ runs over all samples\n",
    "    *  \"the cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\"    \n",
    "    * see [this video by Aurélien Géron](https://www.youtube.com/watch?v=ErfnhcEV1O8) for an explanation of the information theory interpretation of this loss\n",
    "* `metrics`: list of metrics to be evaluated by the model during training and testing (typically accuracy)\n",
    "\n",
    "[1]: https://keras.io/optimizers/\n",
    "[2]: https://arxiv.org/abs/1412.6980v8\n",
    "[3]: https://keras.io/losses/\n",
    "[4]: https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another innovation we're introducing here is reweighting of the events. We are doing three things here:\n",
    "1. Applying event-based weights which are stored in `weight_train` (and `weight_test`). This helps to give more weight (in the computation of the loss function) to backgrounds events that have larger cross sections and are therefore more important to suppress than others.\n",
    "1. Reweighting the signal and background back such that their total weight is again about the same. Note that the unweighted sample has a ratio of about 1:2 for signal:background events, and we had seen that after applying the weight this ratio was reduced to about 1:500. Such a drastic difference in the weights can cause problems in the training, therefore we restore a roughly equal total weight by multiplying with the two (global) weights for signal and background we compute in `class_weight`.\n",
    "1. Normalizing the weights, such that the mean weight is 1. This avoids producing an overall shift in the loss value which would mean we also have to shift optimization parameters (like learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = np.array([\n",
    "    len(y_train) / weight_train[y_train==0].sum(),\n",
    "    len(y_train) / weight_train[y_train==1].sum(),\n",
    "])\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_tot = weight_train * class_weight[y_train.astype(int)]\n",
    "weight_test_tot = weight_test * class_weight[y_test.astype(int)]\n",
    "weight_train_tot /= weight_train_tot.mean()\n",
    "weight_test_tot /= weight_test_tot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_tot[y_train==0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_train_tot[y_train==1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    sample_weight=weight_train_tot,\n",
    "    validation_data=(X_test_scaled, y_test, weight_test_tot),\n",
    "    callbacks=[EarlyStopping(verbose=True, patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `batch_size`: number of samples per gradient update\n",
    "* `epochs`: number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training history returned by model.fit\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "y_test_prob_keras = model.predict(X_test_scaled)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the AMS scan\n",
    "from sklearn.metrics import roc_curve\n",
    "def ams_scan(y, y_prob, weights, label):\n",
    "    fpr, tpr, thr = roc_curve(y, y_prob, sample_weight=weights)\n",
    "    ams_vals = ams(tpr * sigall, fpr * backall)\n",
    "    print(\"{}: Maximum AMS {:.3f} for pcut {:.3f}\".format(label, ams_vals.max(), thr[np.argmax(ams_vals)]))\n",
    "    return thr, ams_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob_keras, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob_keras, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0.8, 1.)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
