{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Keras\n",
    "\n",
    "The most convenient way to use TensorFlow with neural networks is through [Keras](http://keras.io). It provides a high-level interface that is somewhat a compromise between very high-level abstractions like scikit-learn and the complete control of every detail you get when directly using the low-level APIs of libraries like TensorFlow. There is a separate [Keras Documentation](https://keras.io), as well as [Guides](https://www.tensorflow.org/guide/keras), [Tutorials](https://www.tensorflow.org/tutorials/keras), and the [Keras section on the TensorFlow API Documentation](https://www.tensorflow.org/api_docs/python/tf/keras).\n",
    "\n",
    "In the past, besides TensorFlow, Keras also supported [Theano](http://www.deeplearning.net/software/theano/) and [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/). Since end of 2019, the multi-backend version of keras is deprecated and development only continues in the version that is included in TensorFlow. Keras is now also the recommended/default way to work with neural networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick example, let's again build a model to classify the \"Moons\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = make_moons(n_samples=10000, noise=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways to use Keras - via the Sequential API, the Functional API or via creating layers and models by subclassing. Lets start with `Sequential`. This is convenient for all models where we just have one input and one output Tensor with stacked Layers in between. Here we use the `Dense` layer - which is precisely the fully connected NN layer that applies the $\\sigma(W\\mathbf{x} + \\mathbf{b})$ operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Hidden layer with 2 inputs, 16 outputs\n",
    "    Dense(16, activation=\"relu\", input_shape=(2,)),\n",
    "    # Output layer with 16 inputs (determined automatically) and 1 output\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much parameters will our model have? The answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the underlying Tensors if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models and layers are callables, so you can feed them tensors to get transformed outputs. This can be very useful to experiment and understand what transformations are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = Dense(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.matmul(inputs, layer.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run the training, we have to \"compile\" the model. This will configure the loss function and optimization Algorithm. You cat pass each loss from [`keras.losses`](https://keras.io/losses) and each optimizer from [`keras.optimizers`](https://keras.io/optimizers) also as a string with the name if you want to use it with default parameters. Here we want to use the \"Adam\" optimizer with an adjusted initial learning rate, so we pass it directly.\n",
    "\n",
    "We could also pass some metrics that we want to monitor during training (in addition to the Loss value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for fitting looks similar to scikit-learn, but has additional options. There also is a [scikit-learn API  wrapper](https://github.com/adriangb/scikeras) for Keras if you need that in some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "The model can be run using `model.predict` or simply calling it like a function on an input. The main difference is that `model.predict` supports several parameters (like `batch_size`) and returns a numpy array whereas calling the model like a function returns a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max(), 0.1),\n",
    "    np.arange(x[:,1].min(), x[:,1].max(), 0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy = np.stack([grid[0].ravel(), grid[1].ravel()], axis=1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = model(xy).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "plt.colorbar(label=\"NN output\")\n",
    "opts = dict(alpha=0.1, marker=\".\", edgecolors=\"black\")\n",
    "plt.scatter(x[y==0][:,0], x[y==0][:,1], color=\"blue\", **opts)\n",
    "plt.scatter(x[y==1][:,0], x[y==1][:,1], color=\"red\", **opts)\n",
    "plt.xlim(grid[0].min(), grid[0].max())\n",
    "plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "The functional API is very similar to the old (TensorFlow 1) low-level API. That means the computation graph is first build in an abstract way (just specifying input/output shapes, but no data yet). Each layer can be called as a function on an input Tensor and return an output Tensor. One can then build arbitrary computation graphs and finally build a model by passing the input and output Tensors. This is especially useful when we want to organize the processing into different inputs and different outputs or if you want to build computation graphs that have branches.\n",
    "\n",
    "Suppose we want to do some strangely complicated processing of the \"California housing dataset\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's put it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_housing = pd.DataFrame(data.data)\n",
    "df_housing.columns = data.feature_names\n",
    "df_housing['MedHouseVal'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the following funny exercise:\n",
    "* Feed the Latitude and Longitude through a separate NN layer\n",
    "* Combine the output of this layer with the other inputs (except for the median income)\n",
    "* Add another hidden layer\n",
    "* Add a target where we first try to predict the median income\n",
    "* Feed back this predicted median income  together with the outputs of the NN into another hidden layer\n",
    "* Finally predict the median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# For such more complicated structures it is often useful to give the layers names\n",
    "\n",
    "inp_feat = Input((5,), name=\"Features\") # ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
    "inp_coord = Input((2,), name=\"Coordinates\") # ['Latitude', 'Longitude']\n",
    "hl_coord = Dense(64, activation=\"relu\")(inp_coord)\n",
    "joined_inp = concatenate([inp_feat, hl_coord])\n",
    "hl = Dense(64, activation=\"relu\")(joined_inp)\n",
    "# no activation function here, this will be a regression target\n",
    "out_MedInc = Dense(1, name=\"MedIncOutput\")(hl)\n",
    "joined_inp2 = concatenate([hl, out_MedInc])\n",
    "hl2 = Dense(64, activation=\"relu\")(joined_inp2)\n",
    "out_HouseValue = Dense(1, name=\"HouseValueOutput\")(hl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a model now with 2 inputs and 2 outputs. We can use `keras.models.Model` to create models with arbitrary many inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_model = tf.keras.Model(\n",
    "    inputs={\n",
    "        \"Features\" : inp_feat,\n",
    "        \"Coordinates\" : inp_coord\n",
    "    },\n",
    "    outputs={\n",
    "        \"MedIncOutput\" : out_MedInc,\n",
    "        \"HouseValueOutput\" : out_HouseValue\n",
    "    }\n",
    ")\n",
    "housing_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras comes with [plotting utilities](https://keras.io/api/utils/model_plotting_utils) that allow a graph visualization for models created with the functional API. Let's check if we stitched the layers together as planned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(housing_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we named the inputs and outputs, we can give input and target data as dictionaries, but before that we want to standardize both the inputs and the targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates = ['Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_trf = df_housing.copy()\n",
    "df_trf[:] = scaler.transform(df_housing.values)\n",
    "df_trf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_housing = {\n",
    "    \"Features\" : df_trf[features].values,\n",
    "    \"Coordinates\" : df_trf[coordinates].values,\n",
    "}\n",
    "y_housing = {\n",
    "    \"MedIncOutput\" : df_trf[\"MedInc\"].values.reshape(-1, 1),\n",
    "    \"HouseValueOutput\" : df_trf[\"MedHouseVal\"].values.reshape(-1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify loss functions for all outputs. If different outputs should be trained with different loss functions, you need to pass a list. The total loss will be the sum of the individual losses. One could also pass `loss_weights` to weight them relative to each other, but we don't do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_model.fit(x_housing, y_housing, epochs=10, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we predict the median income and finally the house price correctly? Let's have a look at the distributions for true and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = housing_model(x_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = dict(alpha=0.5, bins=100, range=(-3, 5))\n",
    "plt.hist(df_trf[\"MedInc\"], label=\"True\", **opt)\n",
    "plt.hist(predictions[\"MedIncOutput\"].numpy().reshape(-1), label=\"Predicted\", **opt)\n",
    "plt.xlabel(\"Median income (rescaled)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = dict(alpha=0.5, bins=100, range=(-3, 5))\n",
    "plt.hist(df_trf[\"MedHouseVal\"], label=\"True\", **opt)\n",
    "plt.hist(predictions[\"HouseValueOutput\"].numpy().reshape(-1), label=\"Predicted\", **opt)\n",
    "plt.xlabel(\"Median House value (rescaled)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclass API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For maximum flexibility you can also inherit from `tf.keras.models.Model` or `tf.keras.layers.Layer` and implement your own forward pass. This is very similar to how [PyTorch models are commonly built](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).\n",
    "\n",
    "Both for models and for layers the minimum amount of methods that you have to implement are `__init__`, where you typically define parameters and any state and then the forward pass in `call`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDenseReluLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # call the base class constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize weights\n",
    "        self.kernel = tf.Variable(tf.random.uniform((n_inputs, n_outputs)))\n",
    "        self.biases = tf.Variable(tf.zeros(n_outputs))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(tf.matmul(inputs, self.kernel) + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom layers can be arbitrarily combined with existing layers e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "composed_model = tf.keras.models.Sequential([\n",
    "        MyDenseReluLayer(2, 5),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "composed_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "composed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can also be used as layers for new models and you can use existing layers as members of custom layers etc.\n",
    "\n",
    "More information can be found at https://keras.io/guides/making_new_layers_and_models_via_subclassing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "Sometimes the whole training data might not fit into memory or you might want to do some live pre-processing. The simplest way to do this is via [python generators](https://wiki.python.org/moin/Generators). For maximum efficiency it's worth having a look at [tf.data](https://www.tensorflow.org/guide/data).\n",
    "\n",
    "Let's write a generator that yields an infinite amount of mini batches for our \"moon\" dataset. The generator should yield batches of (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def moon_generator(batch_size=128, buffer_size=10000):\n",
    "    # let's make an infinite generator\n",
    "    # - in each pass of the loop we will generate `buffer_size` training examples\n",
    "    while True:\n",
    "        x, y = make_moons(n_samples=buffer_size, noise=0.4)\n",
    "        # this is the loop over mini-batches\n",
    "        for start in range(0, buffer_size, batch_size):\n",
    "            yield x[start : start + batch_size], y[start : start + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an overly complicated model and train it with \"infinite data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_model = tf.keras.models.Sequential([\n",
    "    Dense(1024, activation=\"relu\", input_shape=(2,)),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our generator is infinite we have to pass the `steps_per_epoch` Argument in `fit` that defines how many batches should be used until one epoch is declared finished. Finite generatiors can be created by inheriting from [`tf.keras.utils.Sequence`](https://keras.io/api/utils/python_utils/#sequence-class) or by using [`tf.data`](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_model.fit(moon_generator(), steps_per_epoch=200, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_with_generator(model, generator, steps=5):\n",
    "\n",
    "    # for plotting, just draw a few examples from the generator\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(steps):\n",
    "        data = next(generator)\n",
    "        x.append(data[0])\n",
    "        y.append(data[1])\n",
    "    x = np.concatenate(x)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    grid = np.meshgrid(\n",
    "        np.arange(x[:,0].min(), x[:,0].max(), 0.1),\n",
    "        np.arange(x[:,1].min(), x[:,1].max(), 0.1),\n",
    "    )\n",
    "    \n",
    "    xy = np.stack([grid[0].ravel(), grid[1].ravel()], axis=1)    \n",
    "    scores = model.predict(xy)\n",
    "\n",
    "    plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "    plt.colorbar(label=\"NN output\")\n",
    "    opts = dict(alpha=0.2, marker=\".\", edgecolors=\"black\")\n",
    "    plt.scatter(x[y==0][:,0], x[y==0][:,1], color=\"blue\", **opts)\n",
    "    plt.scatter(x[y==1][:,0], x[y==1][:,1], color=\"red\", **opts)\n",
    "    plt.xlim(grid[0].min(), grid[0].max())\n",
    "    plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_with_generator(stupid_model, moon_generator(), steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models created with the Sequential or functional API it is easy to create new models that evaluate only part of the computation graph.\n",
    "Let's use this to visualize the hidden layers of our first neural network in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = tf.keras.Model(inputs=[model.input], outputs=[model.layers[0].output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed it with a regular grid again for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max()+step, step),\n",
    "    np.arange(x[:,1].min(), x[:,1].max()+step, step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np.stack([grid[0].ravel(), grid[1].ravel()], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_out = hidden_output(xp).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    axs.ravel()[i].contourf(grid[0], grid[1], hl_out[:,i].reshape(grid[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[1].weights[0]\n",
    "bias = model.layers[1].weights[1]\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=16, ncols=2, figsize=(2 * 2, 2 * 16))\n",
    "total = np.zeros_like(hl_out[:, 0])\n",
    "for i in range(16):\n",
    "    total += weights[i, 0] * hl_out[:, i]\n",
    "    axs[i, 0].contourf(grid[0], grid[1], hl_out[:,i].reshape(grid[0].shape))\n",
    "    axs[i, 0].set_title(f\"+ {weights[i, 0]:.3f} *\")\n",
    "    axs[i, 1].contourf(grid[0], grid[1], total.numpy().reshape(grid[0].shape))\n",
    "    axs[i, 1].set_title(\"=\")\n",
    "    axs[i, 0].set_axis_off()\n",
    "    axs[i, 1].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a nice idea about how a NN composes it's output by combining the outputs of the previous layer. A nice visualization of this can be seen at https://playground.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
