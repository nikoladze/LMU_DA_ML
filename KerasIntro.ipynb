{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Keras\n",
    "\n",
    "The most convenient way to use TensorFlow with neural networks is through [Keras](http://keras.io). It provides a high-level interface that is somewhat a compromise between very high-level abstractions like scikit-learn and the complete control of every detail you get when directly using the low-level APIs of libraries like TensorFlow.\n",
    "\n",
    "Besides TensorFlow, Keras also supports [Theano](http://www.deeplearning.net/software/theano/) and [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/). Code that only uses the high-level Keras API should be able to run in Theano and CNTK as well by just switching the backend. If you only want to use TensorFlow, you can use the version of keras that is directly included in TensorFlow (`tensorflow.keras`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick example, let's again build a model to classify our \"Moons\" dataset from [NNFromScratch.ipynb](NNFromScratch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = make_moons(n_samples=10000, noise=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 main ways to use Keras - via the Sequential or the Functional API. Lets start with `Sequential`. This is convenient for all models where we just have one input and one output Tensor with stacked Layers in between. Here we use the `Dense` layer - which is precisely the fully connected NN layer that applies the $\\sigma(W\\mathbf{x} + \\mathbf{b})$ operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # Hidden layer with 2 inputs, 16 outputs\n",
    "    Dense(16, activation=\"relu\", input_shape=(2,)),\n",
    "    # Output layer with 16 inputs (determined automatically) and 1 output\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much parameters will our model have? The answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the underlying Tensors if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.summary.writer.writer import FileWriter\n",
    "FileWriter('logs/train', graph=tf.get_default_graph()).close()\n",
    "%tensorboard --logdir logs/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run the training, we have to \"compile\" the model. This will configure the loss function and optimization Algorithm. You cat pass each loss from [`keras.losses`](https://keras.io/losses) and each optimizer from [`keras.optimizers`](https://keras.io/optimizers) also as a string with the name if you want to use it with default parameters. Here we want to use the \"Adam\" optimizer with an adjusted initial learning rate, so we pass it directly.\n",
    "\n",
    "We could also pass some metrics that we want to monitor during training (in addition to the Loss value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.1), loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for fitting looks similar to scikit-learn, but has additional options. In fact there also is a [scikit-learn API  wrapper](https://keras.io/scikit-learn-api/) for Keras if you need that in some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, y, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `History` object contains the monitored metrics (by default the loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "The model can be run using `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max(), 0.1),\n",
    "    np.arange(x[:,1].min(), x[:,1].max(), 0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xy = np.stack([grid[0].ravel(), grid[1].ravel()], axis=1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = model.predict(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "plt.colorbar(label=\"NN output\")\n",
    "opts = dict(alpha=0.1, marker=\".\", edgecolors=\"black\")\n",
    "plt.scatter(x[y==0][:,0], x[y==0][:,1], color=\"blue\", **opts)\n",
    "plt.scatter(x[y==1][:,0], x[y==1][:,1], color=\"red\", **opts)\n",
    "plt.xlim(grid[0].min(), grid[0].max())\n",
    "plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "The functional API is very similar to the low-level TensorFlow API. Each layer can be called as a function on an input Tensor and return an output Tensor. One can then build arbitrary computation graphs and finally build a model by passing the input and output Tensors. This is especially useful when we want to organize the processing into different inputs and different outputs or if you want to build computation graphs that have branches.\n",
    "\n",
    "Suppose we want to do some strangely complicated processing of the \"California housing dataset\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = california_housing.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's put it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_housing = pd.DataFrame(data.data)\n",
    "df_housing.columns = data.feature_names\n",
    "df_housing['MedHouseVal'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the following funny exercise:\n",
    "* Feed the Latitude and Longitude through a separate NN layer\n",
    "* Combine the output of this layer with the other inputs (except for the median income)\n",
    "* Add another hidden layer\n",
    "* Add a target where we first try to predict the median income\n",
    "* Feed back this predicted median income  together with the outputs of the NN into another hidden layer\n",
    "* Finally predict the median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# For such more complicated structures it is often useful to give the layers names\n",
    "\n",
    "inp_feat = Input((5,), name=\"Features\") # ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
    "inp_coord = Input((2,), name=\"Coordinates\") # ['Latitude', 'Longitude']\n",
    "hl_coord = Dense(64, activation=\"relu\")(inp_coord)\n",
    "joined_inp = concatenate([inp_feat, hl_coord])\n",
    "hl = Dense(64, activation=\"relu\")(joined_inp)\n",
    "# no activation function here, this will be a regression target\n",
    "out_MedInc = Dense(1, name=\"MedIncOutput\")(hl)\n",
    "joined_inp2 = concatenate([hl, out_MedInc])\n",
    "hl2 = Dense(64, activation=\"relu\")(joined_inp2)\n",
    "out_HouseValue = Dense(1, name=\"HouseValueOutput\")(hl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a model now with 2 inputs and 2 outputs. We can use `keras.models.Model` to create models with arbitrary many inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_model = keras.models.Model(inputs=[inp_feat, inp_coord], outputs=[out_MedInc, out_HouseValue])\n",
    "housing_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(housing_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we named the inputs and outputs, we can give input and target data as dictionaries, but before that we want to standardize both the inputs and the targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coordinates = ['Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_trf = df_housing.copy()\n",
    "df_trf[:] = scaler.transform(df_housing.values)\n",
    "df_trf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_housing = {\n",
    "    \"Features\" : df_trf[features].values,\n",
    "    \"Coordinates\" : df_trf[coordinates].values,\n",
    "}\n",
    "y_housing = {\n",
    "    \"MedIncOutput\" : df_trf[\"MedInc\"].values.reshape(-1, 1),\n",
    "    \"HouseValueOutput\" : df_trf[\"MedHouseVal\"].values.reshape(-1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify loss functions for all outputs. If different outputs should be trained with different loss functions, you need to pass a list. The total loss will be the sum of the individual losses. One could also pass `loss_weights` to weight them relative to each other, but we don't do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_model.fit(x_housing, y_housing, epochs=10, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we predict the median income and finally the house price correctly? Let's have a look at the distributions for true and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = housing_model.predict(x_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = dict(alpha=0.5, bins=100, range=(-3, 5))\n",
    "plt.hist(df_trf[\"MedInc\"], label=\"True\", **opt)\n",
    "plt.hist(predictions[0], label=\"Predicted\", **opt)\n",
    "plt.xlabel(\"Median income (rescaled)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = dict(alpha=0.5, bins=100, range=(-3, 5))\n",
    "plt.hist(df_trf[\"MedHouseVal\"], label=\"True\", **opt)\n",
    "plt.hist(predictions[1], label=\"Predicted\", **opt)\n",
    "plt.xlabel(\"Median House value (rescaled)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "Sometimes the whole training data might not fit into memory or you might want to do some live pre-processing. The simplest way to do this is via [python generators](https://wiki.python.org/moin/Generators)\n",
    "\n",
    "Let's write a generator that yields an infinite amount of mini batches for our \"moon\" dataset. The generator should yield batches of (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def moon_generator(batch_size=128, buffer_size=10000):\n",
    "    # let's make an infinite generator\n",
    "    # - in each pass of the loop we will generate `buffer_size` training examples\n",
    "    while True:\n",
    "        x, y = make_moons(n_samples=buffer_size, noise=0.4)\n",
    "        # this is the loop over mini-batches\n",
    "        for start in range(0, buffer_size, batch_size):\n",
    "            yield x[start : start + batch_size], y[start : start + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an overly complicated model and train it with \"infinite data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stupid_model = keras.models.Sequential([\n",
    "    Dense(1024, activation=\"relu\", input_shape=(2,)),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stupid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stupid_model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`fit_generator`](https://keras.io/models/model/#fit_generator) method to train the model. Since our generator is infinite we have to pass the `steps_per_epoch` Argument that defines how many batches should be used until one epoch is declared finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stupid_model.fit_generator(moon_generator(), steps_per_epoch=200, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def validate_with_generator(model, generator, steps=5):\n",
    "\n",
    "    # for plotting, just draw a few examples from the generator\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(steps):\n",
    "        data = next(generator)\n",
    "        x.append(data[0])\n",
    "        y.append(data[1])\n",
    "    x = np.concatenate(x)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    grid = np.meshgrid(\n",
    "        np.arange(x[:,0].min(), x[:,0].max(), 0.1),\n",
    "        np.arange(x[:,1].min(), x[:,1].max(), 0.1),\n",
    "    )\n",
    "    \n",
    "    xy = np.stack([grid[0].ravel(), grid[1].ravel()], axis=1)    \n",
    "    scores = model.predict(xy)\n",
    "\n",
    "    plt.contourf(grid[0], grid[1], scores.reshape(grid[0].shape), cmap=\"Spectral_r\")\n",
    "    plt.colorbar(label=\"NN output\")\n",
    "    opts = dict(alpha=0.2, marker=\".\", edgecolors=\"black\")\n",
    "    plt.scatter(x[y==0][:,0], x[y==0][:,1], color=\"blue\", **opts)\n",
    "    plt.scatter(x[y==1][:,0], x[y==1][:,1], color=\"red\", **opts)\n",
    "    plt.xlim(grid[0].min(), grid[0].max())\n",
    "    plt.ylim(grid[1].min(), grid[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validate_with_generator(stupid_model, moon_generator(), steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is very well possible to operate on the underlying Tensors with TensorFlow you can also do this via the `keras.backend` API. This module wraps the around all operations to provide the mapping to the different backends (Thensorflow, Theano, CNTK). In most examples you find people will call this module `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the hidden layers of our first neural network in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.get_layer('dense_1').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`K.function` creates a function for us that we can feed a numpy array later. The following function will give us the output vector of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = K.function([model.input], [model.get_layer('dense_1').output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed it with a regular grid again for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "grid = np.meshgrid(\n",
    "    np.arange(x[:,0].min(), x[:,0].max()+step, step),\n",
    "    np.arange(x[:,1].min(), x[:,1].max()+step, step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xp = np.c_[grid[0].ravel(), grid[1].ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hl_out = f([xp])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    axs.ravel()[i].contourf(grid[0], grid[1], hl_out[:,i].reshape(grid[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a nice idea about how a NN composes it's output by combining the outputs of the previous layer. A nice visualization of this can be seen at https://playground.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
