{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks\n",
    "In this part we continue to work with the data from the **[Higgs Boson ML Challenge][1]** on Kaggle and attempt a solution using neural networks (NN). See the [previous notebook][2] to get started.\n",
    "\n",
    "We start with some introductory information on [Neural Networks][3].\n",
    "\n",
    "[1]: https://www.kaggle.com/c/Higgs-boson\n",
    "[2]: HiggsChallenge.ipynb\n",
    "[3]: NN_Activation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks to discover the Higgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start trying to apply a NN to the Higgs Challenge data. We will start using Scikit Learn, and then try **[Keras](https://keras.io/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PRI_jet_leading_pt[df.PRI_jet_leading_pt>0].hist(bins=50)\n",
    "plt.yscale('log')\n",
    "\n",
    "f=plt.figure()\n",
    "df.DER_mass_MMC[(df.DER_mass_MMC>0)&(df.DER_mass_MMC<250)].hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks (MLP) in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's first look at a NN in sklearn\n",
    "from sklearn.neural_network import MLPClassifier # Multi-layer Perceptron classifier.\n",
    "mlp = MLPClassifier(verbose=True, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the [approximate median significance][1] from the Kaggle competition to determine how good a solution was. Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function to compute approximate median significance (AMS)\n",
    "%pycat ams.py\n",
    "%run ams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine probability scores\n",
    "y_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = mlp.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob']=mlp.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total weights (yields)\n",
    "sigall  = weight.dot(y)\n",
    "backall = weight.dot(y == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the AMS scan\n",
    "from sklearn.metrics import roc_curve\n",
    "def ams_scan(y, y_prob, weights, label):\n",
    "    fpr, tpr, thr = roc_curve(y, y_prob, sample_weight=weights)\n",
    "    ams_vals = ams(tpr * sigall, fpr * backall)\n",
    "    print(\"{}: Maximum AMS {:.3f} for pcut {:.3f}\".format(label, ams_vals.max(), thr[np.argmax(ams_vals)]))\n",
    "    return thr, ams_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Worse than the BDT from [HiggsChallenge.ipynb](HiggsChallenge.ipynb)\n",
    "![Comparison with submissions](figures/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling\n",
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.DER_mass_MMC.hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=plt.hist(X_train_scaled[:,0],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train a new network\n",
    "mlp_scaled = MLPClassifier(verbose=True, early_stopping=True)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_scaled.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine probability scores\n",
    "y_train_prob_scaled = mlp_scaled.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_scaled = mlp_scaled.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob_scaled, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob_scaled, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved quite a bit by using the same classifier but with rescaled data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral networks with Keras\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probably want to use something like Keras instead. \n",
    "\n",
    "Let's try to create a simple NN using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 100, activation='relu', input_shape=(30,), kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(units =   1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dense`: \"Just your regular densely-connected NN layer.\"\n",
    "  * implements the operation: output = activation(dot(input, kernel) + bias)\n",
    "    * kernel is a weights matrix created by the layer\n",
    "    * bias is a bias vector created by the layer (only applicable if `use_bias` is True)\n",
    "  * `units`: dimensionality of the output array (note: we do not need to specify to size of the input array, except...)\n",
    "  * `input_shape`: expected shape of the input arrays (...only needed for the first layer)\n",
    "  * `activation`: element-wise activation function\n",
    "  * `kernel_regularizer`: constraint function applied to the kernel weights matrix (see [constraints][1])\n",
    "  \n",
    "  \n",
    "[1]: https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model -- output to file\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following cell needs pydot - install with e.g.\n",
    "#!pip install --user pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize model -- inline (need pydot, graphviz and pydotplus)\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg').replace(b\"1.3333 1.3333\", b\"1 1 \")) # HACK: remove scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `optimizer`: name of optimizer or optimizer instance. See [optimizers][1].\n",
    "  * _Adam_: an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments ([paper][2], a short [summary][4])\n",
    "* `loss`: name of objective function or objective function. See [losses][3].\n",
    "  * _binary crossentropy_: \n",
    "    $$H_p(q) = -\\frac{1}{N}\\sum_{i=1}^N [{y_i} \\log(\\hat{y}_i)+(1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "    * a measure of dissimilarity, used here to define the loss function that should be minimized: \"The cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p.\"\n",
    "    (The minimum number of bits to encode an independent event that occurs with probability $y_i$ is $-\\log_2(y)$.)\n",
    "       * here the true labels are $y_i=1$ for the positive class and $y_i=0$ for the negative class\n",
    "       * the estimated probabilities are $\\hat y_{i}$\n",
    "       * $N$ runs over all samples\n",
    "* `metrics`: list of metrics to be evaluated by the model during training and testing (typically accuracy)\n",
    "\n",
    "[1]: https://keras.io/optimizers/\n",
    "[2]: https://arxiv.org/abs/1412.6980v8\n",
    "[3]: https://keras.io/losses/\n",
    "[4]: https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218\n",
    "[5]: https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Keras NN (much faster than the MLP)\n",
    "#model.fit(X_train_scaled, y_train, epochs=5, batch_size=128, sample_weight=weight_train)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `batch_size`: number of samples per gradient update\n",
    "* `epochs`: number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training history returned by model.fit\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc']) -- only available if we do validation split\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "y_test_prob_keras = model.predict(X_test_scaled)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob_keras, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob_keras, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only made a single layer NN in Keras. However, you can easily change the structure of the network. As an assignment, try adding an extra hidden layer and changing the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variations of MLP *(optional)*\n",
    "\n",
    "\n",
    "There are a few things we can easily vary: number of hidden layers, the activation function, the regularization ($\\alpha$). Let's go back to MLPClassifer (scaled) and play with some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_play = MLPClassifier(activation='relu', hidden_layer_sizes=(100,100), alpha=0.01, verbose=True, early_stopping=True)\n",
    "mlp_play.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_play.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob_play = mlp_play.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_play = mlp_play.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*ams_scan(y_train, y_train_prob_play, weight_train, \"Train\"), label=\"Train\")\n",
    "plt.plot(*ams_scan(y_test, y_test_prob_play, weight_test, \"Test\"), label=\"Test\")\n",
    "plt.xlim(0., 1.)\n",
    "plt.grid()\n",
    "plt.xlabel('Pcut')\n",
    "plt.ylabel('AMS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your tasks\n",
    "Problems (can do with either MLPClassifier or Keras):\n",
    "1. Vary the structure of the network (number of hidden layers, number of neurons)\n",
    "1. Vary the activation. (In Keras can do it per layer, in MLPClassifier only for all)\n",
    "1. Vary the regularization. May have to do this as the structure changes.\n",
    "1. Try using derivied variables only or primary variables only.\n",
    "1. Missing data is represented by -999 before scaling. Is there a better value to use in the training?\n",
    "1. Try using the event weights to better match the background and signal shapes in the training. Note, though, that you should still treat background and signal separately; don't scale the signal down by the weight."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
