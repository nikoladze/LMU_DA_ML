{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-and-Random-Forest-Models\" data-toc-modified-id=\"Decision-Tree-and-Random-Forest-Models-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Decision Tree and Random Forest Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-example\" data-toc-modified-id=\"Simple-example-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Simple example</a></span></li><li><span><a href=\"#Decision-tree-example-with-real-data\" data-toc-modified-id=\"Decision-tree-example-with-real-data-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Decision tree example with real data</a></span></li><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Feature importance</a></span></li></ul></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Random Forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-for-Cancer-Data\" data-toc-modified-id=\"Random-Forest-for-Cancer-Data-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Random Forest for Cancer Data</a></span></li><li><span><a href=\"#Random-Forest-for-Digits-Data\" data-toc-modified-id=\"Random-Forest-for-Digits-Data-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Random Forest for Digits Data</a></span></li><li><span><a href=\"#Add-on:-Try-a-more-complicated-dataset:-2-spirals.\" data-toc-modified-id=\"Add-on:-Try-a-more-complicated-dataset:-2-spirals.-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Add-on: Try a more complicated dataset: 2 spirals.</a></span></li></ul></li></ul></li><li><span><a href=\"#Further-reading\" data-toc-modified-id=\"Further-reading-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Further reading</a></span></li><li><span><a href=\"#Exercise:-Feed-rotated-features-to-the-Classifier\" data-toc-modified-id=\"Exercise:-Feed-rotated-features-to-the-Classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exercise: Feed rotated features to the Classifier</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "A further important model category. The basic principle is easy to understand:  \n",
    " Hierarchical series of  **if/else questions** \n",
    "\n",
    "*Example:* Game where you need to distinguish four kinds of animals:  \n",
    "* *Bear, Dolphin, Penguin, Hawk*\n",
    "\n",
    "Goal is to use as few questions as possible.\n",
    "\n",
    "One possible solution:\n",
    "\n",
    "![](figures/DT_animals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple example \n",
    "Illustrate DT with half-moon data, a simple dataset with half-moon shaped data distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,cmap='rainbow');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try previous models first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "print(\"GaussianNB: accuracy = %.2f\" % model.fit(X, y).score(X, y)) # 3. fit model to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "print(\"KNeighborsClassifier: accuracy = %.2f\" % model.fit(X, y).score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DT\n",
    "max_depth=3\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "tree=model.fit(X,y)\n",
    "tree.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to visualize DT\n",
    "#\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='RdBu', plot_proba=False):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, edgecolors=\"black\",\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    X = np.stack([xx.ravel(), yy.ravel()], axis=1)\n",
    "    \n",
    "    if not plot_proba:\n",
    "        Z = model.predict(X).reshape(xx.shape)\n",
    "    else:\n",
    "        Z = model.predict_proba(X)[:,1].reshape(xx.shape)\n",
    "    \n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    if not plot_proba:\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                    levels=np.arange(n_classes + 1) - 0.5,\n",
    "                    cmap=cmap, zorder=1)\n",
    "    else:\n",
    "        ax.pcolormesh(xx, yy, Z, cmap=cmap, shading=\"auto\")\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DT with varying depth and visualize limits\n",
    "max_depth=9\n",
    "model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
    "tree=model.fit(X,y)\n",
    "print ('Depth, score: ', max_depth, tree.score(X,y))\n",
    "visualize_classifier(model, X, y, cmap = \"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high depth, clearly goes into over-training\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees** work well in principle, however, they are rather sensitive to over-training  \n",
    "&rarr; Validation curve left for exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Decision tree example with real data\n",
    "\n",
    "A frequently used data set for ML is a data set for *breast cancer diagnosis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print (cancer.feature_names)\n",
    "print (cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply decision-tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without limiting the depth, the DT will be evolved until perfect accuracy.\n",
    "\n",
    "But not really useful &rarr; Over-training\n",
    "\n",
    "Better approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance on the test set has improved by introducing a maximum depth of the trees. (The fact that we do no longer get perfect classfication on the training sample is not relevant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "\n",
    "A very useful additional result of DT classification is the *feature importance*.\n",
    "This gives for each feature a rating between 0 and 1 how important it is for the classification:\n",
    "* 0 means no effect, not useful\n",
    "* 1 means perfect separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to visualize\n",
    "def plot_feature_importance( model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.figure(figsize = (6,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features);\n",
    "\n",
    "    \n",
    "plot_feature_importance( tree )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly shows that feature ` ẁorst radius` has largest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Decisions trees are potentially very powerful models but they are very also sensitive to overtraining (overfitting); therefore they are normally not directly used in practice. \n",
    "\n",
    "However, one can mitigate or solve this problem by using an ensemble of decision trees and not just a single DT.  \n",
    "The main trick is randomization:\n",
    "* train many DTs but\n",
    "    * each DT sees different parts of the data\n",
    "    * or different set of features\n",
    "\n",
    "This approach is called **Random Forest**:  \n",
    "Many randomized trees contribute and the final decision is made by some sort of majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with half moon data using 5 DTs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "# random forest with 5 DT\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(forest, X, y, cmap=\"rainbow\", plot_proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively, boundaries are more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better accuracy on validation set\n",
    "\n",
    "**Feature importance** also instructive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* many more features contribute\n",
    "* `worst radius` feature no longer dominant\n",
    "\n",
    "&rarr; Random Forests much better in classification and better exploit information of features \n",
    "\n",
    "Main drawback: decision process rather intransparent compared to single DT.\n",
    "\n",
    "An alternative to Random Forests are **Boosted** Decision Trees  \n",
    "&rarr; literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### Random Forest for Digits Data\n",
    "\n",
    "Come back to example of digit classification and apply RandomForest to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply RFC to digit data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(model.score(Xtrain, ytrain)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(model.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Excellent performance of Random Forest!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add-on: Try a more complicated dataset: 2 spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "p = sorted(np.random.random(N)*np.pi*5)\n",
    "v1 = p*np.cos(p)\n",
    "v2 = p*np.sin(p)\n",
    "p2 = sorted(np.random.random(N)*np.pi*5+np.pi)\n",
    "w1 = p*np.cos(p2)\n",
    "w2 = p*np.sin(p2)\n",
    "plt.scatter(v1, v2)\n",
    "plt.scatter(w1, w2)\n",
    "y1 = np.zeros((N, 1))\n",
    "y2 = np.ones((N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.concatenate([v1, w1]), np.concatenate([v2, w2])], axis = 1)\n",
    "y = np.concatenate([y1, y2]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply RFC to digit data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=1000)\n",
    "forest.fit(Xtrain, ytrain)\n",
    "ypred = forest.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ypred==ytest).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_classifier(forest, Xtest, ytest, cmap=\"rainbow\", plot_proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color by predicted probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(forest, Xtest, ytest, cmap=\"Blues\", plot_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next topic: [preprocessing (scaling) of input features](DataRescaling.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "There is a nice interactive tool that helps to understand how decision trees work:\n",
    "\n",
    "[![Screenshot](figures/screenshot_BDT_playground.png)](https://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)\n",
    "\n",
    "This also allows to use rotated decision trees, originally proposed in [2006](https://ieeexplore.ieee.org/document/1677518). You can read more about this e.g. [here](https://jmlr.csail.mit.edu/papers/volume17/blaser16a/blaser16a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Feed rotated features to the Classifier\n",
    "\n",
    "A similar effect to using \"rotated decision trees\" (or more precisely: rotations of the feature space in ensemble learning) can be achieved by feeding the classifier a set of additional features which are rotated versions of the original input features (called augmentation of the input data).\n",
    "\n",
    "*Your task*: Try feeding the x and y coordinates with 10 different rotations from $0$ to $\\frac{\\pi}{2}$ and see \n",
    "how this improves the decision contour on the moons or spiral dataset.\n",
    "\n",
    "You can try to solve this completely on your own or follow the guidelines to a solution that are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to rotate the input coordinates is via a rotation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(X, angle):\n",
    "    m = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                  [np.sin(angle), np.cos(angle)]])\n",
    "    return np.dot(m, X.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then concatenate e.g. 10 rotated vectors to get the new \"augmented\" input. You can use a [sklearn Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) to automate this. First, define a \"Transformer\" that augments the input coordinates with rotated versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotationAugmentor(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, rotation_angles):\n",
    "        self.rotation_angles = rotation_angles\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.concatenate([rotate(X, angle) for angle in self.rotation_angles], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_angles = np.linspace(0, 0.5 * np.pi, 10)\n",
    "rotation_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_augmentor = RotationAugmentor(rotation_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at one of the input data points to see the rotated versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xrotated = rotation_augmentor.transform(X)\n",
    "Xrotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*Xrotated[0].reshape(-1, 2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define an [sklearn Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) that first transforms the input data and then fits a new `RandomForestClassifier`.\n",
    "\n",
    "Then, fit it and plot the results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
